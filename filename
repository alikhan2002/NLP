 1/1: import pandas as pd
 1/2:
import pandas as pd
import seaborn as sns
 1/3: arr = np.linspace(1, 100)
 1/4:
import pandas as pd
import seaborn as sns
import numpy as np
 1/5: arr = np.linspace(1, 100)
 1/6: arr
 1/7: arr = np.linspace(1, 100,2)
 1/8: arr
 1/9: arr
1/10: arr = np.linspace(1, 100)
1/11: arr
1/12: arr = np.arange(1, 10)
1/13: arr
1/14: arr = np.arange(1, 10,2)
1/15: arr
1/16: df = pd.read_csv('IRIS.csv')
1/17: df
 2/1: inp = input()
 2/2: inp
 2/3: inp = input().split()
 2/4: inp
 2/5:
a = [] 
for x in inp:
    a.append(int(x))
 2/6: a
 2/7:
import pandas as pd
import seaborn as sns
import numpy as np
 2/8: np.mean(a)
 2/9: np.std(a)
2/10: np.var(a)
2/11: pd.var(a)
2/12: pd.DataFrame(a).var()
2/13: inp = input().split()
2/14:
a = [] 
for x in inp:
    a.append(int(x))
2/15: pd.DataFrame(a).var()
2/16: pd.DataFrame(a).std()
2/17: inp = input().split()
2/18:
a = [] 
for x in inp:
    a.append(int(x))
2/19: pd.DataFrame(a).std()
2/20: pd.DataFrame(a).var()
2/21: a
2/22: a.mean()
2/23: np.mean(a)
2/24: np.var(a)
2/25: pd.DataFrame(a).std()
2/26: pd.DataFrame(a).var()
2/27: a
2/28:
def mean(arr):
    return sum(arr)/len(arr)
2/29:
def var(arr):
    mn = mean(arr)
    res = 0
    for x in arr:
        res += (x - mn)
    return res/(len(arr)-1)
2/30: mean = mean(arr)
2/31: mean = mean(a)
2/32: mean
2/33: var(arr)
2/34: var(a|)
2/35: var(a)
2/36: a
2/37: var = var(a)
2/38:
def var(arr):
    mn = sum(arr)/len(arr)
    res = 0
    for x in arr:
        res += (x - mn)
    return res/(len(arr)-1)
2/39: var = var(a)
2/40: var
2/41:
def var(arr):
    mn = sum(arr)/len(arr)
    res = 0
    for x in arr:
        res += (x - mn)
    print(res)
    return res/(len(arr)-1)
2/42: var = var(a)
2/43:
def var(arr):
    mn = sum(arr)/len(arr)
    res = 0
    for x in arr:
        res += (x - mn)
    print(res)
    return res/(len(arr)-1)
2/44: var = var(a)
2/45:
def var(arr):
    mn = sum(arr)/len(arr)
    res = 0
    for x in arr:
        print(x)
        res += (x - mn)
    print(res)
    return res/(len(arr)-1)
2/46: var = var(a)
2/47:
def var(arr):
    mn = sum(arr)/len(arr)
    res = 0
    print(mn)
    for x in arr:
        res += (x - mn)
    print(res)
    return res/(len(arr)-1)
2/48: var = var(a)
2/49:
def var(arr):
    mn = sum(arr)/len(arr)
    res = 0
    for x in arr:
        res += (x - mn)**2
    print(res)
    return res/(len(arr)-1)
2/50: var = var(a)
2/51: pd.DataFrame(a).var()
2/52:
def var(arr):
    mn = sum(arr)/len(arr)
    res = 0
    for x in arr:
        print(x)
        res += (x - mn)**2
    return res/(len(arr)-1)
2/53: var = var(a)
2/54:
def var(arr):
    mn = sum(arr)/len(arr)
    res = 0
    for x in arr:
        res += (x - mn)*(x-mn)
    return res/(len(arr)-1)
2/55: var = var(a)
2/56: mean = mean(a)
2/57:
var = var(a)
var
2/58:
v = var(a)
var
2/59:
v = var(a)
v
2/60: v
2/61: v = var(a)
2/62: vv = var(a)
2/63: v = var(a)
2/64: var(a)
2/65: var(a)
2/66:
def var(arr):
    mn = sum(arr)/len(arr)
    res = 0
    for x in arr:
        res += (x - mn)
    return res/(len(arr)-1)
2/67: var(a)
2/68:
def var(arr):
    mn = sum(arr)/len(arr)
    res = 0
    for x in arr:
        res += (x - mn)**2
    return res/(len(arr)-1)
2/69: var(a)
2/70:
def var(arr):
    mn = sum(arr)/len(arr)
    res = 0
    for x in arr:
        res += (x - mn)**2
    return res/(len(arr))
2/71: var(a)
 3/1: inp = input().split()
 3/2:
a = [] 
for x in inp:
    a.append(int(x))
 3/3: a
 3/4: pd.DataFrame(a).var()
 3/5:
import pandas as pd
import seaborn as sns
import numpy as np
 3/6: pd.DataFrame(a).var()
 3/7: pd.DataFrame(a).std()
 3/8: inp = input().split()
 3/9: inp = input().split()
3/10:
a = [] 
for x in inp:
    a.append(int(x))
3/11: a
3/12: pd.DataFrame(a).std()
3/13:
def mean(arr):
    return sum(arr)/len(arr)
3/14: inp = input().split()
3/15:
a = [] 
for x in inp:
    a.append(int(x))
3/16: a
3/17: pd.DataFrame(a).std()
3/18:
def mean(arr):
    return sum(arr)/len(arr)
3/19: inp = input().split()
3/20:
a = [] 
for x in inp:
    a.append(int(x))
3/21: inp = input().split(',')
3/22:
a = [] 
for x in inp:
    a.append(int(x))
3/23: a
3/24: pd.DataFrame(a).std()
 4/1: !pip install textblob
 6/1:
import textblob
from textblob import TextBlob
 6/2: text = "Hello everyone! Welcome to my blog post on Medium. We are studying Natural Language Processing."
 6/3: TextBlob(text).words
 6/4: python -m textblob.download_corpora
 7/1:
import textblob
from textblob import TextBlob
 7/2: !pip install textblob
 7/3:
import textblob
from textblob import TextBlob
 7/4: text = "Hello everyone! Welcome to my blog post on Medium. We are studying Natural Language Processing."
 7/5: TextBlob(text).words
 7/6:
import nltk
from nltk import sent_tokenize
from nltk import word_tokenize
 7/7:
tokens_sents = nltk.sent_tokenize(text)
print(tokens)
 7/8:
tokens = nltk.sent_tokenize(text)
print(tokens)
 7/9:
tokens_words = nltk.word_tokenize(text)
print(tokens_words)
7/10: from nltk.stem import PorterStemmer
7/11:
ps = PorterStemmer()
word = ("civilization")
ps.stem(word)
7/12: from nltk.stem.snowball import SnowballStemmer
7/13:
stemmer = SnowballStemmer(language = "english")
word = "civilization"
stemmer.stem(word)
7/14:
import nltk
from nltk.stem import WordNetLemmatizer 
lemmatizer = WordNetLemmatizer()
7/15:
# Lemmatize single word

print(lemmatizer.lemmatize("workers"))
print(lemmatizer.lemmatize("beeches"))
7/16: nltk.download()
 8/1:
import textblob
from textblob import TextBlob
 8/2: text = "Hello everyone! Welcome to my blog post on Medium. We are studying Natural Language Processing."
 8/3: TextBlob(text).words
 8/4:
import nltk
from nltk import sent_tokenize
from nltk import word_tokenize
 8/5:
import nltk
nltk.download('omw-1.4')
 8/6:
# Lemmatize single word

print(lemmatizer.lemmatize("workers"))
print(lemmatizer.lemmatize("beeches"))
 8/7:
import nltk
from nltk.stem import WordNetLemmatizer 
lemmatizer = WordNetLemmatizer()
 8/8:
# Lemmatize single word

print(lemmatizer.lemmatize("workers"))
print(lemmatizer.lemmatize("beeches"))
 8/9:
text = "Let’s lemmatize a simple sentence. We first tokenize the sentence into words using nltk.word_tokenize and then we will call lemmatizer.lemmatize() on each word. "
word_list = nltk.word_tokenize(text)
print(word_list)
8/10:
lemmatized_output = ' '.join([lemmatizer.lemmatize(w) for w in word_list])
print(lemmatized_output)
8/11:
# pip install textblob

from textblob import TextBlob, Word
8/12:
word = 'stripes'
w = Word(word)
w.lemmatize()
8/13:
text = "The striped bats are hanging on their feet for best"
sent = TextBlob(text)
" ". join([w.lemmatize() for w in sent.words])
8/14:
text = '''Those , have who information who are at still witnesses best of times BLANK the can contact Pastor ( Tel : or 05461 BLANK 885041 the Nachrichten 05461 ( 930010 / ) .
Königs in emphasized the of guests at the Textile Academy the where of opening the celebrated CTL The : &quot; and industry clothing has ahead decisive a renaissance it of .
to According police the BLANK , several residents Monday the in were BLANK BLANK area alleged officers police Sunday on .'''
8/15: text
8/16: TextBlob(text).words
8/17:
tokens = nltk.sent_tokenize(text)
print(tokens)
8/18:
tokens_words = nltk.word_tokenize(text)
print(tokens_words)
8/19:
ps = PorterStemmer()
word = ("civilization")
ps.stem(word)
8/20:
ps = PorterStemmer()
word = ("civilization")
ps.stem(word)
8/21:
import nltk
from nltk import sent_tokenize
from nltk import word_tokenize
8/22: from nltk.stem import PorterStemmer
8/23:
ps = PorterStemmer()
word = ("civilization")
ps.stem(word)
8/24:
ps = PorterStemmer()
word = ("civilization")
ps.stem(word)
8/25:
word_list = nltk.word_tokenize(text)
print(word_list)
8/26:
lemmatized_output = ' '.join([lemmatizer.lemmatize(w) for w in word_list])
print(lemmatized_output)
8/27: !pip install razdel
8/28: pip install cython
8/29: !pip install numpy==1.14.5
8/30: !pip upgrade tenserflow
8/31: pip install --upgrade tensorflow
 9/1:
import textblob
from textblob import TextBlob
 9/2: text = "Hello everyone! Welcome to my blog post on Medium. We are studying Natural Language Processing."
 9/3: TextBlob(text).words
 9/4:
import nltk
from nltk import sent_tokenize
from nltk import word_tokenize
 9/5:
tokens = nltk.sent_tokenize(text)
print(tokens)
 9/6:
tokens_words = nltk.word_tokenize(text)
print(tokens_words)
 9/7: from nltk.stem import PorterStemmer
 9/8:
ps = PorterStemmer()
word = ("civilization")
ps.stem(word)
 9/9: from nltk.stem.snowball import SnowballStemmer
9/10:
stemmer = SnowballStemmer(language = "english")
word = "civilization"
stemmer.stem(word)
9/11:
import nltk
from nltk.stem import WordNetLemmatizer 
lemmatizer = WordNetLemmatizer()
9/12:
# Lemmatize single word

print(lemmatizer.lemmatize("workers"))
print(lemmatizer.lemmatize("beeches"))
9/13:
text = "Let’s lemmatize a simple sentence. We first tokenize the sentence into words using nltk.word_tokenize and then we will call lemmatizer.lemmatize() on each word. "
word_list = nltk.word_tokenize(text)
print(word_list)
9/14:
lemmatized_output = ' '.join([lemmatizer.lemmatize(w) for w in word_list])
print(lemmatized_output)
9/15:

from textblob import TextBlob, Word
9/16:
word = 'stripes'
w = Word(word)
w.lemmatize()
9/17:
text = "The striped bats are hanging on their feet for best"
sent = TextBlob(text)
" ". join([w.lemmatize() for w in sent.words])
9/18:
text = '''Those , have who information who are at still witnesses best of times BLANK the can contact Pastor ( Tel : or 05461 BLANK 885041 the Nachrichten 05461 ( 930010 / ) .
Königs in emphasized the of guests at the Textile Academy the where of opening the celebrated CTL The : &quot; and industry clothing has ahead decisive a renaissance it of .
to According police the BLANK , several residents Monday the in were BLANK BLANK area alleged officers police Sunday on .'''
9/19: text
9/20: TextBlob(text).words
9/21:
tokens = nltk.sent_tokenize(text)
print(tokens)
9/22:
tokens_words = nltk.word_tokenize(text)
print(tokens_words)
9/23:
lemmatized_output = ' '.join([lemmatizer.lemmatize(w) for w in word_list])
print(lemmatized_output)
9/24: !pip install razdel
9/25: pip install cython
9/26: !pip install setuptools == 39.1.0
9/27: !pip install setuptools==39.1.0
9/28: !pip install razdel
9/29: from razdel import tokenize
9/30: tokens = list(tokenize('Кружка-термос на 0.5л (50/64 см³, 516;...)'))
9/31: tokens
9/32: tokens.text
9/33:
for x in tokens:
    print(x)
9/34:
for x in tokens:
    print(x.text)
9/35: [_.text for _ in tokens]
9/36:
 text = '''
... - "Так в чем же дело?" - "Не ра-ду-ют".
... И т. д. и т. п. В общем, вся газета
... '''
9/37: from razdel import sentenize
9/38: list(sentenize(text))
9/39: import pandas as pd
9/40: !pip install pandas
9/41: import pandas as pd
9/42: import pandas as pd
10/1: import pandas as pd
10/2: !pip install pandas
10/3: !pip install pandas
10/4: import pandas as pd
10/5: df = pd.read_json('bank_faq.json')
10/6: df
10/7: df['Рахмет']
10/8: df.loc['Рахмет']
10/9: df.loc['Рахмет']['Депозит']
10/10: df.loc['Рахмет']
10/11: df
10/12: df['Депозит']
10/13: df['Депозит'].iloc[:4]
10/14: df['Депозит'].iloc[:4].indexes
10/15: df['Депозит'].iloc[:4].index
10/16:
for ind in indexes.to_list():
    print(ind)
10/17: indexes = df['Депозит'].iloc[:4].index
10/18:
for ind in indexes.to_list():
    print(ind)
10/19:
for ind in indexes.to_list():
    print(df.loc[ind])
10/20:
for ind in indexes.to_list():
    print(df.loc[ind]['Депозит'])
10/21:
questions = []
answers = []
10/22:
for ind in indexes.to_list():
#     questions.append()
    for x in df.loc[ind]['Депозит']:
        print(x)
#     print(df.loc[ind]['Депозит'])
10/23:
indexes = df.index
# indexes = df['Карта'].iloc[4:].index.to_list()
10/24: indexes
10/25: indexes = df.index.to_list()
10/26:
for ind in indexes:
#     questions.append()
    for x in df.loc[ind]['Депозит']:
        print(x)
#     print(df.loc[ind]['Депозит'])
10/27:
for ind in indexes:
#     questions.append()
#     for x in df.loc[ind]['Депозит']:
#         print(x)
    print(df.loc[ind]['Депозит'])
10/28:
for ind in indexes:
#     questions.append()
    if df.loc[ind]['Депозит'].isna():
        print('sfs')
#     for x in df.loc[ind]['Депозит']:
#         print(x)
#     print(df.loc[ind]['Депозит'])
10/29:
for ind in indexes:
#     questions.append()
    if df.loc[ind]['Депозит'] == NaN:
        print('sfs')
#     for x in df.loc[ind]['Депозит']:
#         print(x)
#     print(df.loc[ind]['Депозит'])
10/30:
for ind in indexes:
#     questions.append()
    if df.loc[ind]['Депозит'] is None:
        print('sfs')
#     for x in df.loc[ind]['Депозит']:
#         print(x)
#     print(df.loc[ind]['Депозит'])
10/31:
for ind in indexes:
#     questions.append()
#     if df.loc[ind]['Депозит'] is None:
#         print('sfs')
#     for x in df.loc[ind]['Депозит']:
#         print(x)
    print(df.loc[ind]['Депозит'])
10/32:
for ind in indexes[:4]:
#     questions.append()
    print(df.loc[ind]['Депозит'])
10/33:
for ind in indexes[:4]:
#     questions.append()
    for x in df.loc[ind]['Депозит']:
        print(x)
    print(df.loc[ind]['Депозит'])
10/34:
for ind in indexes[:4]:
#     questions.append()
    for x in df.loc[ind]['Депозит']:
#         print(x)
        questions.append(x)
questions
#     print(df.loc[ind]['Депозит'])
10/35:
for ind in indexes[:4]:
#     questions.append()
    for x in df.loc[ind]['Депозит']:
#         print(x)
        questions.append(x['Вопрос'])
questions
#     print(df.loc[ind]['Депозит'])
10/36:
for ind in indexes[:4]:
#     questions.append()
    for x in df.loc[ind]['Депозит']:
#         print(x)
        questions.append(x['вопрос'])
questions
#     print(df.loc[ind]['Депозит'])
10/37:
for ind in indexes[:4]:
    for x in df.loc[ind]['Депозит']:
        questions.append(x['вопрос'])
questions
10/38:
for ind in indexes[:4]:
    for x in df.loc[ind]['Депозит']:
        questions.append(x['вопрос'])
print()
10/39: questions
10/40:
for ind in indexes[:4]:
    for x in df.loc[ind]['Депозит']:
        print(x)
        questions.append(x['вопрос'])
# print()
10/41:
for ind in indexes[:4]:
    for x in df.loc[ind]['Депозит']:
        print(x['вопрос'])
        questions.append(x['вопрос'])
# print()
10/42:
for ind in indexes[:4]:
    for x in df.loc[ind]['Депозит']:
#         print(x['вопрос'])
        questions.append(x['вопрос'])
# print()
10/43: questions
10/44:
for ind in indexes[:4]:
    for x in df.loc[ind]['Депозит']:
        print(x['вопрос'])
        questions.append(x['вопрос'])
# print()
10/45:
questions = []
answers = []
10/46:
questions = []
answers = []
for ind in indexes[:4]:
    for x in df.loc[ind]['Депозит']:
        print(x['вопрос'])
        questions.append(x['вопрос'])
# print()
10/47: questions
10/48:
questions = []
answers = []
for ind in indexes[:4]:
    for x in df.loc[ind]['Депозит']:
#         print(x['вопрос'])
        questions.append(x['вопрос'])
for ind in indexes[4:]:
    for x in df.loc[ind]['Карта']:
#         print(x['вопрос'])
        questions.append(x['вопрос'])
10/49: questions
10/50:
questions = []
answers = []
for ind in indexes[:4]:
    for x in df.loc[ind]['Депозит']:
#         print(x['вопрос'])
        questions.append(x['вопрос'])
        answers.append(x['ответ'])
for ind in indexes[4:]:
    for x in df.loc[ind]['Карта']:
#         print(x['вопрос'])
        questions.append(x['вопрос'])
        answers.append(x['ответ'])
10/51: answers
10/52: TextBlob(text).words
10/53:
import textblob
from textblob import TextBlob
10/54:
[_ for TextBlob(_).words in questions]
# TextBlob(text).words
10/55: [x for x in TextBlob(x).words]
10/56: [TextBlob(x).words for x in answers]
10/57: [TextBlob(x).words.to_list() for x in answers]
10/58: [TextBlob(x).words for x in answers]
10/59: [list(TextBlob(x).words) for x in answers]
10/60: [TextBlob(x).words for x in answers]
10/61: [list(TextBlob(x).words) for x in answers]
11/1: import json
11/2:

with open('bank_faq.json') as f:
    data = json.load(f)
11/3:

with open('bank_faq.json', encoding='utf-8') as f:
    data = json.load(f)
11/4: data
11/5: data['Депозит']
11/6: data['Депозит'].keys()
11/7:
answers = []
questions = []
for k in data['Депозит'].keys():
    print(data[k]['ответ'])
11/8: data
11/9:
answers = []
questions = []
for k in data['Депозит'].keys():
    for x in data['Депозит'][k]:
        print(x)
11/10:
answers = []
questions = []
for k in data['Депозит'].keys():
    for x in data['Депозит'][k]:
        print(x['вопрос'])
11/11:
answers = []
questions = []
for x in data:
    print(x)
# for k in data['Депозит'].keys():
#     for x in data['Депозит'][k]:
#         print(x['вопрос'])
11/12:
answers = []
questions = []
for x in data:
    for k in data[x]
        print(k)
# for k in data['Депозит'].keys():
#     for x in data['Депозит'][k]:
#         print(x['вопрос'])
11/13:
answers = []
questions = []
for x in data:
    for k in data[x]:
        print(k)
# for k in data['Депозит'].keys():
#     for x in data['Депозит'][k]:
#         print(x['вопрос'])
11/14:
answers = []
questions = []
for x in data:
    for k in data[x]:
        for a in data[x][k]:
            print(a)
# for k in data['Депозит'].keys():
#     for x in data['Депозит'][k]:
#         print(x['вопрос'])
11/15:
answers = []
questions = []
for x in data:
    for k in data[x]:
        for a in data[x][k]:
            print(a)
11/16:
answers = []
questions = []
for x in data:
    for k in data[x]:
        for a in data[x][k]:
            questions.append(a['вопрос'])
            answers.append(a['ответ'])
11/17: questions
11/18: answers
11/19: from razdel import tokenize
11/20: [list(tokenize(x)) for x in answers]
11/21: [list(tokenize(x)) for x in questions]
11/22: !pip install gensim
11/23: import gensim.downloader as api
11/24: !pip install gensim
11/25: import gensim.downloader as api
11/26: pip install --upgrade setuptools
12/1: !pip install gensim
12/2: pip install -U steem
12/3: !pip install gensim
12/4: !pip install --upgrade tenserflow
12/5: !pip install --upgrade pip
12/6: python -m pip install --upgrade pip
12/7: !python -m pip install --upgrade pip
12/8: !pip install --upgrade tenserflow
12/9: !pip install gensim
12/10: pip install -U steem
12/11: !pip install gensim
12/12: !python -m pip install pip==10.0.1
12/13: !pip install gensim
12/14: !pip install gensim
12/15: pip install -U steem
12/16: pip install --upgrade tensorflow
12/17: wv = api.load('word2vec-google-news-300')
12/18: !pip install gensim
12/19: import gensim.downloader as api
13/1: !pip install gensim
15/1: import gensim.downloader as api
15/2: info = api.info()
15/3: wv = api.load('word2vec-google-news-300')
15/4: info = api.info()
15/5: wv = api.load('word2vec-google-news-300')
15/6: wv.most_similar('tea')
15/7: glove = api.load('glove-twitter-50')
15/8: fasttext = api.load('fasttext-wiki-news-subwords-300')
15/9: wv.most_similar('tea')
15/10: glove.most_similar('tea')
16/1:
# загрузим библиотеки и установим опции
from __future__ import division, print_function
# отключим всякие предупреждения Anaconda
import warnings
warnings.filterwarnings('ignore')
#%matplotlib inline
import numpy as np
import pandas as pd
from sklearn.metrics import roc_auc_score
16/2:
# загрузим обучающую и тестовую выборки
train_df = pd.read_csv('data/train_sessions.csv')#,index_col='session_id')
test_df = pd.read_csv('data/test_sessions.csv')#, index_col='session_id')

# приведем колонки time1, ..., time10 к временному формату
times = ['time%s' % i for i in range(1, 11)]
train_df[times] = train_df[times].apply(pd.to_datetime)
test_df[times] = test_df[times].apply(pd.to_datetime)

# отсортируем данные по времени
train_df = train_df.sort_values(by='time1')

# посмотрим на заголовок обучающей выборки
train_df.head()
16/3:
# загрузим обучающую и тестовую выборки
train_df = pd.read_csv('train_sessions.csv')#,index_col='session_id')
test_df = pd.read_csv('test_sessions.csv')#, index_col='session_id')

# приведем колонки time1, ..., time10 к временному формату
times = ['time%s' % i for i in range(1, 11)]
train_df[times] = train_df[times].apply(pd.to_datetime)
test_df[times] = test_df[times].apply(pd.to_datetime)

# отсортируем данные по времени
train_df = train_df.sort_values(by='time1')

# посмотрим на заголовок обучающей выборки
train_df.head()
16/4:
# загрузим обучающую и тестовую выборки
train_df = pd.read_csv('train_sessions.csv')#,index_col='session_id')
test_df = pd.read_csv('test_sessions.csv')#, index_col='session_id')

# приведем колонки time1, ..., time10 к временному формату
times = ['time%s' % i for i in range(1, 11)]
train_df[times] = train_df[times].apply(pd.to_datetime)
test_df[times] = test_df[times].apply(pd.to_datetime)

# отсортируем данные по времени
train_df = train_df.sort_values(by='time1')

# посмотрим на заголовок обучающей выборки
train_df.head()
16/5:
sites = ['site%s' % i for i in range(1, 11)]
#заменим nan на 0
train_df[sites] = train_df[sites].fillna(0).astype('int').astype('str')
test_df[sites] = test_df[sites].fillna(0).astype('int').astype('str')
#создадим тексты необходимые для обучения word2vec
train_df['list'] = train_df['site1']
test_df['list'] = test_df['site1']
for s in sites[1:]:
    train_df['list'] = train_df['list']+","+train_df[s]
    test_df['list'] = test_df['list']+","+test_df[s]
train_df['list_w'] = train_df['list'].apply(lambda x: x.split(','))
test_df['list_w'] = test_df['list'].apply(lambda x: x.split(','))
16/6:
#В нашем случае предложение это набор сайтов, которые посещал пользователь
#нам необязательно переводить цифры в названия сайтов, т.к. алгоритм будем выявлять взаимосвязь их друг с другом.
train_df['list_w'][10]
16/7:
sites = ['site%s' % i for i in range(1, 11)]
#заменим nan на 0
train_df[sites] = train_df[sites].fillna(0).astype('int').astype('str')
test_df[sites] = test_df[sites].fillna(0).astype('int').astype('str')
#создадим тексты необходимые для обучения word2vec
train_df['list'] = train_df['site1']
test_df['list'] = test_df['site1']
for s in sites[1:]:
    train_df['list'] = train_df['list']+","+train_df[s]
    test_df['list'] = test_df['list']+","+test_df[s]
train_df['list_w'] = train_df['list'].apply(lambda x: x.split(','))
test_df['list_w'] = test_df['list'].apply(lambda x: x.split(','))
16/8:
# подключим word2vec
from gensim.models import word2vec
16/9:
#объединим обучающую и тестовую выборки и обучим нашу модель на всех данных 
#с размером окна в 6=3*2 (длина предложения 10 слов) и итоговыми векторами размерности 300, параметр workers отвечает за количество ядер
test_df['target'] = -1
data = pd.concat([train_df,test_df], axis=0)

model = word2vec.Word2Vec(data['list_w'], size=300, window=3, workers=4)
#создадим словарь со словами и соответсвующими им векторами
w2v = dict(zip(model.wv.index2word, model.wv.syn0))
16/10:
#объединим обучающую и тестовую выборки и обучим нашу модель на всех данных 
#с размером окна в 6=3*2 (длина предложения 10 слов) и итоговыми векторами размерности 300, параметр workers отвечает за количество ядер
test_df['target'] = -1
data = pd.concat([train_df,test_df], axis=0)

model = word2vec.Word2Vec(data['list_w'], vector_size=300, window=3, workers=4)
#создадим словарь со словами и соответсвующими им векторами
w2v = dict(zip(model.wv.index2word, model.wv.syn0))
16/11:
#объединим обучающую и тестовую выборки и обучим нашу модель на всех данных 
#с размером окна в 6=3*2 (длина предложения 10 слов) и итоговыми векторами размерности 300, параметр workers отвечает за количество ядер
test_df['target'] = -1
data = pd.concat([train_df,test_df], axis=0)

model = word2vec.Word2Vec(data['list_w'], vector_size=300, window=3, workers=4)
#создадим словарь со словами и соответсвующими им векторами
w2v = dict(zip(model.wv.index_to_key, model.wv.syn0))
16/12:
#объединим обучающую и тестовую выборки и обучим нашу модель на всех данных 
#с размером окна в 6=3*2 (длина предложения 10 слов) и итоговыми векторами размерности 300, параметр workers отвечает за количество ядер
test_df['target'] = -1
data = pd.concat([train_df,test_df], axis=0)

model = word2vec.Word2Vec(data['list_w'], vector_size=300, window=3, workers=4)
#создадим словарь со словами и соответсвующими им векторами
w2v = dict(zip(model.wv.index_to_key, model.wv.vectors))
16/13: w2v
16/14:
class mean_vectorizer(object):
    def __init__(self, word2vec):
        self.word2vec = word2vec
        self.dim = len(next(iter(w2v.values())))

    def fit(self, X):
        return self 

    def transform(self, X):
        return np.array([
            np.mean([self.word2vec[w] for w in words if w in self.word2vec] 
                    or [np.zeros(self.dim)], axis=0)
            for words in X
        ])
16/15:
data_mean=mean_vectorizer(w2v).fit(train_df['list_w']).transform(train_df['list_w'])
data_mean.shape
16/16:
# Воспользуемся валидацией
def split(train,y,ratio):
    idx = round(train.shape[0] * ratio)
    return train[:idx, :], train[idx:, :], y[:idx], y[idx:]
y = train_df['target']
Xtr, Xval, ytr, yval = split(data_mean, y,0.8)
Xtr.shape,Xval.shape,ytr.mean(),yval.mean()
16/17:
# подключим библиотеки keras 
from keras.models import Sequential, Model
from keras.layers import Dense, Dropout, Activation, Input
from keras.preprocessing.text import Tokenizer
from keras import regularizers
16/18:
# опишем нейронную сеть
model = Sequential()
model.add(Dense(128, input_dim=(Xtr.shape[1])))
model.add(Activation('relu'))
model.add(Dropout(0.5))
model.add(Dense(1))
model.add(Activation('sigmoid'))
model.compile(loss='binary_crossentropy',
              optimizer='adam',
              metrics=['binary_accuracy'])
16/19:
history = model.fit(Xtr, ytr,
                    batch_size=128,
                    epochs=10,
                    validation_data=(Xval, yval),
                    class_weight='auto',
                    verbose=0)
16/20: Xval
16/21:
history = model.fit(Xtr, ytr,
                    batch_size=128,
                    epochs=10,
                    validation_data=(Xval, yval),
                    class_weight='auto',
                    verbose=0)
16/22:
history = model.fit(Xtr, ytr,
                    batch_size=128,
                    epochs=10,
                    validation_data=(Xval, yval),
                    class_weight='auto')
16/23:
history = model.fit(Xtr, ytr,
                    batch_size=128,
                    epochs=10,
                    validation_data=(Xval, yval),
                    class_weight='auto',
                    verbose=0)
16/24:
history = model.fit(Xtr, ytr,
                    batch_size=128,
                    epochs=10,
                    validation_data=(Xval, yval),
                    class_weight={0: weight_for_class_0, 1: weight_for_class_1},
                    verbose=0)
16/25:
history = model.fit(Xtr, ytr,
                    batch_size=128,
                    epochs=10,
                    validation_data=(Xval, yval),
                    class_weight='auto',
                    verbose=0)
16/26:
history = model.fit(Xtr, ytr,
                    batch_size=128,
                    epochs=10,
                    validation_data=(Xval, yval),
                    class_weight='auto',
                    verbose=0)
16/27:
# подключим библиотеки keras 
from keras.models import Sequential, Model
from keras.layers import Dense, Dropout, Activation, Input
from keras.preprocessing.text import Tokenizer
from keras import regularizers
16/28:
# опишем нейронную сеть
model = Sequential()
model.add(Dense(128, input_dim=(Xtr.shape[1])))
model.add(Activation('relu'))
model.add(Dropout(0.5))
model.add(Dense(1))
model.add(Activation('sigmoid'))
model.compile(loss='binary_crossentropy',
              optimizer='adam',
              metrics=['binary_accuracy'])
16/29:
history = model.fit(Xtr, ytr,
                    batch_size=128,
                    epochs=10,
                    validation_data=(Xval, yval),
                    class_weight='auto',
                    verbose=0)
16/30:
history = model.fit(Xtr, ytr,
                    batch_size=128,
                    epochs=10,
                    validation_data=(Xval, yval),
                    
                    verbose=0)
16/31:
classes = model.predict(Xval, batch_size=128)
roc_auc_score(yval, classes)
16/32: import xgboost as xgb
16/33: !pip install xgboost"
16/34: !pip install xgboost
16/35:
dtr = xgb.DMatrix(Xtr, label= ytr,missing = np.nan)
dval = xgb.DMatrix(Xval, label= yval,missing = np.nan)
watchlist = [(dtr, 'train'), (dval, 'eval')]
history = dict(
16/36: import xgboost as xgb
16/37: # !pip install xgboost
16/38:
dtr = xgb.DMatrix(Xtr, label= ytr,missing = np.nan)
dval = xgb.DMatrix(Xval, label= yval,missing = np.nan)
watchlist = [(dtr, 'train'), (dval, 'eval')]
history = dict()
16/39:
params = {
    'max_depth': 26,
    'eta': 0.025,
    'nthread': 4,
    'gamma' : 1,
    'alpha' : 1,
    'subsample': 0.85,
    'eval_metric': ['auc'],
    'objective': 'binary:logistic',
    'colsample_bytree': 0.9,
    'min_child_weight': 100,
    'scale_pos_weight':(1)/y.mean(),
    'seed':7
}
16/40: model_new = xgb.train(params, dtr, num_boost_round=200, evals=watchlist, evals_result=history, verbose_eval=20)
16/41:
#пропишем класс выполняющий tfidf преобразование.
from sklearn.feature_extraction.text import TfidfVectorizer
from collections import defaultdict

class tfidf_vectorizer(object):
    def __init__(self, word2vec):
        self.word2vec = word2vec
        self.word2weight = None
        self.dim = len(next(iter(w2v.values())))

    def fit(self, X):
        tfidf = TfidfVectorizer(analyzer=lambda x: x)
        tfidf.fit(X)
        max_idf = max(tfidf.idf_)
        self.word2weight = defaultdict(
            lambda: max_idf,
            [(w, tfidf.idf_[i]) for w, i in tfidf.vocabulary_.items()])

        return self

    def transform(self, X):
        return np.array([
                np.mean([self.word2vec[w] * self.word2weight[w]
                         for w in words if w in self.word2vec] or
                        [np.zeros(self.dim)], axis=0)
                for words in X
            ])
16/42: data_mean = tfidf_vectorizer(w2v).fit(train_df['list_w']).transform(train_df['list_w'])
16/43: get_auc_lr_valid(data_mean, y, C=1, seed=7, ratio = 0.8)
16/44:
from sklearn.linear_model import LogisticRegression
def get_auc_lr_valid(X, y, C=1, seed=7, ratio = 0.8):
    # разделим выборку на обучающую и валидационную
    idx = round(X.shape[0] * ratio)
    # обучение классификатора
    lr = LogisticRegression(C=C, random_state=seed, n_jobs=-1).fit(X[:idx], y[:idx])
    # прогноз для валидационной выборки
    y_pred = lr.predict_proba(X[idx:, :])[:, 1]
    # считаем качество
    score = roc_auc_score(y[idx:], y_pred)

    return score
16/45: get_auc_lr_valid(data_mean, y, C=1, seed=7, ratio = 0.8)
16/46:
import tensorflow as tf

# Check for GPU availability
print("Num GPUs Available:", len(tf.config.experimental.list_physical_devices('GPU')))

# Print GPU information
print(tf.test.gpu_device_name())
17/1:
import tensorflow as tf

# Check for GPU availability
print("Num GPUs Available:", len(tf.config.experimental.list_physical_devices('GPU')))

# Print GPU information
print(tf.test.gpu_device_name())
17/2:
import tensorflow as tf

# Build and train a simple model
model = tf.keras.Sequential([
    tf.keras.layers.Dense(128, activation='relu', input_shape=(784,)),
    tf.keras.layers.Dropout(0.2),
    tf.keras.layers.Dense(10)
])

# Compile the model
model.compile(optimizer='adam',
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])

# Train the model on GPU
(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data()
train_images, test_images = train_images / 255.0, test_images / 255.0
model.fit(train_images, train_labels, epochs=5)
17/3:
import tensorflow as tf

# Load and preprocess the MNIST dataset
(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data()
train_images, test_images = train_images / 255.0, test_images / 255.0

# Flatten the images
train_images = train_images.reshape((train_images.shape[0], -1))
test_images = test_images.reshape((test_images.shape[0], -1))

# Build the model
model = tf.keras.Sequential([
    tf.keras.layers.Dense(128, activation='relu', input_shape=(784,)),
    tf.keras.layers.Dropout(0.2),
    tf.keras.layers.Dense(10)
])

# Compile the model
model.compile(optimizer='adam',
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])

# Train the model
model.fit(train_images, train_labels, epochs=5)
17/4:
import tensorflow as tf

# Check for GPU availability
print("Num GPUs Available:", len(tf.config.experimental.list_physical_devices('GPU')))

# Print GPU information
print(tf.test.gpu_device_name())
17/5: pip install tensorflow-gpu
17/6:
import tensorflow as tf

# Explicitly set GPU configurations
gpus = tf.config.experimental.list_physical_devices('GPU')
if gpus:
    try:
        # Currently, memory growth needs to be the same across GPUs
        for gpu in gpus:
            tf.config.experimental.set_memory_growth(gpu, True)
        logical_gpus = tf.config.experimental.list_logical_devices('GPU')
        print(len(gpus), "Physical GPUs,", len(logical_gpus), "Logical GPUs")
    except RuntimeError as e:
        # Memory growth must be set before GPUs have been initialized
        print(e)
18/1:
import tensorflow as tf

# Check for GPU availability
print("Num GPUs Available:", len(tf.config.experimental.list_physical_devices('GPU')))

# Print GPU information
print(tf.test.gpu_device_name())
18/2:
import tensorflow as tf

# Explicitly set GPU configurations
gpus = tf.config.experimental.list_physical_devices('GPU')
if gpus:
    try:
        # Currently, memory growth needs to be the same across GPUs
        for gpu in gpus:
            tf.config.experimental.set_memory_growth(gpu, True)
        logical_gpus = tf.config.experimental.list_logical_devices('GPU')
        print(len(gpus), "Physical GPUs,", len(logical_gpus), "Logical GPUs")
    except RuntimeError as e:
        # Memory growth must be set before GPUs have been initialized
        print(e)
18/3:
import tensorflow as tf

# Load and preprocess the MNIST dataset
(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data()
train_images, test_images = train_images / 255.0, test_images / 255.0

# Flatten the images
train_images = train_images.reshape((train_images.shape[0], -1))
test_images = test_images.reshape((test_images.shape[0], -1))

# Build the model
model = tf.keras.Sequential([
    tf.keras.layers.Dense(128, activation='relu', input_shape=(784,)),
    tf.keras.layers.Dropout(0.2),
    tf.keras.layers.Dense(10)
])

# Compile the model
model.compile(optimizer='adam',
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])

# Train the model
model.fit(train_images, train_labels, epochs=5)
18/4:
import tensorflow as tf

# Check for GPU availability
print("Num GPUs Available:", len(tf.config.experimental.list_physical_devices('GPU')))

# Print GPU information
print(tf.test.gpu_device_name())
18/5: with tf.device('/device:GPU:0'):
18/6: with tf.device('/device:GPU:0')
18/7: gpus = tf.config.experimental.list_physical_devices('GPU')
18/8:
import tensorflow as tf

# Explicitly set GPU configurations
gpus = tf.config.experimental.list_physical_devices('GPU')
if gpus:
    try:
        # Currently, memory growth needs to be the same across GPUs
        for gpu in gpus:
            tf.config.experimental.set_memory_growth(gpu, True)
        logical_gpus = tf.config.experimental.list_logical_devices('GPU')
        print(len(gpus), "Physical GPUs,", len(logical_gpus), "Logical GPUs")
    except RuntimeError as e:
        # Memory growth must be set before GPUs have been initialized
        print(e)
18/9:
import tensorflow as tf

# Check for GPU availability
print("Num GPUs Available:", len(tf.config.experimental.list_physical_devices('GPU')))

# Print GPU information
print(tf.test.gpu_device_name())
18/10:
from tensorflow.python.client import device_lib 
print(device_lib.list_local_devices())
18/11:
import os
os.environ["CUDA_VISIBLE_DEVICES"] = "-1"
18/12:
import tensorflow as tf

# Check for GPU availability
print("Num GPUs Available:", len(tf.config.experimental.list_physical_devices('GPU')))

# Print GPU information
print(tf.test.gpu_device_name())
18/13: gpus = tf.config.experimental.list_physical_devices('GPU')
18/14:
import tensorflow as tf

# Explicitly set GPU configurations
gpus = tf.config.experimental.list_physical_devices('GPU')
if gpus:
    try:
        # Currently, memory growth needs to be the same across GPUs
        for gpu in gpus:
            tf.config.experimental.set_memory_growth(gpu, True)
        logical_gpus = tf.config.experimental.list_logical_devices('GPU')
        print(len(gpus), "Physical GPUs,", len(logical_gpus), "Logical GPUs")
    except RuntimeError as e:
        # Memory growth must be set before GPUs have been initialized
        print(e)
18/15:
import tensorflow as tf

# Load and preprocess the MNIST dataset
(train_images, train_labels), (test_images, test_labels) = tf.keras.datasets.mnist.load_data()
train_images, test_images = train_images / 255.0, test_images / 255.0

# Flatten the images
train_images = train_images.reshape((train_images.shape[0], -1))
test_images = test_images.reshape((test_images.shape[0], -1))

# Build the model
model = tf.keras.Sequential([
    tf.keras.layers.Dense(128, activation='relu', input_shape=(784,)),
    tf.keras.layers.Dropout(0.2),
    tf.keras.layers.Dense(10)
])

# Compile the model
model.compile(optimizer='adam',
              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
              metrics=['accuracy'])

# Train the model
model.fit(train_images, train_labels, epochs=5)
18/16:
import tensorflow as tf
physical_devices = tf.config.list_physical_devices('GPU')
if len(physical_devices) > 0:
    tf.config.experimental.set_memory_growth(physical_devices[0], True)
    print(f"Num GPUs Available: {len(physical_devices)}")
else:
    print("No GPU devices found.")
18/17:
# загрузим библиотеки и установим опции
from __future__ import division, print_function
# отключим всякие предупреждения Anaconda
import warnings
warnings.filterwarnings('ignore')
#%matplotlib inline
import numpy as np
import pandas as pd
from sklearn.metrics import roc_auc_score
18/18:
# загрузим обучающую и тестовую выборки
train_df = pd.read_csv('train_sessions.csv')#,index_col='session_id')
test_df = pd.read_csv('test_sessions.csv')#, index_col='session_id')

# приведем колонки time1, ..., time10 к временному формату
times = ['time%s' % i for i in range(1, 11)]
train_df[times] = train_df[times].apply(pd.to_datetime)
test_df[times] = test_df[times].apply(pd.to_datetime)

# отсортируем данные по времени
train_df = train_df.sort_values(by='time1')

# посмотрим на заголовок обучающей выборки
train_df.head()
18/19:
sites = ['site%s' % i for i in range(1, 11)]
#заменим nan на 0
train_df[sites] = train_df[sites].fillna(0).astype('int').astype('str')
test_df[sites] = test_df[sites].fillna(0).astype('int').astype('str')
#создадим тексты необходимые для обучения word2vec
train_df['list'] = train_df['site1']
test_df['list'] = test_df['site1']
for s in sites[1:]:
    train_df['list'] = train_df['list']+","+train_df[s]
    test_df['list'] = test_df['list']+","+test_df[s]
train_df['list_w'] = train_df['list'].apply(lambda x: x.split(','))
test_df['list_w'] = test_df['list'].apply(lambda x: x.split(','))
18/20:
#В нашем случае предложение это набор сайтов, которые посещал пользователь
#нам необязательно переводить цифры в названия сайтов, т.к. алгоритм будем выявлять взаимосвязь их друг с другом.
train_df['list_w'][10]
18/21:
# подключим word2vec
from gensim.models import word2vec
18/22:
#объединим обучающую и тестовую выборки и обучим нашу модель на всех данных 
#с размером окна в 6=3*2 (длина предложения 10 слов) и итоговыми векторами размерности 300, параметр workers отвечает за количество ядер
test_df['target'] = -1
data = pd.concat([train_df,test_df], axis=0)

model = word2vec.Word2Vec(data['list_w'], vector_size=300, window=3, workers=4)
#создадим словарь со словами и соответсвующими им векторами
w2v = dict(zip(model.wv.index_to_key, model.wv.vectors))
18/23: w2v
18/24:
class mean_vectorizer(object):
    def __init__(self, word2vec):
        self.word2vec = word2vec
        self.dim = len(next(iter(w2v.values())))

    def fit(self, X):
        return self 

    def transform(self, X):
        return np.array([
            np.mean([self.word2vec[w] for w in words if w in self.word2vec] 
                    or [np.zeros(self.dim)], axis=0)
            for words in X
        ])
18/25:
data_mean=mean_vectorizer(w2v).fit(train_df['list_w']).transform(train_df['list_w'])
data_mean.shape
18/26:
# Воспользуемся валидацией
def split(train,y,ratio):
    idx = round(train.shape[0] * ratio)
    return train[:idx, :], train[idx:, :], y[:idx], y[idx:]
y = train_df['target']
Xtr, Xval, ytr, yval = split(data_mean, y,0.8)
Xtr.shape,Xval.shape,ytr.mean(),yval.mean()
18/27:
# подключим библиотеки keras 
from keras.models import Sequential, Model
from keras.layers import Dense, Dropout, Activation, Input
from keras.preprocessing.text import Tokenizer
from keras import regularizers
18/28:
# опишем нейронную сеть
model = Sequential()
model.add(Dense(128, input_dim=(Xtr.shape[1])))
model.add(Activation('relu'))
model.add(Dropout(0.5))
model.add(Dense(1))
model.add(Activation('sigmoid'))
model.compile(loss='binary_crossentropy',
              optimizer='adam',
              metrics=['binary_accuracy'])
18/29:
history = model.fit(Xtr, ytr,
                    batch_size=128,
                    epochs=10,
                    validation_data=(Xval, yval),
                    
                    verbose=0)
18/30:
classes = model.predict(Xval, batch_size=128)
roc_auc_score(yval, classes)
18/31: import xgboost as xgb
18/32: # !pip install xgboost
18/33:
dtr = xgb.DMatrix(Xtr, label= ytr,missing = np.nan)
dval = xgb.DMatrix(Xval, label= yval,missing = np.nan)
watchlist = [(dtr, 'train'), (dval, 'eval')]
history = dict()
18/34:
params = {
    'max_depth': 26,
    'eta': 0.025,
    'nthread': 4,
    'gamma' : 1,
    'alpha' : 1,
    'subsample': 0.85,
    'eval_metric': ['auc'],
    'objective': 'binary:logistic',
    'colsample_bytree': 0.9,
    'min_child_weight': 100,
    'scale_pos_weight':(1)/y.mean(),
    'seed':7
}
18/35:
Xtrain = pd.read_csv('train_content.csv')
Xtest = pd.read_csv('test_content.csv')
print(Xtrain.shape,Xtest.shape)
Xtrain.head()
20/1:
import tensorflow as tf

# Check for GPU availability
print("Num GPUs Available:", len(tf.config.experimental.list_physical_devices('GPU')))

# Print GPU information
print(tf.test.gpu_device_name())
19/1:
# imports needed and set up logging
import gensim 
import logging

logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO
19/2:
# imports needed and set up logging
import gensim 
import logging

logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)
19/3: !pip install python-Levenshtein
19/4: from natasha import Doc
19/5: pip install natasha
19/6: !pip install natasha
19/7: from natasha import Doc
19/8: [list(tokenize(x.lower())) for x in answers]
19/9: import json
19/10:
with open('bank_faq.json', encoding='utf-8') as f:
    data = json.load(f)
19/11: data
19/12:
answers = []
questions = []
for x in data:
    for k in data[x]:
        for a in data[x][k]:
            questions.append(a['вопрос'])
            answers.append(a['ответ'])
19/13: questions
19/14: answers
19/15: [list(tokenize(x.lower())) for x in answers]
19/16: from razdel import tokenize
19/17: [list(tokenize(x.lower())) for x in answers]
19/18: [list(tokenize(x.lower())) for x in questions]
19/19: from natasha import Doc
19/20:
from natasha import (
    Segmenter,
    MorphVocab,
    
    Doc
)
19/21:
for d in answers:
    print(d)
19/22:
for d in answers:
    doc = Doc(d)
    print(doc.tokens)
19/23:
for d in answers:
    doc = Doc(d)
    print(doc)
    print(doc.tokens)
19/24:
for d in answers:
    doc = Doc(d)
    doc.segment(segmenter)
    print(doc)
    print(doc.tokens)
19/25: segmenter = Segmenter()
19/26:
for d in answers:
    doc = Doc(d)
    doc.segment(segmenter)
    print(doc)
    print(doc.tokens)
19/27:
for d in answers:
    doc = Doc(d)
    doc.segment(segmenter)
#     print(doc)
    print(doc.tokens)
19/28:
for d in answers:
    doc = Doc(d)
    doc.segment(segmenter)
#     print(doc)
    print(doc.tokens.text)
19/29:
for d in answers:
    doc = Doc(d)
    doc.segment(segmenter)
#     print(doc)
    print(doc.tokens.text())
19/30:
for d in answers:
    doc = Doc(d)
    doc.segment(segmenter)
#     print(doc)
    print(doc.tokens.text
19/31:
for d in answers:
    doc = Doc(d)
    doc.segment(segmenter)
#     print(doc)
    print(doc.tokens.text)
19/32:
for d in answers:
    doc = Doc(d)
    doc.segment(segmenter)
#     print(doc)
    print(doc.tokens)
19/33:
i = 0
dd = 0
for d in answers:
   
    doc = Doc(d)
    doc.segment(segmenter)
#     print(doc)
    if i == 0:
        dd = doc[0]
    i += 1
#     print(doc.tokens)
print(dd)
19/34:
i = 0
dd = 0
for d in answers:
   
    doc = Doc(d)
    doc.segment(segmenter)
#     print(doc)
    if i == 0:
        dd = doc.tokens[0]
    i += 1
#     print(doc.tokens)
print(dd)
19/35:
i = 0
dd = 0
for d in answers:
   
    doc = Doc(d)
    doc.segment(segmenter)
#     print(doc)
    if i == 0:
        dd = doc.tokens[0]
    i += 1
#     print(doc.tokens)
print(dd.text)
19/36:
i = 0
dd = 0
ans = []
que = []
for d in answers:
   
    doc = Doc(d)
    doc.segment(segmenter)
    
    print(doc.tokens)
19/37:
i = 0
dd = 0
ans = []
que = []
for d in answers:
   
    doc = Doc(d)
    doc.segment(segmenter)
    
    for x in doc.tokens:
        print(x)
19/38: pip install pymorphy2
19/39:
import pymorphy2
morph = pymorphy2.MorphAnalyzer()
19/40: morph.parse('стали')
19/41: morph.parse('буквы')
19/42:
i = 0
dd = 0
ans = []
que = []
for d in answers:
    print(d)
19/43:
i = 0
dd = 0
ans = []
que = []
for d in answers:
    ans.append(morph.parse(d))
print(ans)
19/44:
i = 0
dd = 0
ans = []
que = []
for sentence in answers:
    for letter in sentence:
        ans.append(morph.parse(letter))
print(ans)
19/45:
i = 0
dd = 0
ans = []
que = []
for sentence in answers:
    for letter in sentence:
#         ans.append(morph.parse(letter))
        print(morph.parse(letter))
print(ans)
19/46:
i = 0
dd = 0
ans = []
que = []
for sentence in answers:
#     for letter in sentence:
#         ans.append(morph.parse(letter))
    print(morph.parse(sentence))
print(ans)
19/47:
i = 0
dd = 0
ans = []
que = []
for sentence in answers:
#     for letter in sentence:
#         ans.append(morph.parse(letter))
#     print(morph.parse(sentence))
    print(sentence)
print(ans)
19/48:
i = 0
dd = 0
ans = []
que = []
answers
# for sentence in answers:
#     for letter in sentence:
#         ans.append(morph.parse(letter))
#     print(morph.parse(sentence))
print(ans)
19/49:
answers = []
questions = []
for x in data:
    for k in data[x]:
        for a in data[x][k]:
            questions.append(a['вопрос'])
            answers.append(a['ответ'])
19/50: questions
19/51: ans_tok = [list(tokenize(x.lower())) for x in answers]
19/52: ques_tok = [list(tokenize(x.lower())) for x in questions]
19/53:
# pip install pymorphy2
# /
ans_tok
19/54:
i = 0
dd = 0
ans = []
que = []
answers
for sentence in ans_tok:
    print(sentence.text)
#     for letter in sentence:
#         ans.append(morph.parse(letter))
#     print(morph.parse(sentence))
print(ans)
19/55:
i = 0
dd = 0
ans = []
que = []
answers
for sentence in ans_tok:
    print(sentence)
#     for letter in sentence:
#         ans.append(morph.parse(letter))
#     print(morph.parse(sentence))
print(ans)
19/56:
i = 0
dd = 0
ans = []
que = []
answers
for sentence in ans_tok:
    for word in sentence:
        print(word)
#     for letter in sentence:
#         ans.append(morph.parse(letter))
#     print(morph.parse(sentence))
print(ans)
19/57:
i = 0
dd = 0
ans = []
que = []
answers
for sentence in ans_tok:
    for word in sentence:
        print(word.text)
#     for letter in sentence:
#         ans.append(morph.parse(letter))
#     print(morph.parse(sentence))
print(ans)
19/58:
i = 0
dd = 0
ans = []
que = []
answers
for sentence in ans_tok:
    for word in sentence:
        ans.append(morph.parse(word))
#     for letter in sentence:
#         ans.append(morph.parse(letter))
#     print(morph.parse(sentence))
print(ans)
19/59: ans_tok = [list(tokenize(x.lower())) for x in answers].copy()
19/60: ques_tok = [list(tokenize(x.lower())) for x in questions].copy()
19/61:
i = 0
dd = 0
ans = []
que = []
answers
for sentence in ans_tok:
    for word in sentence:
        ans.append(morph.parse(word))
#     for letter in sentence:
#         ans.append(morph.parse(letter))
#     print(morph.parse(sentence))
print(ans)
19/62:
i = 0
dd = 0
ans = []
que = []
answers
for sentence in ans_tok:
    for word in sentence:
        print(word)
#         ans.append(morph.parse(word))
#     for letter in sentence:
#         ans.append(morph.parse(letter))
#     print(morph.parse(sentence))
print(ans)
19/63:
i = 0
dd = 0
ans = []
que = []
answers
for sentence in ans_tok:
    for word in sentence:
        print(word.text)
#         ans.append(morph.parse(word))
#     for letter in sentence:
#         ans.append(morph.parse(letter))
#     print(morph.parse(sentence))
print(ans)
19/64:
i = 0
dd = 0
ans = []
que = []
answers
for sentence in ans_tok:
    for word in sentence:
#         print(word.text)
        ans.append(morph.parse(word.text))
#     for letter in sentence:
#         ans.append(morph.parse(letter))
#     print(morph.parse(sentence))
print(ans)
19/65:
i = 0
dd = 0
ans = []
que = []
answers
for sentence in ans_tok:
    for word in sentence:
        print(word.text)
#         ans.append(morph.parse(word.text))
#     for letter in sentence:
#         ans.append(morph.parse(letter))
#     print(morph.parse(sentence))
print(ans)
19/66:
i = 0
dd = 0
ans = []
que = []
answers
for sentence in ans_tok:
    for word in sentence:
        print(word.text)
#         ans.append(morph.parse(word.text))
#     for letter in sentence:
#         ans.append(morph.parse(letter))
#     print(morph.parse(sentence))
# print(ans)
19/67:
i = 0
dd = 0
ans = []
que = []
answers
for sentence in ans_tok:
    for word in sentence:
        for text in word.text:
            print(text)
#         ans.append(morph.parse(word.text))
#     for letter in sentence:
#         ans.append(morph.parse(letter))
#     print(morph.parse(sentence))
# print(ans)
19/68:
i = 0
dd = 0
ans = []
que = []
answers
for sentence in ans_tok:
    for word in sentence:
        for text in word:
            print(text)
#         ans.append(morph.parse(word.text))
#     for letter in sentence:
#         ans.append(morph.parse(letter))
#     print(morph.parse(sentence))
# print(ans)
19/69:
i = 0
dd = 0
ans = []
que = []
answers
for sentence in ans_tok:
    for word in sentence:
        print(word.text)
#         ans.append(morph.parse(word.text))
#     for letter in sentence:
#         ans.append(morph.parse(letter))
#     print(morph.parse(sentence))
# print(ans)
19/70:
i = 0
dd = 0
ans = []
que = []
answers
for sentence in ans_tok:
    for word in sentence:
        print(morph.parse(word.text))
#         ans.append(morph.parse(word.text))
#     for letter in sentence:
#         ans.append(morph.parse(letter))
#     print(morph.parse(sentence))
# print(ans)
19/71:
i = 0
dd = 0
ans = []
que = []
answers
for sentence in ans_tok:
    for word in sentence:
        m = morph.parse(word.text)[0]
        ans.append(m.normal_form)
#         ans.append(morph.parse(word.text))
#     for letter in sentence:
#         ans.append(morph.parse(letter))
#     print(morph.parse(sentence))
print(ans)
19/72:
i = 0
dd = 0
ans = []
que = []
answers
for sentence in ans_tok:
    temp = []
    for word in sentence:
        m = morph.parse(word.text)[0]
        temp.append(m.normal_form)
    ans.append(temp.copy())
#         ans.append(morph.parse(word.text))
#     for letter in sentence:
#         ans.append(morph.parse(letter))
#     print(morph.parse(sentence))
print(ans)
19/73:
i = 0
dd = 0
ans = []
que = []
answers
for sentence in ans_tok:
    temp = []
    for word in sentence:
        m = morph.parse(word.text)[0]
        temp.append(m.normal_form)
    ans.append(temp.copy())

for sentence in ques_tok:
    temp = []
    for word in sentence:
        m = morph.parse(word.text)[0]
        temp.append(m.normal_form)
    que.append(temp.copy())

print(ans)
print(que)
19/74:
data_file="reviews_data.txt.gz"

with gzip.open ('reviews_data.txt.gz', 'rb') as f:
    for i,line in enumerate (f):
        print(line)
        break
19/75:
import gzip
data_file="reviews_data.txt.gz"

with gzip.open ('reviews_data.txt.gz', 'rb') as f:
    for i,line in enumerate (f):
        print(line)
        break
19/76: answers
19/77:  gensim.utils.simple_preprocess (answer[0])
19/78:  gensim.utils.simple_preprocess (answers[0])
19/79:  gensim.utils.simple_preprocess(answers[0])
19/80:  gensim.utils.simple_preprocess(ans[0])
19/81:  gensim.utils.simple_preprocess(answers[0])
19/82: ''.join(ans[0][0])
19/83: ''.join(ans[0])
19/84: ' '.join(ans[0])
19/85: sss = ' '.join(ans[0])
19/86:  gensim.utils.simple_preprocess(sss)
19/87:
i = 0
dd = 0
ans = []
que = []
answers
for sentence in ans_tok:
    temp = []
    for word in sentence:
        m = morph.parse(word.text)[0]
        temp.append(m.normal_form)
    ans.append(temp.copy())

for sentence in ques_tok:
    temp = []
    for word in sentence:
        m = morph.parse(word.text)[0]
        temp.append(m.normal_form)
    sss = ' '.join(temp)
    que.append(sss)

print(ans)
print(que)
19/88: sss = ' '.join(ans[0])
19/89:
i = 0
dd = 0
ans = []
que = []
answers
for sentence in ans_tok:
    temp = []
    for word in sentence:
        m = morph.parse(word.text)[0]
        temp.append(m.normal_form)
    ans.append(temp.copy())

for sentence in ques_tok:
    temp = []
    for word in sentence:
        m = morph.parse(word.text)[0]
        temp.append(m.normal_form)
    sss = ' '.join(temp)
    tt = gensim.utils.simple_preprocess(sss)
    que.append(tt)

print(ans)
print(que)
19/90:
i = 0
dd = 0
ans = []
que = []
answers
for sentence in ans_tok:
    temp = []
    for word in sentence:
        m = morph.parse(word.text)[0]
        temp.append(m.normal_form)
    ans.append(temp.copy())

for sentence in ques_tok:
    temp = []
    for word in sentence:
        m = morph.parse(word.text)[0]
        temp.append(m.normal_form)
#     sss = ' '.join(temp)
#     tt = gensim.utils.simple_preprocess(sss)
#     que.append(tt)
    que.append(temp)

print(ans)
print(que)
19/91:
i = 0
dd = 0
ans = []
que = []
answers
for sentence in ans_tok:
    temp = []
    for word in sentence:
        m = morph.parse(word.text)[0]
        temp.append(m.normal_form)
    sss = ' '.join(temp)
    tt = gensim.utils.simple_preprocess(sss)
    que.append(tt)
    ans.append(sss)

for sentence in ques_tok:
    temp = []
    for word in sentence:
        m = morph.parse(word.text)[0]
        temp.append(m.normal_form)
    sss = ' '.join(temp)
    tt = gensim.utils.simple_preprocess(sss)
    que.append(tt)
#     que.append(temp)

print(ans)
print(que)
19/92:
i = 0
dd = 0
ans = []
que = []
answers
for sentence in ans_tok:
    temp = []
    for word in sentence:
        m = morph.parse(word.text)[0]
        temp.append(m.normal_form)
    sss = ' '.join(temp)
    tt = gensim.utils.simple_preprocess(sss)
    ans.append(sss)

for sentence in ques_tok:
    temp = []
    for word in sentence:
        m = morph.parse(word.text)[0]
        temp.append(m.normal_form)
    sss = ' '.join(temp)
    tt = gensim.utils.simple_preprocess(sss)
    que.append(tt)
#     que.append(temp)

print(ans)
print(que)
19/93:
i = 0
dd = 0
ans = []
que = []
answers
for sentence in ans_tok:
    temp = []
    for word in sentence:
        m = morph.parse(word.text)[0]
        temp.append(m.normal_form)
    sss = ' '.join(temp)
    print(sss)
    tt = gensim.utils.simple_preprocess(sss)
    ans.append(sss)

for sentence in ques_tok:
    temp = []
    for word in sentence:
        m = morph.parse(word.text)[0]
        temp.append(m.normal_form)
    sss = ' '.join(temp)
    tt = gensim.utils.simple_preprocess(sss)
    que.append(tt)
#     que.append(temp)

# print(ans)
# print(que)
19/94:
i = 0
dd = 0
ans = []
que = []
answers
for sentence in ans_tok:
    temp = []
    for word in sentence:
        m = morph.parse(word.text)[0]
        temp.append(m.normal_form)
    sss = ' '.join(temp)
    tt = gensim.utils.simple_preprocess(sss)
    ans.append(tt)

for sentence in ques_tok:
    temp = []
    for word in sentence:
        m = morph.parse(word.text)[0]
        temp.append(m.normal_form)
    sss = ' '.join(temp)
    tt = gensim.utils.simple_preprocess(sss)
    que.append(tt)
#     que.append(temp)

# print(ans)
# print(que)
19/95:
i = 0
dd = 0
ans = []
que = []
answers
for sentence in ans_tok:
    temp = []
    for word in sentence:
        m = morph.parse(word.text)[0]
        temp.append(m.normal_form)
    sss = ' '.join(temp)
    tt = gensim.utils.simple_preprocess(sss)
    ans.append(tt)

for sentence in ques_tok:
    temp = []
    for word in sentence:
        m = morph.parse(word.text)[0]
        temp.append(m.normal_form)
    sss = ' '.join(temp)
    tt = gensim.utils.simple_preprocess(sss)
    que.append(tt)
#     que.append(temp)

print(ans)
print(que)
19/96:
model = gensim.models.Word2Vec (ans, size=150, window=10, min_count=2, workers=10)
model.train(ans,total_examples=len(ans),epochs=10)
19/97:
model = gensim.models.Word2Vec (ans, vector_size=150, window=10, min_count=2, workers=10)
model.train(ans,total_examples=len(ans),epochs=10)
19/98: model.wv.most_similar (positive='карта')
19/99: model.wv.most_similar (positive='банк')
19/100: model.wv.most_similar (positive='деньги')
19/101: model.wv.most_similar (positive='деньга')
19/102:
model_que = gensim.models.Word2Vec (ans, vector_size=150, window=10, min_count=2, workers=10)
model_que.train(ans,total_examples=len(ans),epochs=10)
19/103:
model_que = gensim.models.Word2Vec (que, vector_size=150, window=10, min_count=2, workers=10)
model_que.train(ans,total_examples=len(que),epochs=10)
19/104: model_ans.wv.most_similar (positive='деньга')
19/105:
model_ans = gensim.models.Word2Vec (ans, vector_size=150, window=10, min_count=2, workers=10)
model_ans.train(ans,total_examples=len(ans),epochs=10)
19/106: model_ans.wv.most_similar (positive='деньга')
19/107: model_que.wv.most_similar (positive='деньга')
19/108: import gensim.downloader as api
21/1:
print('PyDev console: using IPython 8.15.0\n')

import sys; print('Python %s on %s' % (sys.version, sys.platform))
sys.path.extend(['C:\\Users\\aliha\\Desktop\\NLP'])
21/2: pip install pyTelegramBotAPI
19/109: model_que.wv.most_similar (positive='карта')
19/110:
import nltk
from nltk.corpus import stopwords
sw_nltk = stopwords.words('english')
print(sw_nltk
19/111:
import nltk
from nltk.corpus import stopwords
sw_nltk = stopwords.words('english')
print(sw_nltk()
19/112:
import nltk
from nltk.corpus import stopwords
sw_nltk = stopwords.words('english')
print(sw_nltk
19/113:
import nltk
from nltk.corpus import stopwords
sw_nltk = stopwords.words('english')
print(sw_nltk)
19/114:
import nltk
nltk.download('stopwords')
# from nltk.corpus import stopwords
# sw_nltk = stopwords.words('english')
# print(sw_nltk)
19/115:
import nltk
# nltk.download('stopwords')
from nltk.corpus import stopwords
# sw_nltk = stopwords.words('english')
# print(sw_nltk)
19/116:
import nltk
# nltk.download('stopwords')
from nltk.corpus import stopwords
sw_nltk = stopwords.words('english')
# print(sw_nltk)
19/117:
import nltk
# nltk.download('stopwords')
from nltk.corpus import stopwords
sw_nltk = stopwords.words('english')
print(sw_nltk)
19/118:
import nltk
# nltk.download('stopwords')
from nltk.corpus import stopwords
sw_nltk = stopwords.words('russian')
print(sw_nltk)
19/119:
import nltk
from nltk.corpus import stopwords
sw_nltk = stopwords.words('russian')
# ans = []
# que = []
print(ans)
19/120: [[w for w in ans[x] if w.lower() not in ] for x in range(len(ans))]
19/121: [[w for w in ans[x] if w.lower() not in sw_nltk] for x in range(len(ans))]
19/122:
ans = [[w for w in ans[x] if w.lower() not in sw_nltk] for x in range(len(ans))]
que = [[w for w in que[x] if w.lower() not in sw_nltk] for x in range(len(que))]
19/123:
model_ans = gensim.models.Word2Vec (ans, vector_size=150, window=10, min_count=2, workers=10)
model_ans.train(ans,total_examples=len(ans),epochs=10)
19/124: model_ans.wv.most_similar (positive='деньга')
19/125:
model_que = gensim.models.Word2Vec (que, vector_size=150, window=10, min_count=2, workers=10)
model_que.train(ans,total_examples=len(que),epochs=10)
19/126: model_que.wv.most_similar (positive='карта')
19/127:
ans = [[w for w in ans[x] if w not in sw_nltk] for x in range(len(ans))]
que = [[w for w in que[x] if w not in sw_nltk] for x in range(len(que))]
19/128:
import nltk
from nltk.corpus import stopwords
sw_nltk = stopwords.words('russian')
# ans = []
# que = []
# [[w for w in ans[x] if w.lower() not in ] for x in range(len(ans))]
print(sw_nltk)
19/129: sw = "c а алло без белый близко более больше большой будем будет будете будешь будто буду будут будь бы бывает бывь был была были было быть в важная важное важные важный вам вами вас ваш ваша ваше ваши вверх вдали вдруг ведь везде вернуться весь вечер взгляд взять вид видел видеть вместе вне вниз внизу во вода война вокруг вон вообще вопрос восемнадцатый восемнадцать восемь восьмой вот впрочем времени время все все еще всегда всего всем всеми всему всех всею всю всюду вся всё второй вы выйти г где главный глаз говорил говорит говорить год года году голова голос город да давать давно даже далекий далеко дальше даром дать два двадцатый двадцать две двенадцатый двенадцать дверь двух девятнадцатый девятнадцать девятый девять действительно дел делал делать делаю дело день деньги десятый десять для до довольно долго должен должно должный дом дорога друг другая другие других друго другое другой думать душа е его ее ей ему если есть еще ещё ею её ж ждать же жена женщина жизнь жить за занят занята занято заняты затем зато зачем здесь земля знать значит значить и иди идти из или им имеет имел именно иметь ими имя иногда их к каждая каждое каждые каждый кажется казаться как какая какой кем книга когда кого ком комната кому конец конечно которая которого которой которые который которых кроме кругом кто куда лежать лет ли лицо лишь лучше любить люди м маленький мало мать машина между меля менее меньше меня место миллионов мимо минута мир мира мне много многочисленная многочисленное многочисленные многочисленный мной мною мог могу могут мож может может быть можно можхо мои мой мор москва мочь моя моё мы на наверху над надо назад наиболее найти наконец нам нами народ нас начала начать наш наша наше наши не него недавно недалеко нее ней некоторый нельзя нем немного нему непрерывно нередко несколько нет нею неё ни нибудь ниже низко никакой никогда никто никуда ним ними них ничего ничто но новый нога ночь ну нужно нужный нх о об оба обычно один одиннадцатый одиннадцать однажды однако одного одной оказаться окно около он она они оно опять особенно остаться от ответить отец откуда отовсюду отсюда очень первый перед писать плечо по под подойди подумать пожалуйста позже пойти пока пол получить помнить понимать понять пор пора после последний посмотреть посреди потом потому почему почти правда прекрасно при про просто против процентов путь пятнадцатый пятнадцать пятый пять работа работать раз разве рано раньше ребенок решить россия рука русский ряд рядом с с кем сам сама сами самим самими самих само самого самой самом самому саму самый свет свое своего своей свои своих свой свою сделать сеаой себе себя сегодня седьмой сейчас семнадцатый семнадцать семь сидеть сила сих сказал сказала сказать сколько слишком слово случай смотреть сначала снова со собой собою советский совсем спасибо спросить сразу стал старый стать стол сторона стоять страна суть считать т та так такая также таки такие такое такой там твои твой твоя твоё те тебе тебя тем теми теперь тех то тобой тобою товарищ тогда того тоже только том тому тот тою третий три тринадцатый тринадцать ту туда тут ты тысяч у увидеть уж уже улица уметь утро хороший хорошо хотел бы хотеть хоть хотя хочешь час часто часть чаще чего человек чем чему через четвертый четыре четырнадцатый четырнадцать что чтоб чтобы чуть шестнадцатый шестнадцать шестой шесть эта эти этим этими этих это этого этой этом этому этот эту я являюсь".split()
19/130: sw
19/131: que
19/132:
model_que = gensim.models.Word2Vec (que, vector_size=150, window=10, min_count=2, workers=10)
model_que.train(que,total_examples=len(que),epochs=10)
19/133: model_que.wv.most_similar (positive='карта')
19/134:
model_que = gensim.models.Word2Vec (que, vector_size=300, window=10, min_count=2, workers=10)
model_que.train(que,total_examples=len(que),epochs=10)
19/135: model_que.wv.most_similar (positive='карта')
19/136:
model_que = gensim.models.Word2Vec (que, vector_size=300, window=5, min_count=2, workers=10)
model_que.train(que,total_examples=len(que),epochs=10)
19/137: model_que.wv.most_similar (positive='карта')
19/138:
model_que = gensim.models.Word2Vec (que, vector_size=300, window=15, min_count=2, workers=10)
model_que.train(que,total_examples=len(que),epochs=10)
19/139: model_que.wv.most_similar (positive='карта')
19/140:
model_que = gensim.models.Word2Vec (que, vector_size=200, window=12, min_count=2, workers=10)
model_que.train(que,total_examples=len(que),epochs=10)
19/141: model_que.wv.most_similar (positive='карта')
19/142:
model_que = gensim.models.Word2Vec (que, vector_size=400, window=12, min_count=2, workers=10)
model_que.train(que,total_examples=len(que),epochs=10)
19/143: model_que.wv.most_similar (positive='карта')
19/144:
model_que = gensim.models.Word2Vec (que, vector_size=500, window=12, min_count=2, workers=10)
model_que.train(que,total_examples=len(que),epochs=10)
19/145: model_que.wv.most_similar (positive='карта')
19/146:
model_que = gensim.models.Word2Vec (que, vector_size=500, window=5, min_count=2, workers=10)
model_que.train(que,total_examples=len(que),epochs=10)
19/147: model_que.wv.most_similar (positive='карта')
19/148:
model_que = gensim.models.Word2Vec (que, vector_size=100, window=5, min_count=2, workers=10)
model_que.train(que,total_examples=len(que),epochs=10)
19/149: model_que.wv.most_similar (positive='карта')
19/150:
model_que = gensim.models.Word2Vec (que, vector_size=50, window=5, min_count=2, workers=10)
model_que.train(que,total_examples=len(que),epochs=10)
19/151: model_que.wv.most_similar (positive='карта')
19/152:
model_que = gensim.models.Word2Vec (que, vector_size=100, window=5, min_count=2, workers=10)
model_que.train(que,total_examples=len(que),epochs=10)
19/153: model_que.wv.most_similar (positive='карта')
19/154:
model_que = gensim.models.Word2Vec (que, vector_size=100, window=3, min_count=2, workers=10)
model_que.train(que,total_examples=len(que),epochs=10)
19/155:
model_que = gensim.models.Word2Vec (que, vector_size=100, window=1, min_count=2, workers=10)
model_que.train(que,total_examples=len(que),epochs=10)
19/156: model_que.wv.most_similar (positive='карта')
19/157:
model_que = gensim.models.Word2Vec (que, vector_size=100, window=5, min_count=2, workers=10)
model_que.train(que,total_examples=len(que),epochs=10)
19/158:
model_que = gensim.models.Word2Vec (que, vector_size=100, window=10, min_count=2, workers=10)
model_que.train(que,total_examples=len(que),epochs=10)
19/159: model_que.wv.most_similar (positive='карта')
19/160: model_que
19/161: model_que['карта']
19/162: model_que.vocab
19/163: model_que.vocab()
19/164: model_que.wv['карта']
19/165: import numpy as np
19/166: np.dot(model_que.wv['карта'], model_que.wv['депозит'])
19/167: np.dot(model_que.wv['карта'], model_que.wv['депозит'])/(np.sqrt(np.sum(model_que.wv['карта']**2))*np.sqrt(np.sum(model_que.wv['депозит']**2)))
19/168: model_que.wv['карта']
19/169:
# mean_ans = ans[0]
ans[0]
19/170:
mean_ans = 0
sm = 0
for a in ans[0]:
    sm += model_que.wv[a]
print(sm)
19/171:
mean_ans = 0
sm = 0
for a in ans[0]:
    sm += model_ans.wv[a]
print(sm)
19/172:
mean_ans = 0
sm = 0
for a in ans[0]:
    sm += model_ans.wv[a]
print(sm/len(ans[0]))
19/173:
sm = 0
for a in ans[0]:
    sm += model_ans.wv[a]
mean_ans = sm/len(ans[0])

sm = 0
for a in que[0]:
    sm += model_que.wv[a]
mean_que = sm/len(que[0])
19/174:
sm = 0
for a in ans[0]:
    sm += model_ans.wv[a]
mean_ans = sm/len(ans[0])

sm = 0
for a in que[0]:
    sm += model_que.wv[a]
mean_que = sm/len(que[0])

print(np.dot(mean_ans, mean_que)/(np.sqrt(np.sum(mean_ans**2)))*(np.sqrt(np.sum(mean_que**2))))
19/175:
sm = 0
for a in ans[0]:
    sm += model_ans.wv[a]
mean_ans = sm/len(ans[0])

sm = 0
for a in que[0]:
    sm += model_que.wv[a]
mean_que = sm/len(que[0])
mean_ans.shape
# print(np.dot(mean_ans, mean_que)/(np.sqrt(np.sum(mean_ans**2)))*(np.sqrt(np.sum(mean_que**2))))
19/176:
sm = 0
for a in ans[0]:
    sm += model_ans.wv[a]
mean_ans = sm/len(ans[0])

sm = 0
for a in que[0]:
    sm += model_que.wv[a]
mean_que = sm/len(que[0])
mean_que.shape
# print(np.dot(mean_ans, mean_que)/(np.sqrt(np.sum(mean_ans**2)))*(np.sqrt(np.sum(mean_que**2))))
19/177:
model_ans = gensim.models.Word2Vec (ans, vector_size=100, window=10, min_count=2, workers=10)
model_ans.train(ans,total_examples=len(ans),epochs=10)
19/178: model_ans.wv.most_similar (positive='деньга')
19/179:
sm = 0
for a in ans[0]:
    sm += model_ans.wv[a]
mean_ans = sm/len(ans[0])

sm = 0
for a in que[0]:
    sm += model_que.wv[a]
mean_que = sm/len(que[0])
# mean_que.shape
print(np.dot(mean_ans, mean_que)/(np.sqrt(np.sum(mean_ans**2)))*(np.sqrt(np.sum(mean_que**2))))
19/180:
sm = 0
for a in ans[0]:
    sm += model_ans.wv[a]
mean_ans = sm/len(ans[0])

sm = 0
for q in que[0]:
    sm += model_que.wv[q]
mean_que = sm/len(que[0])
# mean_que.shape
print(np.dot(mean_ans, mean_que)/np.sqrt(np.sum(mean_ans**2)*np.sum(mean_que**2))
19/181:
sm = 0
for a in ans[0]:
    sm += model_ans.wv[a]
mean_ans = sm/len(ans[0])

sm = 0
for q in que[0]:
    sm += model_que.wv[q]
mean_que = sm/len(que[0])
# mean_que.shape
print(np.dot(mean_ans, mean_que)/np.sqrt(np.sum(mean_ans**2)*np.sum(mean_que**2))
19/182:
sm = 0
for a in ans[0]:
    sm += model_ans.wv[a]
mean_ans = sm/len(ans[0])

sm = 0
for q in que[0]:
    sm += model_que.wv[q]
mean_que = sm/len(que[0])
# mean_que.shape
print(np.dot(mean_ans, mean_que)/np.sqrt(np.sum(mean_ans**2)*np.sum(mean_que**2))
19/183:
sm = 0
for a in ans[0]:
    sm += model_ans.wv[a]
mean_ans = sm/len(ans[0])

sm = 0
for q in que[0]:
    sm += model_que.wv[q]
mean_que = sm/len(que[0])
# mean_que.shape
print(np.dot(mean_ans, mean_que)/(np.sqrt(np.sum(mean_ans**2)*np.sum(mean_que**2)))
19/184:
sm = 0
for a in ans[0]:
    sm += model_ans.wv[a]
mean_ans = sm/len(ans[0])

sm = 0
for q in que[0]:
    sm += model_que.wv[q]
mean_que = sm/len(que[0])
# mean_que.shape
print(np.dot(mean_ans, mean_que)/(np.sqrt(np.sum(mean_ans**2)*np.sum(mean_que**2)))
19/185:
sm = 0
for a in ans[0]:
    sm += model_ans.wv[a]
mean_ans = sm/len(ans[0])

sm = 0
for q in que[0]:
    sm += model_que.wv[q]
mean_que = sm/len(que[0])
# mean_que.shape
print(np.dot(mean_ans, mean_que)/(np.sqrt(np.sum(mean_ans**2)*np.sum(mean_que**2)))
19/186:
sm = 0
for a in ans[0]:
    sm += model_ans.wv[a]
mean_ans = sm/len(ans[0])

sm = 0
for q in que[0]:
    sm += model_que.wv[q]
mean_que = sm/len(que[0])
# mean_que.shape
print(np.dot(mean_ans, mean_que)/np.sqrt(np.sum(mean_ans**2) * np.sum(mean_que**2))
19/187: np.dot(model_que.wv['карта'], model_que.wv['депозит'])/(np.sqrt(np.sum(model_que.wv['карта']**2))*np.sqrt(np.sum(model_que.wv['депозит']**2)))
19/188: model_que.wv.most_similar (positive='карта')
19/189:
sm = 0
for a in ans[0]:
    sm += model_ans.wv[a]
mean_ans = sm/len(ans[0])

sm = 0
for q in que[0]:
    sm += model_que.wv[q]
mean_que = sm/len(que[0])
# mean_que.shape
# print(np.dot(mean_ans, mean_que)/np.sqrt(np.sum(mean_ans**2) * np.sum(mean_que**2))
np.sum(mean_que**2)
19/190:
sm = 0
for a in ans[0]:
    sm += model_ans.wv[a]
mean_ans = sm/len(ans[0])

sm = 0
for q in que[0]:
    sm += model_que.wv[q]
mean_que = sm/len(que[0])
# mean_que.shape
# print(np.dot(mean_ans, mean_que)/np.sqrt(np.sum(mean_ans**2) * np.sum(mean_que**2))
# np.sum(mean_que**2)
np.sum(mean_ans**2)
19/191:
sm = 0
for a in ans[0]:
    sm += model_ans.wv[a]
mean_ans = sm/len(ans[0])

sm = 0
for q in que[0]:
    sm += model_que.wv[q]
mean_que = sm/len(que[0])
# mean_que.shape
print(np.dot(mean_ans, mean_que)/np.sqrt(np.sum(mean_ans**2) * np.sum(mean_que**2))
19/192:
sm = 0
for a in ans[0]:
    sm += model_ans.wv[a]
mean_ans = sm/len(ans[0])

sm = 0
for q in que[0]:
    sm += model_que.wv[q]
mean_que = sm/len(que[0])
# mean_que.shape
print(np.dot(mean_ans, mean_que)/np.sqrt(np.sum(mean_ans**2) * np.sqrt(np.sum(mean_que**2)))
19/193:
sm = 0
for a in ans[0]:
    sm += model_ans.wv[a]
mean_ans = sm/len(ans[0])

sm = 0
for q in que[0]:
    sm += model_que.wv[q]
mean_que = sm/len(que[0])
# mean_que.shape
print(np.dot(mean_ans, mean_que)/np.sqrt(np.sum(mean_ans**2) * np.sqrt(np.sum(mean_que**2))))
19/194:
sm = 0
for a in ans[0]:
    sm += model_ans.wv[a]
mean_ans = sm/len(ans[0])

sm = 0
for q in que[0]:
    sm += model_que.wv[q]
mean_que = sm/len(que[0])
# mean_que.shape
print(np.dot(mean_ans, mean_que)/np.sqrt(np.sum(mean_ans**2) * np.sum(mean_que**2)))
19/195:
sm = 0
for a in ans[0]:
    sm += model_ans.wv[a]
mean_ans = sm/len(ans[0])

sm = 0
for q in que[0]:
    sm += model_que.wv[q]
mean_que = sm/len(que[0])
# mean_que.shape
print(np.dot(mean_ans, mean_que)/np.sqrt(np.sum(mean_ans**2) * np.sum(mean_que**2)))
np.dot(mean_ans, mean_que)
19/196: np.dot(model_que.wv['карта'], model_que.wv['депозит'])/np.sqrt(np.sum(model_que.wv['карта']**2)*np.sum(model_que.wv['депозит']**2))
19/197:
sm = 0
for a in ans[0]:
    sm += model_ans.wv[a]
mean_ans = sm/len(ans[0])

sm = 0
for q in que[0]:
    sm += model_que.wv[q]
mean_que = sm/len(que[0])
# mean_que.shape
print(np.dot(mean_ans, mean_que)/np.sqrt(np.sum(mean_ans**2) * np.sum(mean_que**2)))
np.dot(mean_ans, mean_que)
19/198:
sm = 0
for a in ans[0]:
    sm += model_ans.wv[a]
mean_ans = sm/len(ans[0])

sm = 0
for q in que[0]:
    sm += model_que.wv[q]
mean_que = sm/len(que[0])
# mean_que.shape
print(np.dot(mean_ans, mean_que)/np.sqrt(np.sum(mean_ans**2) * np.sum(mean_que**2)))
# np.dot(mean_ans, mean_que)
19/199:
def metric(v1, v2):
    return np.dot(v1, v2)/np.sqrt(np.sum(v1**2)*np.sum(v2**2))
19/200: rand = random.randint(0, len(que))
19/201: rand = random.randint(0, len(que))
19/202:
import random
rand = random.randint(0, len(que))
19/203:
import random
rand = random.randint(0, len(que))
print(rand)
19/204:
import random
rand = random.randint(0, len(que))
print(rand)
for q in que[rand]:
    sm += model_que.wv[q]
mean_que = sm/len(que[rand])

for x in ans:
    print(x)
19/205:
import random
rand = random.randint(0, len(que))
print(rand)
for q in que[rand]:
    sm += model_que.wv[q]
mean_que = sm/len(que[rand])
res = []
for i, x in enumurate(ans):
    print(i, x)
#     sm = 0
#     for a in x:
#         sm += model_ans.wv[a]
#     mean = sm/len(ans[x])
#     res.append(zip(mean, ))
19/206:
import random
rand = random.randint(0, len(que))
print(rand)
for q in que[rand]:
    sm += model_que.wv[q]
mean_que = sm/len(que[rand])
res = []
for i, x in enumerate(ans):
    print(i, x)
#     sm = 0
#     for a in x:
#         sm += model_ans.wv[a]
#     mean = sm/len(ans[x])
#     res.append(zip(mean, ))
19/207:
import random
rand = random.randint(0, len(que))
print(rand)
sm = 0
for q in que[rand]:
    sm += model_que.wv[q]
mean_que = sm/len(que[rand])
res = []
for i, x in enumerate(ans):
    print(i, x)
#     sm = 0
#     for a in x:
#         sm += model_ans.wv[a]
#     mean = sm/len(ans[x])
#     res.append(zip(mean, ))
19/208:
import random
rand = random.randint(0, len(que))
print(rand)
sm = 0
for q in que[rand]:
    sm += model_que.wv[q]
mean_que = sm/len(que[rand])
res = []
for i, x in enumerate(ans):
    print(i, x)
    sm = 0
    for a in x:
        sm += model_ans.wv[a]
    mean = sm/len(ans[x])
    res.append(zip(mean, i))
19/209:
import nltk
from nltk.corpus import stopwords
sw_nltk = stopwords.words('russian')
# ans = []
# que = []
# [[w for w in ans[x] if w.lower() not in ] for x in range(len(ans))]
print(sw_nltk)
19/210:
ans = [[w for w in ans[x] if w not in sw_nltk] for x in range(len(ans))]
que = [[w for w in que[x] if w not in sw_nltk] for x in range(len(que))]
19/211:
ans = [[w for w in ans[x] if w not in sw_nltk] for x in range(len(ans))]
que = [[w for w in que[x] if w not in sw_nltk] for x in range(len(que))]
print(ans)
19/212:
ans = [[w for w in ans[x] if w not in sw_nltk] for x in range(len(ans))]
que = [[w for w in que[x] if w not in sw_nltk] for x in range(len(que))]
print(que)
19/213:
ans = [[w for w in ans[x] if w not in sw_nltk] for x in range(len(ans))]
que = [[w for w in que[x] if w not in sw_nltk] for x in range(len(que))]
19/214:
model_ans = gensim.models.Word2Vec (ans, vector_size=100, window=10, min_count=2, workers=10)
model_ans.train(ans,total_examples=len(ans),epochs=10)
19/215: model_ans.wv.most_similar (positive='деньга')
19/216: que
19/217:
model_que = gensim.models.Word2Vec (que, vector_size=100, window=10, min_count=2, workers=10)
model_que.train(que,total_examples=len(que),epochs=10)
19/218: import numpy as np
19/219: model_que.wv['карта']
19/220: model_que.wv.most_similar (positive='карта')
19/221:
import random
rand = random.randint(0, len(que))
print(rand)
sm = 0
for q in que[rand]:
    sm += model_que.wv[q]
mean_que = sm/len(que[rand])
res = []
for i, x in enumerate(ans):
    print(i, x)
    sm = 0
    for a in x:
        sm += model_ans.wv[a]
    mean = sm/len(ans[x])
    res.append(zip(mean, i))
19/222:
import random
rand = random.randint(0, len(que))
print(rand)
sm = 0
for q in que[rand]:
    sm += model_que.wv[q]
mean_que = sm/len(que[rand])
res = []
for i, x in enumerate(ans):
    print(i, x)
    sm = 0
    for a in x:
        print(a)
#         sm += model_ans.wv[a]
    mean = sm/len(ans[x])
    res.append(zip(mean, i))
19/223:
import random
rand = random.randint(0, len(que))
print(rand)
sm = 0
for q in que[rand]:
    sm += model_que.wv[q]
mean_que = sm/len(que[rand])
res = []
for i, x in enumerate(ans):
    print(i, x)
    sm = 0
    for a in x:
        print(a)
        sm += model_ans.wv[a]
    mean = sm/len(ans[i])
    res.append(zip(mean, i))
19/224:
import random
rand = random.randint(0, len(que))
print(rand)
sm = 0
for q in que[rand]:
    sm += model_que.wv[q]
mean_que = sm/len(que[rand])
res = []
for i, x in enumerate(ans):
    print(i, x)
    sm = 0
    for a in x:
        print(a)
        sm += model_ans.wv[a]
    mean = sm/len(ans[i])
    print(mean, i)
    res.append(zip(mean, i))
19/225:
import random
rand = random.randint(0, len(que))
print(rand)
sm = 0
for q in que[rand]:
    sm += model_que.wv[q]
mean_que = sm/len(que[rand])
res = []
for i, x in enumerate(ans):
    print(i, x)
    sm = 0
    for a in x:
        print(a)
        sm += model_ans.wv[a]
    mean = sm/len(ans[i])
    met = metric(mean_que, mean)
    res.append(zip(met, i))
19/226: que[26]
19/227: model_que.wv['предлагаться']
19/228: que
19/229:
import random
rand = random.randint(0, len(que))
print(rand)
sm = 0
for q in que[rand]:
    sm += model_que.wv[q]
mean_que = sm/len(que[rand])
res = []
for i, x in enumerate(ans):
    print(i, x)
    sm = 0
    for a in x:
        print(a)
        sm += model_ans.wv[a]
    mean = sm/len(ans[i])
    met = metric(mean_que, mean)
    res.append(zip(met, i))
19/230:
import random
rand = random.randint(0, len(que))
print(rand)
sm = 0
for q in que[rand]:
    sm += model_que.wv[q]
mean_que = sm/len(que[rand])
res = []
for i, x in enumerate(ans):
#     print(i, x)
    sm = 0
    for a in x:
        print(a)
        sm += model_ans.wv[a]
    mean = sm/len(ans[i])
    met = metric(mean_que, mean)
    print(met)
    res.append(zip(met, i))
19/231:
import random
rand = random.randint(0, len(que))
print(rand)
sm = 0
for q in que[rand]:
    sm += model_que.wv[q]
mean_que = sm/len(que[rand])
res = []
for i, x in enumerate(ans):
#     print(i, x)
    sm = 0
    for a in x:
#         print(a)
        sm += model_ans.wv[a]
    mean = sm/len(ans[i])
    met = metric(mean_que, mean)
    print(met)
    res.append(zip(met, i))
19/232:
import random
rand = random.randint(0, len(que))
print(rand)
sm = 0
for q in que[rand]:
    sm += model_que.wv[q]
mean_que = sm/len(que[rand])
res = []
for i, x in enumerate(ans):
#     print(i, x)
    sm = 0
    for a in x:
#         print(a)
        sm += model_ans.wv[a]
    mean = sm/len(ans[i])
    met = metric(mean_que, mean)
    print(met)
    print(zip(met, i))
    res.append(zip(met, i))
19/233:
import random
rand = random.randint(0, len(que))
print(rand)
sm = 0
for q in que[rand]:
    sm += model_que.wv[q]
mean_que = sm/len(que[rand])
res = []
for i, x in enumerate(ans):
#     print(i, x)
    sm = 0
    for a in x:
#         print(a)
        sm += model_ans.wv[a]
    mean = sm/len(ans[i])
    met = metric(mean_que, mean)
    print(met)
    print(met, i)
#     print(zip(met, i))
    res.append(zip(met, i))
19/234:
import random
rand = random.randint(0, len(que))
print(rand)
sm = 0
for q in que[rand]:
    sm += model_que.wv[q]
mean_que = sm/len(que[rand])
res = []
for i, x in enumerate(ans):
#     print(i, x)
    sm = 0
    for a in x:
#         print(a)
        sm += model_ans.wv[a]
    mean = sm/len(ans[i])
    met = metric(mean_que, mean)
    print(met)
    print(met, i)
#     print(zip(met, i))
#     res.append(zip(met, i))
19/235:
import random
rand = random.randint(0, len(que))
print(rand)
sm = 0
for q in que[rand]:
    sm += model_que.wv[q]
mean_que = sm/len(que[rand])
res = []
for i, x in enumerate(ans):
#     print(i, x)
    sm = 0
    for a in x:
#         print(a)
        sm += model_ans.wv[a]
    mean = sm/len(ans[i])
    met = metric(mean_que, mean)
    print(met)
    print(met, i)
#     print(zip(met, i))
#     res.append(zip(met, i))
19/236:
import random
rand = random.randint(0, len(que))
print(rand)
sm = 0
for q in que[rand]:
    sm += model_que.wv[q]
mean_que = sm/len(que[rand])
res = []
for i, x in enumerate(ans):
#     print(i, x)
    sm = 0
    for a in x:
#         print(a)
        sm += model_ans.wv[a]
    mean = sm/len(ans[i])
    met = metric(mean_que, mean)
#     print(met)
#     print(met, i)
    print(zip(met, i))
#     res.append(zip(met, i))
19/237:
import random
rand = random.randint(0, len(que))
print(rand)
sm = 0
for q in que[rand]:
    sm += model_que.wv[q]
mean_que = sm/len(que[rand])
res = []
for i, x in enumerate(ans):
#     print(i, x)
    sm = 0
    for a in x:
#         print(a)
        sm += model_ans.wv[a]
    mean = sm/len(ans[i])
    met = metric(mean_que, mean)
#     print(met)
#     print(met, i)
    print(zip(met, i))
res.append(met)
19/238:
import random
rand = random.randint(0, len(que))
print(rand)
sm = 0
for q in que[rand]:
    sm += model_que.wv[q]
mean_que = sm/len(que[rand])
res = []
for i, x in enumerate(ans):
#     print(i, x)
    sm = 0
    for a in x:
#         print(a)
        sm += model_ans.wv[a]
    mean = sm/len(ans[i])
    met = metric(mean_que, mean)
#     print(met)
#     print(met, i)
res.append(met)
19/239:
import random
rand = random.randint(0, len(que))
print(rand)
sm = 0
for q in que[rand]:
    sm += model_que.wv[q]
mean_que = sm/len(que[rand])
res = []
for i, x in enumerate(ans):
#     print(i, x)
    sm = 0
    for a in x:
#         print(a)
        sm += model_ans.wv[a]
    mean = sm/len(ans[i])
    met = metric(mean_que, mean)
#     print(met)
#     print(met, i)
    res.append(met)
19/240:
model_que = gensim.models.Word2Vec (que, vector_size=300, window=10, min_count=2, workers=10)
model_que.train(que,total_examples=len(que),epochs=10)
19/241:
model_ans = gensim.models.Word2Vec (ans, vector_size=300, window=10, min_count=2, workers=10)
model_ans.train(ans,total_examples=len(ans),epochs=10)
19/242: model_que.wv['предлагаться']
19/243: que[26]
19/244: model_que.wv.most_similar (positive='предлагаться')
19/245: model_que.wv['карта']
19/246: model_que.wv['предлагаться']
19/247:
model_que = gensim.models.Word2Vec (que, vector_size=300, window=20, min_count=2, workers=10)
model_que.train(que,total_examples=len(que),epochs=10)
19/248: model_que.wv.most_similar (positive='предлагаться')
19/249:
model_que = gensim.models.Word2Vec (que, vector_size=600, window=20, min_count=2, workers=10)
model_que.train(que,total_examples=len(que),epochs=10)
19/250: model_que.wv.most_similar (positive='предлагаться')
19/251:
model_que = gensim.models.Word2Vec (que, vector_size=300, window=10, min_count=0, workers=10)
model_que.train(que,total_examples=len(que),epochs=10)
19/252: model_que.wv.most_similar (positive='предлагаться')
19/253: model_que.wv['предлагаться']
19/254: que[26]
19/255: model_que.wv['предлагаться']
19/256:
import random
rand = random.randint(0, len(que))
print(rand)
sm = 0
for q in que[rand]:
    sm += model_que.wv[q]
mean_que = sm/len(que[rand])
res = []
for i, x in enumerate(ans):
#     print(i, x)
    sm = 0
    for a in x:
#         print(a)
        sm += model_ans.wv[a]
    mean = sm/len(ans[i])
    met = metric(mean_que, mean)
    res.append(met)
19/257:
model_ans = gensim.models.Word2Vec (ans, vector_size=300, window=10, min_count=0, workers=10)
model_ans.train(ans,total_examples=len(ans),epochs=10)
19/258:
import random
rand = random.randint(0, len(que))
print(rand)
sm = 0
for q in que[rand]:
    sm += model_que.wv[q]
mean_que = sm/len(que[rand])
res = []
for i, x in enumerate(ans):
#     print(i, x)
    sm = 0
    for a in x:
#         print(a)
        sm += model_ans.wv[a]
    mean = sm/len(ans[i])
    met = metric(mean_que, mean)
    res.append(met)
19/259:
import random
rand = random.randint(0, len(que))
print(rand)
sm = 0
for q in que[rand]:
    sm += model_que.wv[q]
mean_que = sm/len(que[rand])
res = []
for i, x in enumerate(ans):
#     print(i, x)
    sm = 0
    for a in x:
#         print(a)
        sm += model_ans.wv[a]
    mean = sm/len(ans[i])
    met = metric(mean_que, mean)
    res.append(met)
res
19/260:
import random
rand = random.randint(0, len(que))
print(rand)
sm = 0
for q in que[rand]:
    sm += model_que.wv[q]
mean_que = sm/len(que[rand])
res = []
for i, x in enumerate(ans):
#     print(i, x)
    sm = 0
    for a in x:
#         print(a)
        sm += model_ans.wv[a]
    mean = sm/len(ans[i])
    met = metric(mean_que, mean)
    res.append(met,i)
res.sort()
19/261:
import random
rand = random.randint(0, len(que))
print(rand)
sm = 0
for q in que[rand]:
    sm += model_que.wv[q]
mean_que = sm/len(que[rand])
res = []
for i, x in enumerate(ans):
#     print(i, x)
    sm = 0
    for a in x:
#         print(a)
        sm += model_ans.wv[a]
    mean = sm/len(ans[i])
    met = metric(mean_que, mean)
    res.append([met,i])
res.sort()
19/262:
import random
rand = random.randint(0, len(que))
print(rand)
sm = 0
for q in que[rand]:
    sm += model_que.wv[q]
mean_que = sm/len(que[rand])
res = []
for i, x in enumerate(ans):
#     print(i, x)
    sm = 0
    for a in x:
#         print(a)
        sm += model_ans.wv[a]
    mean = sm/len(ans[i])
    met = metric(mean_que, mean)
    res.append([met,i])
# res.sort()
# res.reverse()
print(res)
19/263:
import random
rand = random.randint(0, len(que))
print(rand)
sm = 0
for q in que[rand]:
    sm += model_que.wv[q]
mean_que = sm/len(que[rand])
res = []
for i, x in enumerate(ans):
#     print(i, x)
    sm = 0
    for a in x:
#         print(a)
        sm += model_ans.wv[a]
    mean = sm/len(ans[i])
    met = metric(mean_que, mean)
    res.append([met,i])
# res.sort()
# res.reverse()
print(sorted(res)[0])
19/264:
import random
rand = random.randint(0, len(que))
print(rand)
sm = 0
for q in que[rand]:
    sm += model_que.wv[q]
mean_que = sm/len(que[rand])
res = []
for i, x in enumerate(ans):
#     print(i, x)
    sm = 0
    for a in x:
#         print(a)
        sm += model_ans.wv[a]
    mean = sm/len(ans[i])
    met = metric(mean_que, mean)
    res.append([met,i])
# res.sort()
# res.reverse()
print(sorted(res)[-1])
19/265:
import random
rand = random.randint(0, len(que))
print(rand)
sm = 0
for q in que[rand]:
    sm += model_que.wv[q]
mean_que = sm/len(que[rand])
res = []
for i, x in enumerate(ans):
#     print(i, x)
    sm = 0
    for a in x:
#         print(a)
        sm += model_ans.wv[a]
    mean = sm/len(ans[i])
    met = metric(mean_que, mean)
    res.append([met,i])
# res.sort()
# res.reverse()
print(sorted(res)[0])
19/266:
import random
rand = random.randint(0, len(que))
print(rand)
sm = 0
for q in que[rand]:
    sm += model_que.wv[q]
mean_que = sm/len(que[rand])
res = []
for i, x in enumerate(ans):
#     print(i, x)
    sm = 0
    for a in x:
#         print(a)
        sm += model_ans.wv[a]
    mean = sm/len(ans[i])
    met = metric(mean_que, mean)
    res.append([met,i])
# res.sort()
# res.reverse()
print(res)
print(sorted(res)[0])
19/267:
import random
rand = random.randint(0, len(que))
print(rand)
sm = 0
for q in que[rand]:
    sm += model_que.wv[q]
mean_que = sm/len(que[rand])
res = []
for i, x in enumerate(ans):
#     print(i, x)
    sm = 0
    for a in x:
#         print(a)
        sm += model_ans.wv[a]
    mean = sm/len(ans[i])
    met = metric(mean_que, mean)
    res.append((met,i))
# res.sort()
# res.reverse()
print(res)
print(sorted(res)[0])
19/268: sorted(res)
19/269: sorted(res, reversed = True)
19/270: sorted(res, reverse = True)
19/271:
sorted(res, reverse = True)
ans[75]
19/272:
import random
rand = random.randint(0, len(que))
print(rand)
sm = 0
for q in que[rand]:
    sm += model_que.wv[q]
mean_que = sm/len(que[rand])
res = []
for i, x in enumerate(ans):
#     print(i, x)
    sm = 0
    for a in x:
#         print(a)
        sm += model_ans.wv[a]
    mean = sm/len(ans[i])
    met = metric(mean_que, mean)
    res.append((met,i))
# res.sort()
# res.reverse()
# print(res)
# print(sorted(res)[0])
19/273:
sorted(res, reverse = True)
# ans[]
19/274: sorted(res, reverse = True)
19/275: print(que[89], ans[89])
19/276: print(que[89], ans[85])
19/277:
import random
rand = random.randint(0, len(que))
print(rand)
sm = 0
for q in que[rand]:
    sm += model_que.wv[q]
mean_que = sm/len(que[rand])
res = []
for i, x in enumerate(ans):
#     print(i, x)
    sm = 0
    for a in x:
#         print(a)
        sm += model_ans.wv[a]
    mean = sm/len(ans[i])
    met = metric(mean_que, mean)
    res.append((met,i))
# res.sort()
# res.reverse()
# print(res)
# print(sorted(res)[0])
19/278: sorted(res, reverse = True)
19/279: print(que[62], ans[103])
19/280: len(res)
19/281: len(ans)
19/282:
model_ans = gensim.models.Word2Vec (ans, vector_size=100, window=10, min_count=0, workers=10)
model_ans.train(ans,total_examples=len(ans),epochs=10)
19/283: model_ans.wv.most_similar (positive='деньга')
19/284:
model_que = gensim.models.Word2Vec (que, vector_size=100, window=10, min_count=0, workers=10)
model_que.train(que,total_examples=len(que),epochs=10)
19/285: model_que.wv.most_similar (positive='предлагаться')
19/286: import numpy as np
19/287:
sm = 0
for a in ans[0]:
    sm += model_ans.wv[a]
mean_ans = sm/len(ans[0])

sm = 0
for q in que[0]:
    sm += model_que.wv[q]
mean_que = sm/len(que[0])
# mean_que.shape
print(np.dot(mean_ans, mean_que)/np.sqrt(np.sum(mean_ans**2) * np.sum(mean_que**2)))
19/288:
def metric(v1, v2):
    return np.dot(v1, v2)/np.sqrt(np.sum(v1**2)*np.sum(v2**2))
19/289:
import random
rand = random.randint(0, len(que))
print(rand)
sm = 0
for q in que[rand]:
    sm += model_que.wv[q]
mean_que = sm/len(que[rand])
res = []
for i, x in enumerate(ans):
#     print(i, x)
    sm = 0
    for a in x:
#         print(a)
        sm += model_ans.wv[a]
    mean = sm/len(ans[i])
    met = metric(mean_que, mean)
    res.append((met,i))
# res.sort()
# res.reverse()
# print(res)
# print(sorted(res)[0])
19/290: len(ans)
19/291: sorted(res, reverse = True)
19/292: print(que[rand], ans[104])
19/293: from sklearn.metrics.pairwise import cosine_similarity
19/294:
from sklearn.metrics.pairwise import cosine_similarity
cosine_similarity(model_que['предлагаться'], model_que['разница'])
19/295:
from sklearn.metrics.pairwise import cosine_similarity
cosine_similarity(model_que.wv['предлагаться'], model_que.wv['разница'])
19/296:
from sklearn.metrics.pairwise import cosine_similarity
cosine_similarity(model_que.wv['предлагаться'].reshape(1-1), model_que.wv['разница'].reshape(1, -1))
19/297: model = gensim.models.KeyedVectors.load_word2vec_format('path to word2vec e.g. GoogleNews-vectors-negative300.bin', binary=True)
19/298:
from gensim.models import KeyedVectors

# Load pre-trained word vectors
word_vectors = KeyedVectors.load_word2vec_format('path_to_pretrained_embeddings.bin', binary=True)

# Define a list of words for which you want to calculate the mean
word_list = ["apple", "banana", "orange"]

# Filter out words that are not in the vocabulary
word_list = [word for word in word_list if word in word_vectors]

# Calculate the mean of word vectors
mean_vector = word_vectors[word_list].mean(axis=0)

print("Mean vector shape:", mean_vector.shape)
print("Mean vector:", mean_vector)
19/299:
# import random
# rand = random.randint(0, len(que))
rand = 0
# print(rand)
sm = 0
for q in que[rand]:
    sm += model_que.wv[q]
mean_que = sm/len(que[rand])
res = []
for i, x in enumerate(ans):
    sm = 0
    for a in x:
        sm += model_ans.wv[a]
    mean = sm/len(ans[i])
    met = metric(mean_que, mean)
    res.append((met,i))
    print(a)
#     mean_vector = np.mean([word_vectors[word] for word in words_in_vocab], axis=0)

# res.sort()
# res.reverse()
# print(res)
# print(sorted(res)[0])
19/300:
# import random
# rand = random.randint(0, len(que))
rand = 0
# print(rand)
sm = 0
for q in que[rand]:
    sm += model_que.wv[q]
mean_que = sm/len(que[rand])
res = []
for i, x in enumerate(ans):
    sm = 0
    for a in x:
        sm += model_ans.wv[a]
    mean = sm/len(ans[i])
    met = metric(mean_que, mean)
    res.append((met,i))
        print(a)
#     mean_vector = np.mean([word_vectors[word] for word in words_in_vocab], axis=0)

# res.sort()
# res.reverse()
# print(res)
# print(sorted(res)[0])
19/301:
# import random
# rand = random.randint(0, len(que))
rand = 0
# print(rand)
sm = 0
for q in que[rand]:
    sm += model_que.wv[q]
mean_que = sm/len(que[rand])
res = []
for i, x in enumerate(ans):
    sm = 0
    for a in x:
        sm += model_ans.wv[a]
    mean = sm/len(ans[i])
    met = metric(mean_que, mean)
    res.append((met,i))
    print(a)
#     mean_vector = np.mean([word_vectors[word] for word in words_in_vocab], axis=0)

# res.sort()
# res.reverse()
# print(res)
# print(sorted(res)[0])
19/302:
# import random
# rand = random.randint(0, len(que))
rand = 0
# print(rand)
sm = 0
for q in que[rand]:
    sm += model_que.wv[q]
mean_que = sm/len(que[rand])
res = []
for i, x in enumerate(ans):
    sm = 0
    print(a)
    for a in x:
        sm += model_ans.wv[a]
    mean = sm/len(ans[i])
    met = metric(mean_que, mean)
    res.append((met,i))
#     mean_vector = np.mean([word_vectors[word] for word in words_in_vocab], axis=0)

# res.sort()
# res.reverse()
# print(res)
# print(sorted(res)[0])
19/303:
# import random
# rand = random.randint(0, len(que))
rand = 0
# print(rand)
sm = 0
for q in que[rand]:
    sm += model_que.wv[q]
mean_que = sm/len(que[rand])
res = []
for i, x in enumerate(ans):
    sm = 0
    print(a)
#     for a in x:
#         sm += model_ans.wv[a]
    mean = sm/len(ans[i])
    met = metric(mean_que, mean)
    res.append((met,i))
#     mean_vector = np.mean([word_vectors[word] for word in words_in_vocab], axis=0)

# res.sort()
# res.reverse()
# print(res)
# print(sorted(res)[0])
22/1:
# import random
# rand = random.randint(0, len(que))
rand = 0
# print(rand)
sm = 0
for q in que[rand]:
    sm += model_que.wv[q]
mean_que = sm/len(que[rand])
res = []
for i, x in enumerate(ans):
    sm = 0
    print(a)
#     for a in x:
#         sm += model_ans.wv[a]
    mean = sm/len(ans[i])
    met = metric(mean_que, mean)
    res.append((met,i))
#     mean_vector = np.mean([word_vectors[word] for word in words_in_vocab], axis=0)

# res.sort()
# res.reverse()
# print(res)
# print(sorted(res)[0])
23/1: import json
23/2:
with open('bank_faq.json', encoding='utf-8') as f:
    data = json.load(f)
23/3: data
23/4:
answers = []
questions = []
for x in data:
    for k in data[x]:
        for a in data[x][k]:
            questions.append(a['вопрос'])
            answers.append(a['ответ'])
23/5: questions
23/6: answers
23/7: ans_tok = [list(tokenize(x.lower())) for x in answers].copy()
23/8:
import textblob
from textblob import TextBlob
23/9: text = "Hello everyone! Welcome to my blog post on Medium. We are studying Natural Language Processing."
23/10: TextBlob(text).words
23/11:
import nltk
from nltk import sent_tokenize
from nltk import word_tokenize
23/12:
tokens = nltk.sent_tokenize(text)
print(tokens)
23/13:
tokens_words = nltk.word_tokenize(text)
print(tokens_words)
23/14: from nltk.stem import PorterStemmer
23/15:
ps = PorterStemmer()
word = ("civilization")
ps.stem(word)
23/16: from nltk.stem.snowball import SnowballStemmer
23/17:
stemmer = SnowballStemmer(language = "english")
word = "civilization"
stemmer.stem(word)
23/18:
import nltk
from nltk.stem import WordNetLemmatizer 
lemmatizer = WordNetLemmatizer()
23/19:
# Lemmatize single word
print(lemmatizer.lemmatize("workers"))
print(lemmatizer.lemmatize("beeches"))
23/20:
text = "Let’s lemmatize a simple sentence. We first tokenize the sentence into words using nltk.word_tokenize and then we will call lemmatizer.lemmatize() on each word. "
word_list = nltk.word_tokenize(text)
print(word_list)
23/21:
lemmatized_output = ' '.join([lemmatizer.lemmatize(w) for w in word_list])
print(lemmatized_output)
23/22:

from textblob import TextBlob, Word
23/23:
word = 'stripes'
w = Word(word)
w.lemmatize()
23/24:
text = "The striped bats are hanging on their feet for best"
sent = TextBlob(text)
" ". join([w.lemmatize() for w in sent.words])
23/25:
text = '''Those , have who information who are at still witnesses best of times BLANK the can contact Pastor ( Tel : or 05461 BLANK 885041 the Nachrichten 05461 ( 930010 / ) .
Königs in emphasized the of guests at the Textile Academy the where of opening the celebrated CTL The : &quot; and industry clothing has ahead decisive a renaissance it of .
to According police the BLANK , several residents Monday the in were BLANK BLANK area alleged officers police Sunday on .'''
23/26: text
23/27: TextBlob(text).words
23/28:
tokens = nltk.sent_tokenize(text)
print(tokens)
23/29:
tokens_words = nltk.word_tokenize(text)
print(tokens_words)
23/30:
lemmatized_output = ' '.join([lemmatizer.lemmatize(w) for w in word_list])
print(lemmatized_output)
23/31: from razdel import tokenize
23/32: tokens = list(tokenize('Кружка-термос на 0.5л (50/64 см³, 516;...)'))
23/33: tokens
23/34: [_.text for _ in tokens]
23/35:
 text = '''
... - "Так в чем же дело?" - "Не ра-ду-ют".
... И т. д. и т. п. В общем, вся газета
... '''
23/36: from razdel import sentenize
23/37: list(sentenize(text))
23/38: import pandas as pd
23/39: !pip install pandas
23/40: df = pd.read_json('bank_faq.json')
23/41: df
23/42:
with open('bank_faq.json', encoding='utf-8') as f:
    data = json.load(f)
23/43: data
23/44:
answers = []
questions = []
for x in data:
    for k in data[x]:
        for a in data[x][k]:
            questions.append(a['вопрос'])
            answers.append(a['ответ'])
23/45: questions
23/46: answers
23/47: ans_tok = [list(tokenize(x.lower())) for x in answers].copy()
23/48: ques_tok = [list(tokenize(x.lower())) for x in questions].copy()
23/49:
ans_tok = [list(tokenize(x.lower())) for x in answers].copy()
ans_tok
23/50: ans_tok = [list(tokenize(x.lower())) for x in answers].copy()
23/51: ques_tok = [list(tokenize(x.lower())) for x in questions].copy()
23/52:
from natasha import (
    Segmenter,
    MorphVocab,
    
    Doc
)
23/53:
# pip install pymorphy2
# /
23/54:
import pymorphy2
morph = pymorphy2.MorphAnalyzer()
23/55: morph.parse('буквы')
23/56:
ans = []
que = []
for sentence in ans_tok:
    temp = []
    for word in sentence:
        m = morph.parse(word.text)[0]
        temp.append(m.normal_form)
    sss = ' '.join(temp)
    tt = gensim.utils.simple_preprocess(sss)
    ans.append(tt)

for sentence in ques_tok:
    temp = []
    for word in sentence:
        m = morph.parse(word.text)[0]
        temp.append(m.normal_form)
    sss = ' '.join(temp)
    tt = gensim.utils.simple_preprocess(sss)
    que.append(tt)
#     que.append(temp)

print(ans)
print(que)
23/57:
from natasha import (
    Segmenter,
    MorphVocab,
    
    Doc
)
import gensim
23/58:
import pymorphy2
morph = pymorphy2.MorphAnalyzer()
23/59: morph.parse('буквы')
23/60:
ans = []
que = []
for sentence in ans_tok:
    temp = []
    for word in sentence:
        m = morph.parse(word.text)[0]
        temp.append(m.normal_form)
    sss = ' '.join(temp)
    tt = gensim.utils.simple_preprocess(sss)
    ans.append(tt)

for sentence in ques_tok:
    temp = []
    for word in sentence:
        m = morph.parse(word.text)[0]
        temp.append(m.normal_form)
    sss = ' '.join(temp)
    tt = gensim.utils.simple_preprocess(sss)
    que.append(tt)
#     que.append(temp)

print(ans)
print(que)
23/61: sss = ' '.join(ans[0])
23/62:
import nltk
from nltk.corpus import stopwords
sw_nltk = stopwords.words('russian')
# ans = []
# que = []
# [[w for w in ans[x] if w.lower() not in ] for x in range(len(ans))]
print(sw_nltk)
23/63:
ans = [[w for w in ans[x] if w not in sw_nltk] for x in range(len(ans))]
que = [[w for w in que[x] if w not in sw_nltk] for x in range(len(que))]
23/64:
ans = [[w for w in ans[x] if w not in sw_nltk] for x in range(len(ans))]
que = [[w for w in que[x] if w not in sw_nltk] for x in range(len(que))]
ans
23/65:
ans = [[w for w in ans[x] if w not in sw_nltk] for x in range(len(ans))]
que = [[w for w in que[x] if w not in sw_nltk] for x in range(len(que))]
23/66:
model_ans = gensim.models.Word2Vec (ans, vector_size=100, window=10, min_count=0, workers=10)
model_ans.train(ans,total_examples=len(ans),epochs=10)
23/67: model_ans.wv.most_similar (positive='деньга')
23/68:
model_ans = gensim.models.Word2Vec (ans, vector_size=100, window=10, min_count=0, workers=10)
model_ans.train(ans,total_examples=len(ans),epochs=10)
23/69: model_ans.wv.most_similar (positive='деньга')
23/70:
model_que = gensim.models.Word2Vec (que, vector_size=100, window=10, min_count=0, workers=10)
model_que.train(que,total_examples=len(que),epochs=10)
23/71:
from sklearn.metrics.pairwise import cosine_similarity
cosine_similarity(model_que.wv['предлагаться'].reshape(1-1), model_que.wv['разница'].reshape(1, -1))
23/72:
from sklearn.metrics.pairwise import cosine_similarity
# cosine_similarity(model_que.wv['предлагаться'].reshape(1-1), model_que.wv['разница'].reshape(1, -1))
23/73:
from sklearn.metrics.pairwise import cosine_similarity
# cosine_similarity(model_que.wv['предлагаться'].reshape(1-1), model_que.wv['разница'].reshape(1, -1))
23/74:
from sklearn.metrics.pairwise import cosine_similarity
# cosine_similarity(model_que.wv['предлагаться'].reshape(1-1), model_que.wv['разница'].reshape(1, -1))
23/75: model_que.wv.most_similar (positive='предлагаться')
23/76: import numpy as np
23/77: model_que.wv.most_similar (positive='карта')
23/78: import numpy as np
23/79:
sm = 0
for a in ans[0]:
    sm += model_ans.wv[a]
mean_ans = sm/len(ans[0])

sm = 0
for q in que[0]:
    sm += model_que.wv[q]
mean_que = sm/len(que[0])
# mean_que.shape
print(np.dot(mean_ans, mean_que)/np.sqrt(np.sum(mean_ans**2) * np.sum(mean_que**2)))
23/80:
def metric(v1, v2):
    return np.dot(v1, v2)/np.sqrt(np.sum(v1**2) * np.sum(v2**2))
23/81:
# import random
# rand = random.randint(0, len(que))
rand = 0
# print(rand)
sm = 0
for q in que[rand]:
    sm += model_que.wv[q]
mean_que = sm/len(que[rand])
res = []
for i, x in enumerate(ans):
    sm = 0
    print(a)
#     for a in x:
#         sm += model_ans.wv[a]
    mean = sm/len(ans[i])
    met = metric(mean_que, mean)
    res.append((met,i))
#     mean_vector = np.mean([word_vectors[word] for word in words_in_vocab], axis=0)

# res.sort()
# res.reverse()
# print(res)
# print(sorted(res)[0])
23/82:
# import random
# rand = random.randint(0, len(que))
rand = 0
# print(rand)
sm = 0
for q in que[rand]:
    sm += model_que.wv[q]
mean_que = sm/len(que[rand])
res = []
for i, x in enumerate(ans):
    sm = 0
#     print(a)
#     for a in x:
#         sm += model_ans.wv[a]
    mean = sm/len(ans[i])
    met = metric(mean_que, mean)
    res.append((met,i))
#     mean_vector = np.mean([word_vectors[word] for word in words_in_vocab], axis=0)

# res.sort()
# res.reverse()
# print(res)
# print(sorted(res)[0])
23/83: len(ans)
23/84: print(que[rand], ans[104])
23/85: sorted(res, reverse = True)
23/86: res
23/87:
# import random
# rand = random.randint(0, len(que))
rand = 0
# print(rand)
sm = 0
for q in que[rand]:
    sm += model_que.wv[q]
mean_que = sm/len(que[rand])
print(mean_que)
res = []
for i, x in enumerate(ans):
    sm = 0
#     print(a)
#     for a in x:
#         sm += model_ans.wv[a]
    mean = sm/len(ans[i])
    met = metric(mean_que, mean)
    
    res.append((met,i))
#     mean_vector = np.mean([word_vectors[word] for word in words_in_vocab], axis=0)

# res.sort()
# res.reverse()
# print(res)
# print(sorted(res)[0])
23/88:
# import random
# rand = random.randint(0, len(que))
rand = 0
# print(rand)
sm = 0
for q in que[rand]:
    sm += model_que.wv[q]
mean_que = sm/len(que[rand])
res = []
for i, x in enumerate(ans):
    sm = 0
    print(a)
#     for a in x:
#         sm += model_ans.wv[a]
    mean = sm/len(ans[i])
    met = metric(mean_que, mean)
    
    res.append((met,i))
#     mean_vector = np.mean([word_vectors[word] for word in words_in_vocab], axis=0)

# res.sort()
# res.reverse()
# print(res)
# print(sorted(res)[0])
23/89:
# import random
# rand = random.randint(0, len(que))
rand = 0
# print(rand)
sm = 0
for q in que[rand]:
    sm += model_que.wv[q]
mean_que = sm/len(que[rand])
res = []
for i, x in enumerate(ans):
    sm = 0
    print(x)
#     for a in x:
#         sm += model_ans.wv[a]
    mean = sm/len(ans[i])
    met = metric(mean_que, mean)
    
    res.append((met,i))
#     mean_vector = np.mean([word_vectors[word] for word in words_in_vocab], axis=0)

# res.sort()
# res.reverse()
# print(res)
# print(sorted(res)[0])
23/90:
# import random
# rand = random.randint(0, len(que))
rand = 0
# print(rand)
sm = 0
for q in que[rand]:
    sm += model_que.wv[q]
mean_que = sm/len(que[rand])
res = []
for i, x in enumerate(ans):
    sm = 0
#     print(x)
    for a in x:
        sm += model_ans.wv[a]
    mean = sm/len(ans[i])
    met = metric(mean_que, mean)
    
    res.append((met,i))
#     mean_vector = np.mean([word_vectors[word] for word in words_in_vocab], axis=0)

# res.sort()
# res.reverse()
# print(res)
# print(sorted(res)[0])
23/91: res
23/92:
# import random
# rand = random.randint(0, len(que))
rand = 0
# print(rand)
sm = 0
for q in que[rand]:
    sm += model_que.wv[q]
mean_que = sm/len(que[rand])
res = []
for i, x in enumerate(ans):
    sm = 0
#     print(x)
    for a in x:
        sm += model_ans.wv[a]
    mean = sm/len(ans[i])
    met = metric(mean_que, mean)
    
    res.append((met,i))
#     mean_vector = np.mean([word_vectors[word] for word in words_in_vocab], axis=0)

# res.sort()
# res.reverse()
# print(res)
# print(sorted(res)[0])
23/93: # res
23/94: len(ans)
23/95: print(que[rand], ans[104])
23/96: sorted(res, reverse = True)
23/97: from gensim.similarities import SparseMatrixSimilarity
23/98:
from gensim.models import Word2Vec
from gensim.similarities import SparseMatrixSimilarity
from gensim.corpora import Dictionary
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
import string

# Example question-answer dataset
dataset = [
    ("What is your name?", "My name is John."),
    ("How old are you?", "I am 25 years old."),
    # Add more question-answer pairs here
]

# Preprocessing function
def preprocess(text):
    stop_words = set(stopwords.words('english'))
    tokens = word_tokenize(text.lower())
    tokens = [word for word in tokens if word not in stop_words and word not in string.punctuation]
    return tokens

# Preprocess dataset
preprocessed_dataset = [(preprocess(question), preprocess(answer)) for question, answer in dataset]

# Build a Dictionary and Corpus
dictionary = Dictionary(preprocessed_dataset)
corpus = [dictionary.doc2bow(doc) for doc in preprocessed_dataset]

# Train a Word2Vec model
word2vec_model = Word2Vec(preprocessed_dataset, vector_size=100, window=5, min_count=1, workers=4)

# Compute document embeddings
document_embeddings = []
for doc in preprocessed_dataset:
    doc_vector = sum(word2vec_model.wv[word] for word in doc[0]) / len(doc[0])
    document_embeddings.append(doc_vector)

# Convert document embeddings to a similarity index
similarity_index = SparseMatrixSimilarity(document_embeddings, num_features=100)

# Function to find most similar answer
def find_most_similar_answer(question):
    query = preprocess(question)
    query_vector = sum(word2vec_model.wv[word] for word in query) / len(query)
    similarities = similarity_index[query_vector]
    most_similar_index = similarities.argmax()
    return dataset[most_similar_index][1]

# Test the function
question = "What is your age?"
most_similar_answer = find_most_similar_answer(question)
print("Question:", question)
print("Most Similar Answer:", most_similar_answer)
23/99:
from gensim.models import Word2Vec
from gensim.similarities import SparseMatrixSimilarity
from gensim.corpora import Dictionary
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
import string

# Example question-answer dataset
dataset = [
    ("What is your name?", "My name is John."),
    ("How old are you?", "I am 25 years old."),
    # Add more question-answer pairs here
]

# Preprocessing function
def preprocess(text):
    stop_words = set(stopwords.words('english'))
    tokens = word_tokenize(text.lower())
    tokens = [word for word in tokens if word not in stop_words and word not in string.punctuation]
    return tokens

# Preprocess dataset
preprocessed_dataset = [(preprocess(question), preprocess(answer)) for question, answer in dataset]

# Build a Dictionary and Corpus
dictionary = Dictionary(preprocessed_dataset)
corpus = [dictionary.doc2bow(doc) for doc in preprocessed_dataset]

# Train a Word2Vec model
word2vec_model = Word2Vec(preprocessed_dataset, vector_size=100, window=5, min_count=1, workers=4)

# Compute document embeddings
document_embeddings = []
for doc in preprocessed_dataset:
    doc_vector = sum(word2vec_model.wv[word] for word in doc[0]) / len(doc[0])
    document_embeddings.append(doc_vector)

# Convert document embeddings to a similarity index
similarity_index = SparseMatrixSimilarity(document_embeddings, num_features=100)

# Function to find most similar answer
def find_most_similar_answer(question):
    query = preprocess(question)
    query_vector = sum(word2vec_model.wv[word] for word in query) / len(query)
    similarities = similarity_index[query_vector]
    most_similar_index = similarities.argmax()
    return dataset[most_similar_index][1]

# Test the function
question = "What is your age?"
most_similar_answer = find_most_similar_answer(question)
print("Question:", question)
print("Most Similar Answer:", most_similar_answer)
23/100:
from gensim.models import Word2Vec
from gensim.similarities import SparseMatrixSimilarity
from gensim.corpora import Dictionary
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
import string

# Example question-answer dataset
dataset = [
    ("What is your name?", "My name is John."),
    ("How old are you?", "I am 25 years old."),
    # Add more question-answer pairs here
]

# Preprocessing function
def preprocess(text):
    stop_words = set(stopwords.words('english'))
    tokens = word_tokenize(text.lower())
    tokens = [word for word in tokens if word not in stop_words and word not in string.punctuation]
    return tokens

# Preprocess dataset
preprocessed_dataset = [(preprocess(question), preprocess(answer)) for question, answer in dataset]

# Build a Dictionary and Corpus
dictionary = Dictionary(preprocessed_dataset)
corpus = [dictionary.doc2bow(doc) for doc in preprocessed_dataset]

# Train a Word2Vec model
word2vec_model = Word2Vec([doc[0] for doc in preprocessed_dataset], vector_size=100, window=5, min_count=1, workers=4)

# Compute document embeddings
document_embeddings = []
for doc in preprocessed_dataset:
    doc_vector = sum(word2vec_model.wv[word] for word in doc[0]) / len(doc[0])
    document_embeddings.append(doc_vector)

# Convert document embeddings to a similarity index
similarity_index = SparseMatrixSimilarity(document_embeddings, num_features=100)

# Function to find most similar answer
def find_most_similar_answer(question):
    query = preprocess(question)
    query_vector = sum(word2vec_model.wv[word] for word in query) / len(query)
    similarities = similarity_index[query_vector]
    most_similar_index = similarities.argmax()
    return dataset[most_similar_index][1]

# Test the function
question = "What is your age?"
most_similar_answer = find_most_similar_answer(question)
print("Question:", question)
print("Most Similar Answer:", most_similar_answer)
24/1:
from sklearn.feature_extraction.text import TfidfVectorizer 
from sklearn.metrics.pairwise import cosine_similarity
24/2:
tfidfvectoriser=TfidfVectorizer(max_features=64)
tfidfvectoriser.fit(documents_df.documents_cleaned)
tfidf_vectors=tfidfvectoriser.transform(documents_df.documents_cleaned)
24/3:
import textblob
from textblob import TextBlob
24/4: text = "Hello everyone! Welcome to my blog post on Medium. We are studying Natural Language Processing."
24/5: TextBlob(text).words
24/6:
import nltk
from nltk import sent_tokenize
from nltk import word_tokenize
24/7:
tokens = nltk.sent_tokenize(text)
print(tokens)
24/8:
tokens_words = nltk.word_tokenize(text)
print(tokens_words)
24/9: from nltk.stem import PorterStemmer
24/10:
ps = PorterStemmer()
word = ("civilization")
ps.stem(word)
24/11: from nltk.stem.snowball import SnowballStemmer
24/12:
stemmer = SnowballStemmer(language = "english")
word = "civilization"
stemmer.stem(word)
24/13:
import nltk
from nltk.stem import WordNetLemmatizer 
lemmatizer = WordNetLemmatizer()
24/14:
# Lemmatize single word
print(lemmatizer.lemmatize("workers"))
print(lemmatizer.lemmatize("beeches"))
24/15:
text = "Let’s lemmatize a simple sentence. We first tokenize the sentence into words using nltk.word_tokenize and then we will call lemmatizer.lemmatize() on each word. "
word_list = nltk.word_tokenize(text)
print(word_list)
24/16:
lemmatized_output = ' '.join([lemmatizer.lemmatize(w) for w in word_list])
print(lemmatized_output)
24/17:

from textblob import TextBlob, Word
24/18:
word = 'stripes'
w = Word(word)
w.lemmatize()
24/19:
text = "The striped bats are hanging on their feet for best"
sent = TextBlob(text)
" ". join([w.lemmatize() for w in sent.words])
24/20:
text = '''Those , have who information who are at still witnesses best of times BLANK the can contact Pastor ( Tel : or 05461 BLANK 885041 the Nachrichten 05461 ( 930010 / ) .
Königs in emphasized the of guests at the Textile Academy the where of opening the celebrated CTL The : &quot; and industry clothing has ahead decisive a renaissance it of .
to According police the BLANK , several residents Monday the in were BLANK BLANK area alleged officers police Sunday on .'''
24/21: text
24/22: TextBlob(text).words
24/23:
tokens = nltk.sent_tokenize(text)
print(tokens)
24/24:
tokens_words = nltk.word_tokenize(text)
print(tokens_words)
24/25:
lemmatized_output = ' '.join([lemmatizer.lemmatize(w) for w in word_list])
print(lemmatized_output)
24/26: from razdel import tokenize
24/27: tokens = list(tokenize('Кружка-термос на 0.5л (50/64 см³, 516;...)'))
24/28: tokens
24/29: [_.text for _ in tokens]
24/30:
 text = '''
... - "Так в чем же дело?" - "Не ра-ду-ют".
... И т. д. и т. п. В общем, вся газета
... '''
24/31: from razdel import sentenize
24/32: list(sentenize(text))
24/33: import pandas as pd
24/34: df = pd.read_json('bank_faq.json')
24/35: df
24/36: indexes = df.index.to_list()
24/37:
questions = []
answers = []
for ind in indexes[:4]:
    for x in df.loc[ind]['Депозит']:
#         print(x['вопрос'])
        questions.append(x['вопрос'])
        answers.append(x['ответ'])
for ind in indexes[4:]:
    for x in df.loc[ind]['Карта']:
#         print(x['вопрос'])
        questions.append(x['вопрос'])
        answers.append(x['ответ'])
24/38: [_.text for _ ]
24/39:
import nltk
from nltk.corpus import stopwords
sw_nltk = stopwords.words('russian')
# ans = []
# que = []
# [[w for w in ans[x] if w.lower() not in ] for x in range(len(ans))]
print(sw_nltk)
24/40:
ans = [[w for w in ans[x] if w not in sw_nltk] for x in range(len(ans))]
que = [[w for w in que[x] if w not in sw_nltk] for x in range(len(que))]
24/41:
model_ans = gensim.models.Word2Vec (ans, vector_size=100, window=10, min_count=0, workers=10)
model_ans.train(ans,total_examples=len(ans),epochs=10)
24/42: model_ans.wv.most_similar (positive='деньга')
24/43:
model_que = gensim.models.Word2Vec (que, vector_size=100, window=10, min_count=0, workers=10)
model_que.train(que,total_examples=len(que),epochs=10)
24/44:
from sklearn.metrics.pairwise import cosine_similarity
# cosine_similarity(model_que.wv['предлагаться'].reshape(1-1), model_que.wv['разница'].reshape(1, -1))
24/45: # [_.text for _ ]
24/46: answers
24/47: [list(TextBlob(x).words) for x in answers]
24/48: [list(TextBlob(x).words) for x in answers]
24/49: import json
24/50:
with open('bank_faq.json', encoding='utf-8') as f:
    data = json.load(f)
24/51: data
24/52:
answers = []
questions = []
for x in data:
    for k in data[x]:
        for a in data[x][k]:
            questions.append(a['вопрос'])
            answers.append(a['ответ'])
24/53: questions
24/54: answers
24/55: ans_tok = [list(tokenize(x.lower())) for x in answers].copy()
24/56: ques_tok = [list(tokenize(x.lower())) for x in questions].copy()
24/57:
from natasha import (
    Segmenter,
    MorphVocab,
    
    Doc
)
import gensim
24/58:
import pymorphy2
morph = pymorphy2.MorphAnalyzer()
24/59: morph.parse('буквы')
24/60:
ans = []
que = []
for sentence in ans_tok:
    temp = []
    for word in sentence:
        m = morph.parse(word.text)[0]
        temp.append(m.normal_form)
    sss = ' '.join(temp)
    tt = gensim.utils.simple_preprocess(sss)
    ans.append(tt)

for sentence in ques_tok:
    temp = []
    for word in sentence:
        m = morph.parse(word.text)[0]
        temp.append(m.normal_form)
    sss = ' '.join(temp)
    tt = gensim.utils.simple_preprocess(sss)
    que.append(tt)
#     que.append(temp)

print(ans)
print(que)
24/61: # sw = "c а алло без белый близко более больше большой будем будет будете будешь будто буду будут будь бы бывает бывь был была были было быть в важная важное важные важный вам вами вас ваш ваша ваше ваши вверх вдали вдруг ведь везде вернуться весь вечер взгляд взять вид видел видеть вместе вне вниз внизу во вода война вокруг вон вообще вопрос восемнадцатый восемнадцать восемь восьмой вот впрочем времени время все все еще всегда всего всем всеми всему всех всею всю всюду вся всё второй вы выйти г где главный глаз говорил говорит говорить год года году голова голос город да давать давно даже далекий далеко дальше даром дать два двадцатый двадцать две двенадцатый двенадцать дверь двух девятнадцатый девятнадцать девятый девять действительно дел делал делать делаю дело день деньги десятый десять для до довольно долго должен должно должный дом дорога друг другая другие других друго другое другой думать душа е его ее ей ему если есть еще ещё ею её ж ждать же жена женщина жизнь жить за занят занята занято заняты затем зато зачем здесь земля знать значит значить и иди идти из или им имеет имел именно иметь ими имя иногда их к каждая каждое каждые каждый кажется казаться как какая какой кем книга когда кого ком комната кому конец конечно которая которого которой которые который которых кроме кругом кто куда лежать лет ли лицо лишь лучше любить люди м маленький мало мать машина между меля менее меньше меня место миллионов мимо минута мир мира мне много многочисленная многочисленное многочисленные многочисленный мной мною мог могу могут мож может может быть можно можхо мои мой мор москва мочь моя моё мы на наверху над надо назад наиболее найти наконец нам нами народ нас начала начать наш наша наше наши не него недавно недалеко нее ней некоторый нельзя нем немного нему непрерывно нередко несколько нет нею неё ни нибудь ниже низко никакой никогда никто никуда ним ними них ничего ничто но новый нога ночь ну нужно нужный нх о об оба обычно один одиннадцатый одиннадцать однажды однако одного одной оказаться окно около он она они оно опять особенно остаться от ответить отец откуда отовсюду отсюда очень первый перед писать плечо по под подойди подумать пожалуйста позже пойти пока пол получить помнить понимать понять пор пора после последний посмотреть посреди потом потому почему почти правда прекрасно при про просто против процентов путь пятнадцатый пятнадцать пятый пять работа работать раз разве рано раньше ребенок решить россия рука русский ряд рядом с с кем сам сама сами самим самими самих само самого самой самом самому саму самый свет свое своего своей свои своих свой свою сделать сеаой себе себя сегодня седьмой сейчас семнадцатый семнадцать семь сидеть сила сих сказал сказала сказать сколько слишком слово случай смотреть сначала снова со собой собою советский совсем спасибо спросить сразу стал старый стать стол сторона стоять страна суть считать т та так такая также таки такие такое такой там твои твой твоя твоё те тебе тебя тем теми теперь тех то тобой тобою товарищ тогда того тоже только том тому тот тою третий три тринадцатый тринадцать ту туда тут ты тысяч у увидеть уж уже улица уметь утро хороший хорошо хотел бы хотеть хоть хотя хочешь час часто часть чаще чего человек чем чему через четвертый четыре четырнадцатый четырнадцать что чтоб чтобы чуть шестнадцатый шестнадцать шестой шесть эта эти этим этими этих это этого этой этом этому этот эту я являюсь".split()
24/62:
import nltk
from nltk.corpus import stopwords
sw_nltk = stopwords.words('russian')
# ans = []
# que = []
# [[w for w in ans[x] if w.lower() not in ] for x in range(len(ans))]
print(sw_nltk)
24/63:
ans = [[w for w in ans[x] if w not in sw_nltk] for x in range(len(ans))]
que = [[w for w in que[x] if w not in sw_nltk] for x in range(len(que))]
24/64:
model_ans = gensim.models.Word2Vec (ans, vector_size=100, window=10, min_count=0, workers=10)
model_ans.train(ans,total_examples=len(ans),epochs=10)
24/65: model_ans.wv.most_similar (positive='деньга')
24/66:
model_que = gensim.models.Word2Vec (que, vector_size=100, window=10, min_count=0, workers=10)
model_que.train(que,total_examples=len(que),epochs=10)
24/67:
from sklearn.metrics.pairwise import cosine_similarity
# cosine_similarity(model_que.wv['предлагаться'].reshape(1-1), model_que.wv['разница'].reshape(1, -1))
24/68: model_que.wv.most_similar (positive='карта')
24/69: import numpy as np
24/70:
sm = 0
for a in ans[0]:
    sm += model_ans.wv[a]
mean_ans = sm/len(ans[0])

sm = 0
for q in que[0]:
    sm += model_que.wv[q]
mean_que = sm/len(que[0])
# mean_que.shape
print(np.dot(mean_ans, mean_que)/np.sqrt(np.sum(mean_ans**2) * np.sum(mean_que**2)))
24/71:
def metric(v1, v2):
    return np.dot(v1, v2)/np.sqrt(np.sum(v1**2) * np.sum(v2**2))
24/72: from gensim.similarities import SparseMatrixSimilarity
24/73:
# import random
# rand = random.randint(0, len(que))
rand = 0
# print(rand)
sm = 0
for q in que[rand]:
    sm += model_que.wv[q]
mean_que = sm/len(que[rand])
res = []
for i, x in enumerate(ans):
    sm = 0
#     print(x)
    for a in x:
        sm += model_ans.wv[a]
    mean = sm/len(ans[i])
    met = metric(mean_que, mean)
    
    res.append((met,i))
#     mean_vector = np.mean([word_vectors[word] for word in words_in_vocab], axis=0)

# res.sort()
# res.reverse()
# print(res)
# print(sorted(res)[0])
24/74: # res
24/75: len(ans)
24/76: print(que[rand], ans[104])
24/77: sorted(res, reverse = True)
26/1:
import os
import pandas as pd
import numpy as np
import re
import nltk
import unicodedata
import nltk
import gensim
import math
from sklearn.metrics.pairwise import cosine_similarity
from nltk.corpus import stopwords
26/2:
df1 = pd.read_csv('./NLP-QuestionAnswerSystem/dataset/S08_question_answer_pairs.txt', sep='\t')
df2 = pd.read_csv('./NLP-QuestionAnswerSystem/dataset/S09_question_answer_pairs.txt', sep='\t')
df3 = pd.read_csv('./NLP-QuestionAnswerSystem/dataset/S10_question_answer_pairs.txt', sep='\t', encoding = 'ISO-8859-1')
frames = [df1, df2, df3]
df = pd.concat(frames)

def getArticleText(file):
  fpath = './NLP-QuestionAnswerSystem/dataset/text_data/'+file+'.txt.clean'
  try:
    f = open(fpath, 'r')
    text = f.read()
  except UnicodeDecodeError:
    f = open(fpath, 'r', encoding = 'ISO-8859-1')
    text = f.read()
  return text

df = df.dropna(subset=['ArticleFile'])
df['ArticleText'] = df['ArticleFile'].apply(lambda x: getArticleText(x))
df['ArticleText'] = df['ArticleText'].apply(lambda x: re.sub(r'(\n)+', '. ', x))
df = df.drop(['DifficultyFromQuestioner', 'DifficultyFromAnswerer', 'ArticleFile'], axis='columns')

def cleanQuestion(text):
  text = str(text)
  wnl = nltk.stem.WordNetLemmatizer()
  text = text.lower()
  words = re.sub(r'[^\w\s]', '', text).split()
  return " ".join([word for word in words])

def cleanAnswer(text):
  text = str(text)
  wnl = nltk.stem.WordNetLemmatizer()
  text = text.lower()
  words = re.sub(r'[^\w\s]', '', text).split()
  return " ".join([word for word in words])

def cleanText(text):
  text = str(text)
  wnl = nltk.stem.WordNetLemmatizer()
  text = text.lower()
  words = re.sub(r'[^\w\s\.\?]', '', text).split()
  return " ".join([word for word in words])

df['Question'] = df['Question'].apply(lambda x: cleanQuestion(x))
df['Answer'] = df['Answer'].apply(lambda x: cleanAnswer(x))
df['ArticleText'] = df['ArticleText'].apply(lambda x: cleanText(x))
26/3:
df1 = pd.read_csv('./S08_question_answer_pairs.txt', sep='\t')
df2 = pd.read_csv('./S09_question_answer_pairs.txt', sep='\t')
df3 = pd.read_csv('./S10_question_answer_pairs.txt', sep='\t', encoding = 'ISO-8859-1')
frames = [df1, df2, df3]
df = pd.concat(frames)

def getArticleText(file):
    fpath = './NLP-QuestionAnswerSystem/dataset/text_data/'+file+'.txt.clean'
    try:
        f = open(fpath, 'r')
        text = f.read()
    except UnicodeDecodeError:
        f = open(fpath, 'r', encoding = 'ISO-8859-1')
        text = f.read()
    return text

df = df.dropna(subset=['ArticleFile'])
df['ArticleText'] = df['ArticleFile'].apply(lambda x: getArticleText(x))
df['ArticleText'] = df['ArticleText'].apply(lambda x: re.sub(r'(\n)+', '. ', x))
df = df.drop(['DifficultyFromQuestioner', 'DifficultyFromAnswerer', 'ArticleFile'], axis='columns')

def cleanQuestion(text):
    text = str(text)
    wnl = nltk.stem.WordNetLemmatizer()
    text = text.lower()
    words = re.sub(r'[^\w\s]', '', text).split()
    return " ".join([word for word in words])

def cleanAnswer(text):
    text = str(text)
    wnl = nltk.stem.WordNetLemmatizer()
    text = text.lower()
    words = re.sub(r'[^\w\s]', '', text).split()
    return " ".join([word for word in words])

def cleanText(text):
    text = str(text)
    wnl = nltk.stem.WordNetLemmatizer()
    text = text.lower()
    words = re.sub(r'[^\w\s\.\?]', '', text).split()
    return " ".join([word for word in words])

df['Question'] = df['Question'].apply(lambda x: cleanQuestion(x))
df['Answer'] = df['Answer'].apply(lambda x: cleanAnswer(x))
df['ArticleText'] = df['ArticleText'].apply(lambda x: cleanText(x))
26/4:
df1 = pd.read_csv('./S08_question_answer_pairs.txt', sep='\t')
df2 = pd.read_csv('./S09_question_answer_pairs.txt', sep='\t')
df3 = pd.read_csv('./S10_question_answer_pairs.txt', sep='\t', encoding = 'ISO-8859-1')
frames = [df1, df2, df3]
df = pd.concat(frames)

def getArticleText(file):
    fpath = './NLP-QuestionAnswerSystem/dataset/text_data/'+file+'.txt.clean'
    try:
        f = open(fpath, 'r')
        text = f.read()
    except UnicodeDecodeError:
        f = open(fpath, 'r', encoding = 'ISO-8859-1')
        text = f.read()
    return text

df = df.dropna(subset=['ArticleFile'])
df['ArticleText'] = df['ArticleFile'].apply(lambda x: getArticleText(x))
df['ArticleText'] = df['ArticleText'].apply(lambda x: re.sub(r'(\n)+', '. ', x))
df = df.drop(['DifficultyFromQuestioner', 'DifficultyFromAnswerer', 'ArticleFile'], axis='columns')

def cleanQuestion(text):
    text = str(text)
    wnl = nltk.stem.WordNetLemmatizer()
    text = text.lower()
    words = re.sub(r'[^\w\s]', '', text).split()
    return " ".join([word for word in words])

def cleanAnswer(text):
    text = str(text)
    wnl = nltk.stem.WordNetLemmatizer()
    text = text.lower()
    words = re.sub(r'[^\w\s]', '', text).split()
    return " ".join([word for word in words])

def cleanText(text):
    text = str(text)
    wnl = nltk.stem.WordNetLemmatizer()
    text = text.lower()
    words = re.sub(r'[^\w\s\.\?]', '', text).split()
    return " ".join([word for word in words])

df['Question'] = df['Question'].apply(lambda x: cleanQuestion(x))
df['Answer'] = df['Answer'].apply(lambda x: cleanAnswer(x))
df['ArticleText'] = df['ArticleText'].apply(lambda x: cleanText(x))
26/5:
df1 = pd.read_csv('./S08_question_answer_pairs.txt', sep='\t')
df2 = pd.read_csv('./S09_question_answer_pairs.txt', sep='\t')
df3 = pd.read_csv('./S10_question_answer_pairs.txt', sep='\t', encoding = 'ISO-8859-1')
frames = [df1, df2, df3]
df = pd.concat(frames)

def getArticleText(file):
    fpath = './text_data/'+file+'.txt.clean'
    try:
        f = open(fpath, 'r')
        text = f.read()
    except UnicodeDecodeError:
        f = open(fpath, 'r', encoding = 'ISO-8859-1')
        text = f.read()
    return text

df = df.dropna(subset=['ArticleFile'])
df['ArticleText'] = df['ArticleFile'].apply(lambda x: getArticleText(x))
df['ArticleText'] = df['ArticleText'].apply(lambda x: re.sub(r'(\n)+', '. ', x))
df = df.drop(['DifficultyFromQuestioner', 'DifficultyFromAnswerer', 'ArticleFile'], axis='columns')

def cleanQuestion(text):
    text = str(text)
    wnl = nltk.stem.WordNetLemmatizer()
    text = text.lower()
    words = re.sub(r'[^\w\s]', '', text).split()
    return " ".join([word for word in words])

def cleanAnswer(text):
    text = str(text)
    wnl = nltk.stem.WordNetLemmatizer()
    text = text.lower()
    words = re.sub(r'[^\w\s]', '', text).split()
    return " ".join([word for word in words])

def cleanText(text):
    text = str(text)
    wnl = nltk.stem.WordNetLemmatizer()
    text = text.lower()
    words = re.sub(r'[^\w\s\.\?]', '', text).split()
    return " ".join([word for word in words])

df['Question'] = df['Question'].apply(lambda x: cleanQuestion(x))
df['Answer'] = df['Answer'].apply(lambda x: cleanAnswer(x))
df['ArticleText'] = df['ArticleText'].apply(lambda x: cleanText(x))
26/6:
dataset = []
title = ""
for i in range(0, len(df), 2):
    this_title = df.iloc[i]['ArticleTitle']
    if (this_title!=title):
        title = this_title
        text = df.iloc[i]['ArticleText']
        splitted = text.split(sep='.')
        for j in range(len(splitted)):
            text = splitted[j]
            if(text!=''):
                words = text.split()
                dataset.append(words)
    dataset.append(df.iloc[i]['Question'].split())
    dataset.append(df.iloc[i]['Answer'].split())
26/7:
model = gensim.models.Word2Vec(dataset, size=100, window=8, min_count=1, sg=0, workers=8) # I have 8 cpu cores
# sg = {0, 1} – Training algorithm: 1 for skip-gram; otherwise CBOW
26/8:
model = gensim.models.Word2Vec(dataset, vector_size=100, window=8, min_count=1, sg=0, workers=8) # I have 8 cpu cores
# sg = {0, 1} – Training algorithm: 1 for skip-gram; otherwise CBOW
26/9: model.train(dataset, total_examples=len(dataset), compute_loss=True, epochs=50)
26/10: model.train(dataset, total_examples=len(dataset), compute_loss=True, epochs=50)
26/11:
def get_embedding(sentence):
    pos_sum = [0.0 for i in range(100)]
    num = 0
    words = sentence.split()
    for i in words:
    try:
        embed = model.wv[i]
    except:
        continue
    else:
        pos_sum += embed
        num +=1
    if(num==0):
        return pos_sum
    else:
        pos_sum /= num
        return pos_sum

def get_answer(question, answer_para):
    question_embedding = get_embedding(rem_stop(question))
    min_distance = math.inf
    answer = 0
    for i in range(len(answer_para)):
        answer_embedding = get_embedding(rem_stop(answer_para[i]))
        distance = np.linalg.norm(question_embedding-answer_embedding)
        if (distance < min_distance):
            answer = i
      # print(answer)
            min_distance = distance
    return answer_para[answer]

def rem_stop(sentence):
    strr=''
    my_string = sentence.split()
    for i in range(len(my_string)):
        if my_string[i] not in stopwords.words('english'):
            strr = strr+' '+my_string[i]
    return strr[1:]

def get_answer_cosine(question, answer_para):
    question_embedding = get_embedding(rem_stop(question))
    max_similarity = -math.inf
    answer = 0
    for i in range(len(answer_para)):
        answer_embedding = get_embedding(rem_stop(answer_para[i]))
        similarity = cosine_similarity(np.expand_dims(question_embedding,0), np.expand_dims(answer_embedding,0))
        if (similarity > max_similarity):
            answer = i
            max_similarity = similarity
        return answer_para[answer]
26/12:
def get_embedding(sentence):
    pos_sum = [0.0 for i in range(100)]
    num = 0
    words = sentence.split()
    for i in words:
        try:
            embed = model.wv[i]
        except:
            continue
        else:
            pos_sum += embed
            num +=1
        if(num==0):
            return pos_sum
        else:
            pos_sum /= num
            return pos_sum

def get_answer(question, answer_para):
    question_embedding = get_embedding(rem_stop(question))
    min_distance = math.inf
    answer = 0
    for i in range(len(answer_para)):
        answer_embedding = get_embedding(rem_stop(answer_para[i]))
        distance = np.linalg.norm(question_embedding-answer_embedding)
        if (distance < min_distance):
            answer = i
      # print(answer)
            min_distance = distance
    return answer_para[answer]

def rem_stop(sentence):
    strr=''
    my_string = sentence.split()
    for i in range(len(my_string)):
        if my_string[i] not in stopwords.words('english'):
            strr = strr+' '+my_string[i]
    return strr[1:]

def get_answer_cosine(question, answer_para):
    question_embedding = get_embedding(rem_stop(question))
    max_similarity = -math.inf
    answer = 0
    for i in range(len(answer_para)):
        answer_embedding = get_embedding(rem_stop(answer_para[i]))
        similarity = cosine_similarity(np.expand_dims(question_embedding,0), np.expand_dims(answer_embedding,0))
        if (similarity > max_similarity):
            answer = i
            max_similarity = similarity
        return answer_para[answer]
26/13:
def get_embedding(sentence):
    pos_sum = [0.0 for i in range(100)]
    num = 0
    words = sentence.split()
    for i in words:
        try:
            embed = model.wv[i]
        except:
            continue
        else:
            pos_sum += embed
            num +=1
        if(num==0):
            return pos_sum
        else:
            pos_sum /= num
            return pos_sum

def get_answer(question, answer_para):
    question_embedding = get_embedding(rem_stop(question))
    min_distance = math.inf
    answer = 0
    for i in range(len(answer_para)):
        answer_embedding = get_embedding(rem_stop(answer_para[i]))
        distance = np.linalg.norm(question_embedding-answer_embedding)
        if (distance < min_distance):
            answer = i
      # print(answer)
            min_distance = distance
    return answer_para[answer]

def rem_stop(sentence):
    strr=''
    my_string = sentence.split()
    for i in range(len(my_string)):
        if my_string[i] not in stopwords.words('english'):
            strr = strr+' '+my_string[i]
    return strr[1:]

def get_answer_cosine(question, answer_para):
    question_embedding = get_embedding(rem_stop(question))
    max_similarity = -math.inf
    answer = 0
    for i in range(len(answer_para)):
        answer_embedding = get_embedding(rem_stop(answer_para[i]))
        similarity = cosine_similarity(np.expand_dims(question_embedding,0), np.expand_dims(answer_embedding,0))
        if (similarity > max_similarity):
            answer = i
            max_similarity = similarity
        return answer_para[answer]
26/14:
def get_embedding(sentence):
    pos_sum = [0.0 for i in range(100)]
    num = 0
    words = sentence.split()
    for i in words:
        try:
            embed = model.wv[i]
        except:
            continue
        else:
            pos_sum += embed
            num +=1
        if(num==0):
            return pos_sum
        else:
            pos_sum /= num
            return pos_sum

def get_answer(question, answer_para):
    question_embedding = get_embedding(rem_stop(question))
    min_distance = math.inf
    answer = 0
    for i in range(len(answer_para)):
        answer_embedding = get_embedding(rem_stop(answer_para[i]))
        distance = np.linalg.norm(question_embedding-answer_embedding)
        if (distance < min_distance):
            answer = i
      # print(answer)
            min_distance = distance
    return answer_para[answer]

def rem_stop(sentence):
    strr=''
    my_string = sentence.split()
    for i in range(len(my_string)):
        if my_string[i] not in stopwords.words('english'):
            strr = strr+' '+my_string[i]
    return strr[1:]

def get_answer_cosine(question, answer_para):
    question_embedding = get_embedding(rem_stop(question))
    max_similarity = -math.inf
    answer = 0
    for i in range(len(answer_para)):
        answer_embedding = get_embedding(rem_stop(answer_para[i]))
        similarity = cosine_similarity(np.expand_dims(question_embedding,0), np.expand_dims(answer_embedding,0))
        if (similarity > max_similarity):
            answer = i
            max_similarity = similarity
        return answer_para[answer]
26/15:
index = 296
my_text = df.iloc[index]['ArticleText']
temp_sentences = my_text.split(sep='.')
sentences=[]
for i in range(len(temp_sentences)):
    if(temp_sentences[i]!=''):
        sentences.append(temp_sentences[i])
my_question = df.iloc[index]['Question']
26/16:
print(my_question) # Actual Question
print(rem_stop(my_question)) # Answer without stopwords
print(df.iloc[index]['Answer']) # Actual Answer
26/17:
print(get_answer(my_question, sentences)) # Our model's prediction using euclidean distance
print("\n")
print(get_answer_cosine(my_question, sentences)) # Our model's prediction using cosine similarity
26/18:
df1 = pd.read_csv('./S08_question_answer_pairs.txt', sep='\t')
df2 = pd.read_csv('./S09_question_answer_pairs.txt', sep='\t')
df3 = pd.read_csv('./S10_question_answer_pairs.txt', sep='\t', encoding = 'ISO-8859-1')
frames = [df1, df2, df3]
df = pd.concat(frames)

def getArticleText(file):
    fpath = './text_data/'+file+'.txt.clean'
    try:
        f = open(fpath, 'r')
        text = f.read()
    except UnicodeDecodeError:
        f = open(fpath, 'r', encoding = 'ISO-8859-1')
        text = f.read()
    return text

df = df.dropna(subset=['ArticleFile'])
df['ArticleText'] = df['ArticleFile'].apply(lambda x: getArticleText(x))
df['ArticleText'] = df['ArticleText'].apply(lambda x: re.sub(r'(\n)+', '. ', x))
df = df.drop(['DifficultyFromQuestioner', 'DifficultyFromAnswerer', 'ArticleFile'], axis='columns')

def cleanQuestion(text):
    text = str(text)
    wnl = nltk.stem.WordNetLemmatizer()
    text = text.lower()
    words = re.sub(r'[^\w\s]', '', text).split()
    return " ".join([word for word in words])

def cleanAnswer(text):
    text = str(text)
    wnl = nltk.stem.WordNetLemmatizer()
    text = text.lower()
    words = re.sub(r'[^\w\s]', '', text).split()
    return " ".join([word for word in words])

def cleanText(text):
    text = str(text)
    wnl = nltk.stem.WordNetLemmatizer()
    text = text.lower()
    words = re.sub(r'[^\w\s\.\?]', '', text).split()
    return " ".join([word for word in words])

df['Question'] = df['Question'].apply(lambda x: cleanQuestion(x))
df['Answer'] = df['Answer'].apply(lambda x: cleanAnswer(x))
df['ArticleText'] = df['ArticleText'].apply(lambda x: cleanText(x))
26/19:
dataset = []
title = ""
for i in range(0, len(df), 2):
    this_title = df.iloc[i]['ArticleTitle']
    if (this_title!=title):
        title = this_title
        text = df.iloc[i]['ArticleText']
        splitted = text.split(sep='.')
        for j in range(len(splitted)):
            text = splitted[j]
            if(text!=''):
                words = text.split()
                dataset.append(words)
    dataset.append(df.iloc[i]['Question'].split())
    dataset.append(df.iloc[i]['Answer'].split())
26/20:
model = gensim.models.Word2Vec(dataset, vector_size=100, window=8, min_count=1, sg=0, workers=8) # I have 8 cpu cores
# sg = {0, 1} – Training algorithm: 1 for skip-gram; otherwise CBOW
26/21: model.train(dataset, total_examples=len(dataset), compute_loss=True, epochs=50)
26/22:
def get_embedding(sentence):
    pos_sum = [0.0 for i in range(100)]
    num = 0
    words = sentence.split()
    for i in words:
        try:
            embed = model.wv[i]
        except:
            continue
        else:
            pos_sum += embed
            num +=1
        if(num==0):
            return pos_sum
        else:
            pos_sum /= num
            return pos_sum

def get_answer(question, answer_para):
    question_embedding = get_embedding(rem_stop(question))
    min_distance = math.inf
    answer = 0
    for i in range(len(answer_para)):
        answer_embedding = get_embedding(rem_stop(answer_para[i]))
        distance = np.linalg.norm(question_embedding-answer_embedding)
        if (distance < min_distance):
            answer = i
      # print(answer)
            min_distance = distance
    return answer_para[answer]

def rem_stop(sentence):
    strr=''
    my_string = sentence.split()
    for i in range(len(my_string)):
        if my_string[i] not in stopwords.words('english'):
            strr = strr+' '+my_string[i]
    return strr[1:]

def get_answer_cosine(question, answer_para):
    question_embedding = get_embedding(rem_stop(question))
    max_similarity = -math.inf
    answer = 0
    for i in range(len(answer_para)):
        answer_embedding = get_embedding(rem_stop(answer_para[i]))
        similarity = cosine_similarity(np.expand_dims(question_embedding,0), np.expand_dims(answer_embedding,0))
        if (similarity > max_similarity):
            answer = i
            max_similarity = similarity
        return answer_para[answer]
26/23:
index = 296
my_text = df.iloc[index]['ArticleText']
temp_sentences = my_text.split(sep='.')
sentences=[]
for i in range(len(temp_sentences)):
    if(temp_sentences[i]!=''):
        sentences.append(temp_sentences[i])
my_question = df.iloc[index]['Question']
26/24:
print(my_question) # Actual Question
print(rem_stop(my_question)) # Answer without stopwords
print(df.iloc[index]['Answer']) # Actual Answer
26/25:
print(get_answer(my_question, sentences)) # Our model's prediction using euclidean distance
print("\n")
print(get_answer_cosine(my_question, sentences)) # Our model's prediction using cosine similarity
26/26:
def get_embedding(sentence):
    pos_sum = [0.0 for i in range(100)]
    num = 0
    words = sentence.split()
    for i in words:
        try:
            embed = model.wv[i]
        except:
            continue
        else:
            pos_sum += embed
            num +=1
        if(num==0):
            return pos_sum
        else:
            pos_sum /= num
            return pos_sum

def get_answer(question, answer_para):
    question_embedding = get_embedding(rem_stop(question))
    min_distance = math.inf
    answer = 0
    for i in range(len(answer_para)):
        answer_embedding = get_embedding(rem_stop(answer_para[i]))
        distance = np.linalg.norm(question_embedding-answer_embedding)
        if (distance < min_distance):
            answer = i
            # print(answer)
            min_distance = distance
    return answer_para[answer]

def rem_stop(sentence):
    strr=''
    my_string = sentence.split()
    for i in range(len(my_string)):
        if my_string[i] not in stopwords.words('english'):
            strr = strr+' '+my_string[i]
    return strr[1:]

def get_answer_cosine(question, answer_para):
    question_embedding = get_embedding(rem_stop(question))
    max_similarity = -math.inf
    answer = 0
    for i in range(len(answer_para)):
        answer_embedding = get_embedding(rem_stop(answer_para[i]))
        similarity = cosine_similarity(np.expand_dims(question_embedding,0), np.expand_dims(answer_embedding,0))
        if (similarity > max_similarity):
            answer = i
            max_similarity = similarity
        return answer_para[answer]
26/27:
index = 296
my_text = df.iloc[index]['ArticleText']
temp_sentences = my_text.split(sep='.')
sentences=[]
for i in range(len(temp_sentences)):
    if(temp_sentences[i]!=''):
        sentences.append(temp_sentences[i])
my_question = df.iloc[index]['Question']
26/28:
print(my_question) # Actual Question
print(rem_stop(my_question)) # Answer without stopwords
print(df.iloc[index]['Answer']) # Actual Answer
26/29:
print(get_answer(my_question, sentences)) # Our model's prediction using euclidean distance
print("\n")
print(get_answer_cosine(my_question, sentences)) # Our model's prediction using cosine similarity
26/30:
def get_embedding(sentence):
    pos_sum = [0.0 for i in range(100)]
    num = 0
    words = sentence.split()
    for i in words:
        try:
            embed = model.wv[i]
        except:
            continue
        else:
            pos_sum += embed
            num +=1
        if(num==0):
            return pos_sum
        else:
            pos_sum /= num
            return pos_sum

def get_answer(question, answer_para):
    question_embedding = get_embedding(rem_stop(question))
    
    print(question_embedding)
    min_distance = math.inf
    answer = 0
    for i in range(len(answer_para)):
        answer_embedding = get_embedding(rem_stop(answer_para[i]))
        distance = np.linalg.norm(question_embedding-answer_embedding)
        if (distance < min_distance):
            answer = i
            # print(answer)
            min_distance = distance
    return answer_para[answer]

def rem_stop(sentence):
    strr=''
    my_string = sentence.split()
    for i in range(len(my_string)):
        if my_string[i] not in stopwords.words('english'):
            strr = strr+' '+my_string[i]
    return strr[1:]

def get_answer_cosine(question, answer_para):
    question_embedding = get_embedding(rem_stop(question))
    max_similarity = -math.inf
    answer = 0
    for i in range(len(answer_para)):
        answer_embedding = get_embedding(rem_stop(answer_para[i]))
        similarity = cosine_similarity(np.expand_dims(question_embedding,0), np.expand_dims(answer_embedding,0))
        if (similarity > max_similarity):
            answer = i
            max_similarity = similarity
        return answer_para[answer]
26/31:
def get_embedding(sentence):
    pos_sum = [0.0 for i in range(100)]
    num = 0
    words = sentence.split()
    for i in words:
        try:
            embed = model.wv[i]
        except:
            continue
        else:
            pos_sum += embed
            num +=1
        if(num==0):
            return pos_sum
        else:
            pos_sum /= num
            return pos_sum

def get_answer(question, answer_para):
    question_embedding = get_embedding(rem_stop(question))
    
    print(question_embedding)
    min_distance = math.inf
    answer = 0
    for i in range(len(answer_para)):
        answer_embedding = get_embedding(rem_stop(answer_para[i]))
        distance = np.linalg.norm(question_embedding-answer_embedding)
        if (distance < min_distance):
            answer = i
            # print(answer)
            min_distance = distance
    return answer_para[answer]

def rem_stop(sentence):
    strr=''
    my_string = sentence.split()
    for i in range(len(my_string)):
        if my_string[i] not in stopwords.words('english'):
            strr = strr+' '+my_string[i]
    return strr[1:]

def get_answer_cosine(question, answer_para):
    question_embedding = get_embedding(rem_stop(question))
    max_similarity = -math.inf
    answer = 0
    for i in range(len(answer_para)):
        answer_embedding = get_embedding(rem_stop(answer_para[i]))
        similarity = cosine_similarity(np.expand_dims(question_embedding,0), np.expand_dims(answer_embedding,0))
        if (similarity > max_similarity):
            answer = i
            max_similarity = similarity
        return answer_para[answer]
26/32:
index = 296
my_text = df.iloc[index]['ArticleText']
temp_sentences = my_text.split(sep='.')
sentences=[]
for i in range(len(temp_sentences)):
    if(temp_sentences[i]!=''):
        sentences.append(temp_sentences[i])
my_question = df.iloc[index]['Question']
26/33:
print(my_question) # Actual Question
print(rem_stop(my_question)) # Answer without stopwords
print(df.iloc[index]['Answer']) # Actual Answer
26/34:
print(get_answer(my_question, sentences)) # Our model's prediction using euclidean distance
print("\n")
print(get_answer_cosine(my_question, sentences)) # Our model's prediction using cosine similarity
26/35:
def get_embedding(sentence):
    pos_sum = [0.0 for i in range(100)]
    num = 0
    words = sentence.split()
    for i in words:
        try:
            embed = model.wv[i]
        except:
            continue
        else:
            pos_sum += embed
            num +=1
        if(num==0):
            return pos_sum
        else:
            pos_sum /= num
            return pos_sum

def get_answer(question, answer_para):
    question_embedding = get_embedding(rem_stop(question))
    
    print(answer_embedding)
    min_distance = math.inf
    answer = 0
    for i in range(len(answer_para)):
        answer_embedding = get_embedding(rem_stop(answer_para[i]))
        distance = np.linalg.norm(question_embedding-answer_embedding)
        if (distance < min_distance):
            answer = i
            # print(answer)
            min_distance = distance
    return answer_para[answer]

def rem_stop(sentence):
    strr=''
    my_string = sentence.split()
    for i in range(len(my_string)):
        if my_string[i] not in stopwords.words('english'):
            strr = strr+' '+my_string[i]
    return strr[1:]

def get_answer_cosine(question, answer_para):
    question_embedding = get_embedding(rem_stop(question))
    max_similarity = -math.inf
    answer = 0
    for i in range(len(answer_para)):
        answer_embedding = get_embedding(rem_stop(answer_para[i]))
        similarity = cosine_similarity(np.expand_dims(question_embedding,0), np.expand_dims(answer_embedding,0))
        if (similarity > max_similarity):
            answer = i
            max_similarity = similarity
        return answer_para[answer]
26/36:
index = 296
my_text = df.iloc[index]['ArticleText']
temp_sentences = my_text.split(sep='.')
sentences=[]
for i in range(len(temp_sentences)):
    if(temp_sentences[i]!=''):
        sentences.append(temp_sentences[i])
my_question = df.iloc[index]['Question']
26/37:
print(my_question) # Actual Question
print(rem_stop(my_question)) # Answer without stopwords
print(df.iloc[index]['Answer']) # Actual Answer
26/38:
print(get_answer(my_question, sentences)) # Our model's prediction using euclidean distance
print("\n")
print(get_answer_cosine(my_question, sentences)) # Our model's prediction using cosine similarity
26/39:
def get_embedding(sentence):
    pos_sum = [0.0 for i in range(100)]
    num = 0
    words = sentence.split()
    for i in words:
        try:
            embed = model.wv[i]
        except:
            continue
        else:
            pos_sum += embed
            num +=1
        if(num==0):
            return pos_sum
        else:
            pos_sum /= num
            return pos_sum

def get_answer(question, answer_para):
    question_embedding = get_embedding(rem_stop(question))
    
    print(answer_para)
    min_distance = math.inf
    answer = 0
    for i in range(len(answer_para)):
        answer_embedding = get_embedding(rem_stop(answer_para[i]))
        distance = np.linalg.norm(question_embedding-answer_embedding)
        if (distance < min_distance):
            answer = i
            # print(answer)
            min_distance = distance
    return answer_para[answer]

def rem_stop(sentence):
    strr=''
    my_string = sentence.split()
    for i in range(len(my_string)):
        if my_string[i] not in stopwords.words('english'):
            strr = strr+' '+my_string[i]
    return strr[1:]

def get_answer_cosine(question, answer_para):
    question_embedding = get_embedding(rem_stop(question))
    max_similarity = -math.inf
    answer = 0
    for i in range(len(answer_para)):
        answer_embedding = get_embedding(rem_stop(answer_para[i]))
        similarity = cosine_similarity(np.expand_dims(question_embedding,0), np.expand_dims(answer_embedding,0))
        if (similarity > max_similarity):
            answer = i
            max_similarity = similarity
        return answer_para[answer]
26/40:
index = 296
my_text = df.iloc[index]['ArticleText']
temp_sentences = my_text.split(sep='.')
sentences=[]
for i in range(len(temp_sentences)):
    if(temp_sentences[i]!=''):
        sentences.append(temp_sentences[i])
my_question = df.iloc[index]['Question']
26/41:
print(my_question) # Actual Question
print(rem_stop(my_question)) # Answer without stopwords
print(df.iloc[index]['Answer']) # Actual Answer
26/42:
print(get_answer(my_question, sentences)) # Our model's prediction using euclidean distance
print("\n")
print(get_answer_cosine(my_question, sentences)) # Our model's prediction using cosine similarity
26/43:
def get_embedding(sentence):
    pos_sum = [0.0 for i in range(100)]
    num = 0
    words = sentence.split()
    for i in words:
        try:
            embed = model.wv[i]
        except:
            continue
        else:
            pos_sum += embed
            num +=1
        if(num==0):
            return pos_sum
        else:
            pos_sum /= num
            return pos_sum

def get_answer(question, answer_para):
    question_embedding = get_embedding(rem_stop(question))
    
    min_distance = math.inf
    answer = 0
    for i in range(len(answer_para)):
        answer_embedding = get_embedding(rem_stop(answer_para[i]))
        print(answer_embedding)
        distance = np.linalg.norm(question_embedding-answer_embedding)
        if (distance < min_distance):
            answer = i
            # print(answer)
            min_distance = distance
    return answer_para[answer]

def rem_stop(sentence):
    strr=''
    my_string = sentence.split()
    for i in range(len(my_string)):
        if my_string[i] not in stopwords.words('english'):
            strr = strr+' '+my_string[i]
    return strr[1:]

def get_answer_cosine(question, answer_para):
    question_embedding = get_embedding(rem_stop(question))
    max_similarity = -math.inf
    answer = 0
    for i in range(len(answer_para)):
        answer_embedding = get_embedding(rem_stop(answer_para[i]))
        similarity = cosine_similarity(np.expand_dims(question_embedding,0), np.expand_dims(answer_embedding,0))
        if (similarity > max_similarity):
            answer = i
            max_similarity = similarity
        return answer_para[answer]
26/44:
index = 296
my_text = df.iloc[index]['ArticleText']
temp_sentences = my_text.split(sep='.')
sentences=[]
for i in range(len(temp_sentences)):
    if(temp_sentences[i]!=''):
        sentences.append(temp_sentences[i])
my_question = df.iloc[index]['Question']
26/45:
print(my_question) # Actual Question
print(rem_stop(my_question)) # Answer without stopwords
print(df.iloc[index]['Answer']) # Actual Answer
26/46:
print(get_answer(my_question, sentences)) # Our model's prediction using euclidean distance
print("\n")
print(get_answer_cosine(my_question, sentences)) # Our model's prediction using cosine similarity
26/47:
def get_embedding(sentence):
    pos_sum = [0.0 for i in range(100)]
    num = 0
    words = sentence.split()
    for i in words:
        try:
            embed = model.wv[i]
        except:
            continue
        else:
            pos_sum += embed
            num +=1
    if(num==0):
        return pos_sum
    else:
        pos_sum /= num
        return pos_sum

def get_answer(question, answer_para):
    question_embedding = get_embedding(rem_stop(question))
    
    min_distance = math.inf
    answer = 0
    for i in range(len(answer_para)):
        answer_embedding = get_embedding(rem_stop(answer_para[i]))
        print(answer_embedding)
        distance = np.linalg.norm(question_embedding-answer_embedding)
        if (distance < min_distance):
            answer = i
            # print(answer)
            min_distance = distance
    return answer_para[answer]

def rem_stop(sentence):
    strr=''
    my_string = sentence.split()
    for i in range(len(my_string)):
        if my_string[i] not in stopwords.words('english'):
            strr = strr+' '+my_string[i]
    return strr[1:]

def get_answer_cosine(question, answer_para):
    question_embedding = get_embedding(rem_stop(question))
    max_similarity = -math.inf
    answer = 0
    for i in range(len(answer_para)):
        answer_embedding = get_embedding(rem_stop(answer_para[i]))
        similarity = cosine_similarity(np.expand_dims(question_embedding,0), np.expand_dims(answer_embedding,0))
        if (similarity > max_similarity):
            answer = i
            max_similarity = similarity
        return answer_para[answer]
26/48:
index = 296
my_text = df.iloc[index]['ArticleText']
temp_sentences = my_text.split(sep='.')
sentences=[]
for i in range(len(temp_sentences)):
    if(temp_sentences[i]!=''):
        sentences.append(temp_sentences[i])
my_question = df.iloc[index]['Question']
26/49:
print(my_question) # Actual Question
print(rem_stop(my_question)) # Answer without stopwords
print(df.iloc[index]['Answer']) # Actual Answer
26/50:
print(get_answer(my_question, sentences)) # Our model's prediction using euclidean distance
print("\n")
print(get_answer_cosine(my_question, sentences)) # Our model's prediction using cosine similarity
26/51:
def get_embedding(sentence):
    pos_sum = [0.0 for i in range(100)]
    num = 0
    words = sentence.split()
    for i in words:
        try:
            embed = model.wv[i]
        except:
            continue
        else:
            pos_sum += embed
            num +=1
    if(num==0):
        return pos_sum
    else:
        pos_sum /= num
        return pos_sum

def get_answer(question, answer_para):
    question_embedding = get_embedding(rem_stop(question))
    
    min_distance = math.inf
    answer = 0
    for i in range(len(answer_para)):
        answer_embedding = get_embedding(rem_stop(answer_para[i]))
#         print(answer_embedding)
        distance = np.linalg.norm(question_embedding-answer_embedding)
        if (distance < min_distance):
            answer = i
            # print(answer)
            min_distance = distance
    return answer_para[answer]

def rem_stop(sentence):
    strr=''
    my_string = sentence.split()
    for i in range(len(my_string)):
        if my_string[i] not in stopwords.words('english'):
            strr = strr+' '+my_string[i]
    return strr[1:]

def get_answer_cosine(question, answer_para):
    question_embedding = get_embedding(rem_stop(question))
    max_similarity = -math.inf
    answer = 0
    for i in range(len(answer_para)):
        answer_embedding = get_embedding(rem_stop(answer_para[i]))
        similarity = cosine_similarity(np.expand_dims(question_embedding,0), np.expand_dims(answer_embedding,0))
        if (similarity > max_similarity):
            answer = i
            max_similarity = similarity
        return answer_para[answer]
26/52:
index = 296
my_text = df.iloc[index]['ArticleText']
temp_sentences = my_text.split(sep='.')
sentences=[]
for i in range(len(temp_sentences)):
    if(temp_sentences[i]!=''):
        sentences.append(temp_sentences[i])
my_question = df.iloc[index]['Question']
26/53:
print(my_question) # Actual Question
print(rem_stop(my_question)) # Answer without stopwords
print(df.iloc[index]['Answer']) # Actual Answer
26/54:
print(get_answer(my_question, sentences)) # Our model's prediction using euclidean distance
print("\n")
print(get_answer_cosine(my_question, sentences)) # Our model's prediction using cosine similarity
26/55:
def get_embedding(sentence):
    pos_sum = [0.0 for i in range(100)]
    num = 0
    words = sentence.split()
    print(words)
    for i in words:
        try:
            embed = model.wv[i]
        except:
            continue
        else:
            pos_sum += embed
            num +=1
    if(num==0):
        return pos_sum
    else:
        pos_sum /= num
        return pos_sum

def get_answer(question, answer_para):
    question_embedding = get_embedding(rem_stop(question))
    
    min_distance = math.inf
    answer = 0
    for i in range(len(answer_para)):
        answer_embedding = get_embedding(rem_stop(answer_para[i]))
#         print(answer_embedding)
        distance = np.linalg.norm(question_embedding-answer_embedding)
        if (distance < min_distance):
            answer = i
            # print(answer)
            min_distance = distance
    return answer_para[answer]

def rem_stop(sentence):
    strr=''
    my_string = sentence.split()
    for i in range(len(my_string)):
        if my_string[i] not in stopwords.words('english'):
            strr = strr+' '+my_string[i]
    return strr[1:]

def get_answer_cosine(question, answer_para):
    question_embedding = get_embedding(rem_stop(question))
    max_similarity = -math.inf
    answer = 0
    for i in range(len(answer_para)):
        answer_embedding = get_embedding(rem_stop(answer_para[i]))
        similarity = cosine_similarity(np.expand_dims(question_embedding,0), np.expand_dims(answer_embedding,0))
        if (similarity > max_similarity):
            answer = i
            max_similarity = similarity
        return answer_para[answer]
26/56:
index = 296
my_text = df.iloc[index]['ArticleText']
temp_sentences = my_text.split(sep='.')
sentences=[]
for i in range(len(temp_sentences)):
    if(temp_sentences[i]!=''):
        sentences.append(temp_sentences[i])
my_question = df.iloc[index]['Question']
26/57:
print(my_question) # Actual Question
print(rem_stop(my_question)) # Answer without stopwords
print(df.iloc[index]['Answer']) # Actual Answer
26/58:
print(get_answer(my_question, sentences)) # Our model's prediction using euclidean distance
print("\n")
print(get_answer_cosine(my_question, sentences)) # Our model's prediction using cosine similarity
26/59:
def get_embedding(sentence):
    pos_sum = [0.0 for i in range(100)]
    num = 0
    words = sentence.split()
#     print(words)
    for i in words:
        try:
            embed = model.wv[i]
        except:
            continue
        else:
            pos_sum += embed
            num +=1
    if(num==0):
        return pos_sum
    else:
        pos_sum /= num
        print(pos_sum)
        print(sum(model.wv[i])/len(model.wv[i]))
        return pos_sum

def get_answer(question, answer_para):
    question_embedding = get_embedding(rem_stop(question))
    
    min_distance = math.inf
    answer = 0
    for i in range(len(answer_para)):
        answer_embedding = get_embedding(rem_stop(answer_para[i]))
#         print(answer_embedding)
        distance = np.linalg.norm(question_embedding-answer_embedding)
        if (distance < min_distance):
            answer = i
            # print(answer)
            min_distance = distance
    return answer_para[answer]

def rem_stop(sentence):
    strr=''
    my_string = sentence.split()
    for i in range(len(my_string)):
        if my_string[i] not in stopwords.words('english'):
            strr = strr+' '+my_string[i]
    return strr[1:]

def get_answer_cosine(question, answer_para):
    question_embedding = get_embedding(rem_stop(question))
    max_similarity = -math.inf
    answer = 0
    for i in range(len(answer_para)):
        answer_embedding = get_embedding(rem_stop(answer_para[i]))
        similarity = cosine_similarity(np.expand_dims(question_embedding,0), np.expand_dims(answer_embedding,0))
        if (similarity > max_similarity):
            answer = i
            max_similarity = similarity
        return answer_para[answer]
26/60:
index = 296
my_text = df.iloc[index]['ArticleText']
temp_sentences = my_text.split(sep='.')
sentences=[]
for i in range(len(temp_sentences)):
    if(temp_sentences[i]!=''):
        sentences.append(temp_sentences[i])
my_question = df.iloc[index]['Question']
26/61:
print(my_question) # Actual Question
print(rem_stop(my_question)) # Answer without stopwords
print(df.iloc[index]['Answer']) # Actual Answer
26/62:
print(get_answer(my_question, sentences)) # Our model's prediction using euclidean distance
print("\n")
print(get_answer_cosine(my_question, sentences)) # Our model's prediction using cosine similarity
26/63:
model = gensim.models.Word2Vec(dataset, vector_size=100, window=8, min_count=0, sg=0, workers=8) # I have 8 cpu cores
# sg = {0, 1} – Training algorithm: 1 for skip-gram; otherwise CBOW
26/64: model.train(dataset, total_examples=len(dataset), compute_loss=True, epochs=50)
26/65:
def get_embedding(sentence):
    pos_sum = [0.0 for i in range(100)]
    num = 0
    words = sentence.split()
#     print(words)
    for i in words:
        try:
            embed = model.wv[i]
        except:
            continue
        else:
            pos_sum += embed
            num +=1
    if(num==0):
        return pos_sum
    else:
        pos_sum /= num
        print(num, len(words))
#         print(pos_sum)
#         /len(model.wv[i]))
        return pos_sum

def get_answer(question, answer_para):
    question_embedding = get_embedding(rem_stop(question))
    
    min_distance = math.inf
    answer = 0
    for i in range(len(answer_para)):
        answer_embedding = get_embedding(rem_stop(answer_para[i]))
#         print(answer_embedding)
        distance = np.linalg.norm(question_embedding-answer_embedding)
        if (distance < min_distance):
            answer = i
            # print(answer)
            min_distance = distance
    return answer_para[answer]

def rem_stop(sentence):
    strr=''
    my_string = sentence.split()
    for i in range(len(my_string)):
        if my_string[i] not in stopwords.words('english'):
            strr = strr+' '+my_string[i]
    return strr[1:]

def get_answer_cosine(question, answer_para):
    question_embedding = get_embedding(rem_stop(question))
    max_similarity = -math.inf
    answer = 0
    for i in range(len(answer_para)):
        answer_embedding = get_embedding(rem_stop(answer_para[i]))
        similarity = cosine_similarity(np.expand_dims(question_embedding,0), np.expand_dims(answer_embedding,0))
        if (similarity > max_similarity):
            answer = i
            max_similarity = similarity
        return answer_para[answer]
26/66:
index = 296
my_text = df.iloc[index]['ArticleText']
temp_sentences = my_text.split(sep='.')
sentences=[]
for i in range(len(temp_sentences)):
    if(temp_sentences[i]!=''):
        sentences.append(temp_sentences[i])
my_question = df.iloc[index]['Question']
26/67:
print(my_question) # Actual Question
print(rem_stop(my_question)) # Answer without stopwords
print(df.iloc[index]['Answer']) # Actual Answer
26/68:
print(get_answer(my_question, sentences)) # Our model's prediction using euclidean distance
print("\n")
print(get_answer_cosine(my_question, sentences)) # Our model's prediction using cosine similarity
26/69:
def get_embedding(sentence):
    pos_sum = [0.0 for i in range(100)]
    num = 0
    words = sentence.split()
#     print(words)
    for i in words:
        try:
            embed = model.wv[i]
        except:
            continue
        else:
            pos_sum += embed
            num +=1
    if(num==0):
        return pos_sum
    else:
        pos_sum /= num
#         print(num, len(words))
#         print(pos_sum)
#         /len(model.wv[i]))
        return pos_sum

def get_answer(question, answer_para):
    question_embedding = get_embedding(rem_stop(question))
    
    min_distance = math.inf
    answer = 0
    for i in range(len(answer_para)):
        answer_embedding = get_embedding(rem_stop(answer_para[i]))
#         print(answer_embedding)
        distance = np.linalg.norm(question_embedding-answer_embedding)
        if (distance < min_distance):
            answer = i
            # print(answer)
            min_distance = distance
    return answer_para[answer]

def rem_stop(sentence):
    strr=''
    my_string = sentence.split()
    for i in range(len(my_string)):
        if my_string[i] not in stopwords.words('english'):
            strr = strr+' '+my_string[i]
    return strr[1:]

def get_answer_cosine(question, answer_para):
    question_embedding = get_embedding(rem_stop(question))
    max_similarity = -math.inf
    answer = 0
    for i in range(len(answer_para)):
        answer_embedding = get_embedding(rem_stop(answer_para[i]))
        similarity = cosine_similarity(np.expand_dims(question_embedding,0), np.expand_dims(answer_embedding,0))
        if (similarity > max_similarity):
            answer = i
            max_similarity = similarity
        return answer_para[answer]
26/70:
index = 296
my_text = df.iloc[index]['ArticleText']
temp_sentences = my_text.split(sep='.')
sentences=[]
for i in range(len(temp_sentences)):
    if(temp_sentences[i]!=''):
        sentences.append(temp_sentences[i])
my_question = df.iloc[index]['Question']
26/71:
print(my_question) # Actual Question
print(rem_stop(my_question)) # Answer without stopwords
print(df.iloc[index]['Answer']) # Actual Answer
28/1:
print('PyDev console: using IPython 8.21.0\n')

import sys; print('Python %s on %s' % (sys.version, sys.platform))
sys.path.extend(['C:\\Users\\aliha\\Desktop\\NLP'])
28/2: pip install Scrapy
29/1: response.css("title")
29/2: response.css("title::text").getall()
29/3: response.css("title").getall()
29/4: response.css("title::text").get()
29/5: response.css("title::text")[0].get()
29/6: response.css("title::text")[0].getall()
29/7: response.css("title::text")[0].get()
29/8: response.css("title::text").get()
29/9: response.css("title::text").getall()
29/10: response.css("title").getall()
29/11: response.css("noelement")[0].get()
29/12: scrapy crawl zakon
28/3: conda
28/5: conda
31/1: !pip install SpeechRecognition
31/2:
#import library
import speech_recognition as sr

# Initialize recognizer class (for recognizing the speech)
r = sr.Recognizer()

# Reading Audio file as source
# listening the audio file and store in audio_text variable

with sr.AudioFile('I-dont-know.wav') as source:
    
    audio_text = r.listen(source)
    
# recoginize_() method will throw a request error if the API is unreachable, hence using exception handling
    try:
        
        # using google speech recognition
        text = r.recognize_google(audio_text)
        print('Converting audio transcripts into text ...')
        print(text)
     
    except:
         print('Sorry.. run again...')
31/3:
#import library
import speech_recognition as sr

# Initialize recognizer class (for recognizing the speech)
r = sr.Recognizer()

# Reading Audio file as source
# listening the audio file and store in audio_text variable

with sr.AudioFile('Idon'tknow.wav') as source:
    
    audio_text = r.listen(source)
    
# recoginize_() method will throw a request error if the API is unreachable, hence using exception handling
    try:
        
        # using google speech recognition
        text = r.recognize_google(audio_text)
        print('Converting audio transcripts into text ...')
        print(text)
     
    except:
         print('Sorry.. run again...')
31/4:
#import library
import speech_recognition as sr

# Initialize recognizer class (for recognizing the speech)
r = sr.Recognizer()

# Reading Audio file as source
# listening the audio file and store in audio_text variable

with sr.AudioFile('Idont know.wav') as source:
    
    audio_text = r.listen(source)
    
# recoginize_() method will throw a request error if the API is unreachable, hence using exception handling
    try:
        
        # using google speech recognition
        text = r.recognize_google(audio_text)
        print('Converting audio transcripts into text ...')
        print(text)
     
    except:
         print('Sorry.. run again...')
31/5:
#import library
import speech_recognition as sr

# Initialize recognizer class (for recognizing the speech)
r = sr.Recognizer()

# Reading Audio file as source
# listening the audio file and store in audio_text variable

with sr.AudioFile('I dont know.wav') as source:
    
    audio_text = r.listen(source)
    
# recoginize_() method will throw a request error if the API is unreachable, hence using exception handling
    try:
        
        # using google speech recognition
        text = r.recognize_google(audio_text)
        print('Converting audio transcripts into text ...')
        print(text)
     
    except:
         print('Sorry.. run again...')
31/6:
#import library
import speech_recognition as sr

# Initialize recognizer class (for recognizing the speech)
r = sr.Recognizer()

# Reading Audio file as source
# listening the audio file and store in audio_text variable

with sr.AudioFile('audio) as source:
    
    audio_text = r.listen(source)
    
# recoginize_() method will throw a request error if the API is unreachable, hence using exception handling
    try:
        
        # using google speech recognition
        text = r.recognize_google(audio_text, language= 'ru-RU')
        print('Converting audio transcripts into text ...')
        print(text)
     
    except:
         print('Sorry.. run again...')
31/7:
#import library
import speech_recognition as sr

# Initialize recognizer class (for recognizing the speech)
r = sr.Recognizer()

# Reading Audio file as source
# listening the audio file and store in audio_text variable

with sr.AudioFile('audio.wav') as source:
    
    audio_text = r.listen(source)
    
# recoginize_() method will throw a request error if the API is unreachable, hence using exception handling
    try:
        
        # using google speech recognition
        text = r.recognize_google(audio_text, language= 'ru-RU')
        print('Converting audio transcripts into text ...')
        print(text)
     
    except:
         print('Sorry.. run again...')
31/8: !pip install PyAudio
31/9:
#import library

import speech_recognition as sr

# Initialize recognizer class (for recognizing the speech)

r = sr.Recognizer()

# Reading Microphone as source
# listening the speech and store in audio_text variable

with sr.Microphone() as source:
    print("Talk")
    audio_text = r.listen(source)
    print("Time over, thanks")
# recoginize_() method will throw a request error if the API is unreachable, hence using exception handling
    
    try:
        # using google speech recognition
        print("Text: "+r.recognize_google(audio_text))
    except:
         print("Sorry, I did not get that")
31/10:
#import library

import speech_recognition as sr

# Initialize recognizer class (for recognizing the speech)

r = sr.Recognizer()

# Reading Microphone as source
# listening the speech and store in audio_text variable

with sr.Microphone() as source:
    print("Talk")
    audio_text = r.listen(source)
    print("Time over, thanks")
# recoginize_() method will throw a request error if the API is unreachable, hence using exception handling
    
    try:
        # using google speech recognition
        print("Text: "+r.recognize_google(audio_text))
    except:
         print("Sorry, I did not get that")
31/11:

import speech_recognition as sr


r = sr.Recognizer()


with sr.Microphone() as source:
    print("Talk")
    audio_text = r.listen(source)
    print("Time over, thanks")
    
    try:
        print("Text: "+r.recognize_google(audio_text))
    except:
         print("Sorry, I did not get that")
31/12:

import speech_recognition as sr


r = sr.Recognizer()


with sr.Microphone() as source:
    print("Talk")
    audio_text = r.listen(source)
    print("Time over, thanks")
    
    try:
        print("Text: "+r.recognize_google(audio_text))
    except:
         print("Sorry, I did not get that")
31/13:
import speech_recognition as sr

r = sr.Recognizer()


with sr.AudioFile('I dont know.wav') as source:
    
    audio_text = r.listen(source)
    try:
        
        text = r.recognize_google(audio_text)
        print('Converting audio transcripts into text ...')
        print(text)
     
    except:
         print('Sorry.. run again...')
31/14:
import speech_recognition as sr

r = sr.Recognizer()

with sr.AudioFile('audio.wav') as source:
    
    audio_text = r.listen(source)
    try:
        
        text = r.recognize_google(audio_text, language= 'ru-RU')
        print('Converting audio transcripts into text ...')
        print(text)
     
    except:
         print('Sorry.. run again...')
31/15:

import speech_recognition as sr


r = sr.Recognizer()


with sr.Microphone() as source:
    print("Talk")
    audio_text = r.listen(source)
    print("Time over, thanks")
    
    try:
        print("Text: "+r.recognize_google(audio_text))
    except:
         print("Sorry, I did not get that")
31/16:
import speech_recognition as sr

r = sr.Recognizer()


with sr.AudioFile('I dont know.wav') as source:
    
    audio_text = r.listen(source)
    try:
        
        text = r.recognize_google(audio_text)
        print('Converting audio transcripts into text ...')
        print(text)
     
    except:
         print('Sorry.. run again...')
31/17:  !pip install label-studio
31/18: label-studio start
31/19: !label-studio start
32/1: fetch('https://adilet.zan.kz/rus/docs/K1400000226')
32/2: response.css('div.gs_12')
32/3: response.css('div.gs_12').get()
32/4: response.css('div.gs_12::text').get()
32/5: response.css('div.gs_12::text').getall()
32/6: response.css('div.gs_12::text')
32/7: response.xpath('//div[@class="gs_12"]//text()').getall()
32/8: 
33/1: fetch('https://adilet.zan.kz/rus/docs/K1400000226')
33/2: response.xpath('//div[@class="gs_12"]//text()').getall()
34/1:
"""
This is a simple application for sentence embeddings: semantic search

We have a corpus with various sentences. Then, for a given query sentence,
we want to find the most similar sentence in this corpus.

This script outputs for various queries the top 5 most similar sentences in the corpus.
"""
from sentence_transformers import SentenceTransformer, util
import torch

embedder = SentenceTransformer("all-MiniLM-L6-v2")

# Corpus with example sentences
corpus = [
    "A man is eating food.",
    "A man is eating a piece of bread.",
    "The girl is carrying a baby.",
    "A man is riding a horse.",
    "A woman is playing violin.",
    "Two men pushed carts through the woods.",
    "A man is riding a white horse on an enclosed ground.",
    "A monkey is playing drums.",
    "A cheetah is running behind its prey.",
]
corpus_embeddings = embedder.encode(corpus, convert_to_tensor=True)

# Query sentences:
queries = [
    "A man is eating pasta.",
    "Someone in a gorilla costume is playing a set of drums.",
    "A cheetah chases prey on across a field.",
]


# Find the closest 5 sentences of the corpus for each query sentence based on cosine similarity
top_k = min(5, len(corpus))
for query in queries:
    query_embedding = embedder.encode(query, convert_to_tensor=True)

    # We use cosine-similarity and torch.topk to find the highest 5 scores
    cos_scores = util.cos_sim(query_embedding, corpus_embeddings)[0]
    top_results = torch.topk(cos_scores, k=top_k)

    print("\n\n======================\n\n")
    print("Query:", query)
    print("\nTop 5 most similar sentences in corpus:")

    for score, idx in zip(top_results[0], top_results[1]):
        print(corpus[idx], "(Score: {:.4f})".format(score))

    """
    # Alternatively, we can also use util.semantic_search to perform cosine similarty + topk
    hits = util.semantic_search(query_embedding, corpus_embeddings, top_k=5)
    hits = hits[0]      #Get the hits for the first query
    for hit in hits:
        print(corpus[hit['corpus_id']], "(Score: {:.4f})".format(hit['score']))
    """
34/2: !pip install sentence_transformers
34/3:
"""
This is a simple application for sentence embeddings: semantic search

We have a corpus with various sentences. Then, for a given query sentence,
we want to find the most similar sentence in this corpus.

This script outputs for various queries the top 5 most similar sentences in the corpus.
"""
from sentence_transformers import SentenceTransformer, util
import torch

embedder = SentenceTransformer("all-MiniLM-L6-v2")

# Corpus with example sentences
corpus = [
    "A man is eating food.",
    "A man is eating a piece of bread.",
    "The girl is carrying a baby.",
    "A man is riding a horse.",
    "A woman is playing violin.",
    "Two men pushed carts through the woods.",
    "A man is riding a white horse on an enclosed ground.",
    "A monkey is playing drums.",
    "A cheetah is running behind its prey.",
]
corpus_embeddings = embedder.encode(corpus, convert_to_tensor=True)

# Query sentences:
queries = [
    "A man is eating pasta.",
    "Someone in a gorilla costume is playing a set of drums.",
    "A cheetah chases prey on across a field.",
]


# Find the closest 5 sentences of the corpus for each query sentence based on cosine similarity
top_k = min(5, len(corpus))
for query in queries:
    query_embedding = embedder.encode(query, convert_to_tensor=True)

    # We use cosine-similarity and torch.topk to find the highest 5 scores
    cos_scores = util.cos_sim(query_embedding, corpus_embeddings)[0]
    top_results = torch.topk(cos_scores, k=top_k)

    print("\n\n======================\n\n")
    print("Query:", query)
    print("\nTop 5 most similar sentences in corpus:")

    for score, idx in zip(top_results[0], top_results[1]):
        print(corpus[idx], "(Score: {:.4f})".format(score))

    """
    # Alternatively, we can also use util.semantic_search to perform cosine similarty + topk
    hits = util.semantic_search(query_embedding, corpus_embeddings, top_k=5)
    hits = hits[0]      #Get the hits for the first query
    for hit in hits:
        print(corpus[hit['corpus_id']], "(Score: {:.4f})".format(hit['score']))
    """
35/1:
import pandas as pd
import numpy as np
35/2: df = pd.read_csv('dataset.csv', encoding='utf-8')
35/3: df.head()
35/4: df
35/5: df.head(10)
35/6: df.head(20)
35/7: df.tail(10)
35/8: df = pd.read_csv('dataset.csv')
35/9: df.head(20)
35/10: df = pd.read_csv('dataset.csv')
35/11: df.head(20)
35/12:
import pandas as pd
import numpy as np
35/13: df = pd.read_csv('dataset.csv')
35/14: df.head(20)
35/15:
import pandas as pd
import numpy as np
35/16: df = pd.read_csv('dataset.csv')
35/17: df.head(20)
35/18: df.tail(10)
35/19: df = pd.read_csv('dataset.csv', encoding='utf-8')
35/20: df
35/21: df.to_excel('dataset')
35/22: df.to_excel('dataset.xlsx', index=False)
35/23: !pip install openpyxl
35/24: df.to_excel('dataset.xlsx', index=False)
36/1:
"""
This is a simple application for sentence embeddings: semantic search

We have a corpus with various sentences. Then, for a given query sentence,
we want to find the most similar sentence in this corpus.

This script outputs for various queries the top 5 most similar sentences in the corpus.
"""
from sentence_transformers import SentenceTransformer, util
import torch

embedder = SentenceTransformer("all-MiniLM-L6-v2")

# Corpus with example sentences
corpus = [
    "A man is eating food.",
    "A man is eating a piece of bread.",
    "The girl is carrying a baby.",
    "A man is riding a horse.",
    "A woman is playing violin.",
    "Two men pushed carts through the woods.",
    "A man is riding a white horse on an enclosed ground.",
    "A monkey is playing drums.",
    "A cheetah is running behind its prey.",
]
corpus_embeddings = embedder.encode(corpus, convert_to_tensor=True)

# Query sentences:
queries = [
    "A man is eating pasta.",
    "Someone in a gorilla costume is playing a set of drums.",
    "A cheetah chases prey on across a field.",
]


# Find the closest 5 sentences of the corpus for each query sentence based on cosine similarity
top_k = min(5, len(corpus))
for query in queries:
    query_embedding = embedder.encode(query, convert_to_tensor=True)

    # We use cosine-similarity and torch.topk to find the highest 5 scores
    cos_scores = util.cos_sim(query_embedding, corpus_embeddings)[0]
    top_results = torch.topk(cos_scores, k=top_k)

    print("\n\n======================\n\n")
    print("Query:", query)
    print("\nTop 5 most similar sentences in corpus:")

    for score, idx in zip(top_results[0], top_results[1]):
        print(corpus[idx], "(Score: {:.4f})".format(score))

    """
    # Alternatively, we can also use util.semantic_search to perform cosine similarty + topk
    hits = util.semantic_search(query_embedding, corpus_embeddings, top_k=5)
    hits = hits[0]      #Get the hits for the first query
    for hit in hits:
        print(corpus[hit['corpus_id']], "(Score: {:.4f})".format(hit['score']))
    """
36/2: !pip install -U sentence-transformers
36/3: !pip install --upgrade pip setuptools
36/4: !pip install --upgrade wheel
36/5: !pip install -U sentence-transformers
36/6: pip cache purge
37/1: !pip install -U sentence-transformers
37/2: sudo apt-get install build-essential
37/3: curl.exe --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh
37/4: !pip install -U sentence-transformers
37/5: !pip install -U sentence-transformers
37/6:
"""
This is a simple application for sentence embeddings: semantic search

We have a corpus with various sentences. Then, for a given query sentence,
we want to find the most similar sentence in this corpus.

This script outputs for various queries the top 5 most similar sentences in the corpus.
"""
from sentence_transformers import SentenceTransformer, util
import torch

embedder = SentenceTransformer("all-MiniLM-L6-v2")

# Corpus with example sentences
corpus = [
    "A man is eating food.",
    "A man is eating a piece of bread.",
    "The girl is carrying a baby.",
    "A man is riding a horse.",
    "A woman is playing violin.",
    "Two men pushed carts through the woods.",
    "A man is riding a white horse on an enclosed ground.",
    "A monkey is playing drums.",
    "A cheetah is running behind its prey.",
]
corpus_embeddings = embedder.encode(corpus, convert_to_tensor=True)

# Query sentences:
queries = [
    "A man is eating pasta.",
    "Someone in a gorilla costume is playing a set of drums.",
    "A cheetah chases prey on across a field.",
]


# Find the closest 5 sentences of the corpus for each query sentence based on cosine similarity
top_k = min(5, len(corpus))
for query in queries:
    query_embedding = embedder.encode(query, convert_to_tensor=True)

    # We use cosine-similarity and torch.topk to find the highest 5 scores
    cos_scores = util.cos_sim(query_embedding, corpus_embeddings)[0]
    top_results = torch.topk(cos_scores, k=top_k)

    print("\n\n======================\n\n")
    print("Query:", query)
    print("\nTop 5 most similar sentences in corpus:")

    for score, idx in zip(top_results[0], top_results[1]):
        print(corpus[idx], "(Score: {:.4f})".format(score))

    """
    # Alternatively, we can also use util.semantic_search to perform cosine similarty + topk
    hits = util.semantic_search(query_embedding, corpus_embeddings, top_k=5)
    hits = hits[0]      #Get the hits for the first query
    for hit in hits:
        print(corpus[hit['corpus_id']], "(Score: {:.4f})".format(hit['score']))
    """
38/1:
"""
This is a simple application for sentence embeddings: semantic search

We have a corpus with various sentences. Then, for a given query sentence,
we want to find the most similar sentence in this corpus.

This script outputs for various queries the top 5 most similar sentences in the corpus.
"""
from sentence_transformers import SentenceTransformer, util
import torch

embedder = SentenceTransformer("all-MiniLM-L6-v2")

# Corpus with example sentences
corpus = [
    "A man is eating food.",
    "A man is eating a piece of bread.",
    "The girl is carrying a baby.",
    "A man is riding a horse.",
    "A woman is playing violin.",
    "Two men pushed carts through the woods.",
    "A man is riding a white horse on an enclosed ground.",
    "A monkey is playing drums.",
    "A cheetah is running behind its prey.",
]
corpus_embeddings = embedder.encode(corpus, convert_to_tensor=True)

# Query sentences:
queries = [
    "A man is eating pasta.",
    "Someone in a gorilla costume is playing a set of drums.",
    "A cheetah chases prey on across a field.",
]


# Find the closest 5 sentences of the corpus for each query sentence based on cosine similarity
top_k = min(5, len(corpus))
for query in queries:
    query_embedding = embedder.encode(query, convert_to_tensor=True)

    # We use cosine-similarity and torch.topk to find the highest 5 scores
    cos_scores = util.cos_sim(query_embedding, corpus_embeddings)[0]
    top_results = torch.topk(cos_scores, k=top_k)

    print("\n\n======================\n\n")
    print("Query:", query)
    print("\nTop 5 most similar sentences in corpus:")

    for score, idx in zip(top_results[0], top_results[1]):
        print(corpus[idx], "(Score: {:.4f})".format(score))

    """
    # Alternatively, we can also use util.semantic_search to perform cosine similarty + topk
    hits = util.semantic_search(query_embedding, corpus_embeddings, top_k=5)
    hits = hits[0]      #Get the hits for the first query
    for hit in hits:
        print(corpus[hit['corpus_id']], "(Score: {:.4f})".format(hit['score']))
    """
38/2: !pip install -U sentence-transformers
38/3: pip install --upgrade setuptools
38/4: pip install ez_setup
38/5: pip install unroll
38/6: !pip install -U sentence-transformers
38/7: pip install "package-name"
38/8: python -m pip install --upgrade pip
39/1: pip install "package-name"
40/1: !pip install -U sentence-transformers
40/2: pip install –upgrade setuptools
40/3: pip install –-upgrade setuptools
40/4: pip install -upgrade setuptools
40/5: pip install --upgrade setuptools
40/6: !pip install --upgrade setuptools
40/7: !pip install -U sentence-transformers
41/1:
"""
This is a simple application for sentence embeddings: semantic search

We have a corpus with various sentences. Then, for a given query sentence,
we want to find the most similar sentence in this corpus.

This script outputs for various queries the top 5 most similar sentences in the corpus.
"""
from sentence_transformers import SentenceTransformer, util
import torch

embedder = SentenceTransformer("all-MiniLM-L6-v2")

# Corpus with example sentences
corpus = [
    "A man is eating food.",
    "A man is eating a piece of bread.",
    "The girl is carrying a baby.",
    "A man is riding a horse.",
    "A woman is playing violin.",
    "Two men pushed carts through the woods.",
    "A man is riding a white horse on an enclosed ground.",
    "A monkey is playing drums.",
    "A cheetah is running behind its prey.",
]
corpus_embeddings = embedder.encode(corpus, convert_to_tensor=True)

# Query sentences:
queries = [
    "A man is eating pasta.",
    "Someone in a gorilla costume is playing a set of drums.",
    "A cheetah chases prey on across a field.",
]


# Find the closest 5 sentences of the corpus for each query sentence based on cosine similarity
top_k = min(5, len(corpus))
for query in queries:
    query_embedding = embedder.encode(query, convert_to_tensor=True)

    # We use cosine-similarity and torch.topk to find the highest 5 scores
    cos_scores = util.cos_sim(query_embedding, corpus_embeddings)[0]
    top_results = torch.topk(cos_scores, k=top_k)

    print("\n\n======================\n\n")
    print("Query:", query)
    print("\nTop 5 most similar sentences in corpus:")

    for score, idx in zip(top_results[0], top_results[1]):
        print(corpus[idx], "(Score: {:.4f})".format(score))

    """
    # Alternatively, we can also use util.semantic_search to perform cosine similarty + topk
    hits = util.semantic_search(query_embedding, corpus_embeddings, top_k=5)
    hits = hits[0]      #Get the hits for the first query
    for hit in hits:
        print(corpus[hit['corpus_id']], "(Score: {:.4f})".format(hit['score']))
    """
41/2: !pip install -U sentence-transformers
41/3:
"""
This is a simple application for sentence embeddings: semantic search

We have a corpus with various sentences. Then, for a given query sentence,
we want to find the most similar sentence in this corpus.

This script outputs for various queries the top 5 most similar sentences in the corpus.
"""
from sentence_transformers import SentenceTransformer, util
import torch

embedder = SentenceTransformer("all-MiniLM-L6-v2")

# Corpus with example sentences
corpus = [
    "A man is eating food.",
    "A man is eating a piece of bread.",
    "The girl is carrying a baby.",
    "A man is riding a horse.",
    "A woman is playing violin.",
    "Two men pushed carts through the woods.",
    "A man is riding a white horse on an enclosed ground.",
    "A monkey is playing drums.",
    "A cheetah is running behind its prey.",
]
corpus_embeddings = embedder.encode(corpus, convert_to_tensor=True)

# Query sentences:
queries = [
    "A man is eating pasta.",
    "Someone in a gorilla costume is playing a set of drums.",
    "A cheetah chases prey on across a field.",
]


# Find the closest 5 sentences of the corpus for each query sentence based on cosine similarity
top_k = min(5, len(corpus))
for query in queries:
    query_embedding = embedder.encode(query, convert_to_tensor=True)

    # We use cosine-similarity and torch.topk to find the highest 5 scores
    cos_scores = util.cos_sim(query_embedding, corpus_embeddings)[0]
    top_results = torch.topk(cos_scores, k=top_k)

    print("\n\n======================\n\n")
    print("Query:", query)
    print("\nTop 5 most similar sentences in corpus:")

    for score, idx in zip(top_results[0], top_results[1]):
        print(corpus[idx], "(Score: {:.4f})".format(score))

    """
    # Alternatively, we can also use util.semantic_search to perform cosine similarty + topk
    hits = util.semantic_search(query_embedding, corpus_embeddings, top_k=5)
    hits = hits[0]      #Get the hits for the first query
    for hit in hits:
        print(corpus[hit['corpus_id']], "(Score: {:.4f})".format(hit['score']))
    """
42/1:
"""
This is a simple application for sentence embeddings: semantic search

We have a corpus with various sentences. Then, for a given query sentence,
we want to find the most similar sentence in this corpus.

This script outputs for various queries the top 5 most similar sentences in the corpus.
"""
from sentence_transformers import SentenceTransformer, util
import torch

embedder = SentenceTransformer("all-MiniLM-L6-v2")

# Corpus with example sentences
corpus = [
    "A man is eating food.",
    "A man is eating a piece of bread.",
    "The girl is carrying a baby.",
    "A man is riding a horse.",
    "A woman is playing violin.",
    "Two men pushed carts through the woods.",
    "A man is riding a white horse on an enclosed ground.",
    "A monkey is playing drums.",
    "A cheetah is running behind its prey.",
]
corpus_embeddings = embedder.encode(corpus, convert_to_tensor=True)

# Query sentences:
queries = [
    "A man is eating pasta.",
    "Someone in a gorilla costume is playing a set of drums.",
    "A cheetah chases prey on across a field.",
]


# Find the closest 5 sentences of the corpus for each query sentence based on cosine similarity
top_k = min(5, len(corpus))
for query in queries:
    query_embedding = embedder.encode(query, convert_to_tensor=True)

    # We use cosine-similarity and torch.topk to find the highest 5 scores
    cos_scores = util.cos_sim(query_embedding, corpus_embeddings)[0]
    top_results = torch.topk(cos_scores, k=top_k)

    print("\n\n======================\n\n")
    print("Query:", query)
    print("\nTop 5 most similar sentences in corpus:")

    for score, idx in zip(top_results[0], top_results[1]):
        print(corpus[idx], "(Score: {:.4f})".format(score))

    """
    # Alternatively, we can also use util.semantic_search to perform cosine similarty + topk
    hits = util.semantic_search(query_embedding, corpus_embeddings, top_k=5)
    hits = hits[0]      #Get the hits for the first query
    for hit in hits:
        print(corpus[hit['corpus_id']], "(Score: {:.4f})".format(hit['score']))
    """
42/2: !pip install -U sentence-transformers
42/3:
"""
This is a simple application for sentence embeddings: semantic search

We have a corpus with various sentences. Then, for a given query sentence,
we want to find the most similar sentence in this corpus.

This script outputs for various queries the top 5 most similar sentences in the corpus.
"""
from sentence_transformers import SentenceTransformer, util
import torch

embedder = SentenceTransformer("all-MiniLM-L6-v2")

# Corpus with example sentences
corpus = [
    "A man is eating food.",
    "A man is eating a piece of bread.",
    "The girl is carrying a baby.",
    "A man is riding a horse.",
    "A woman is playing violin.",
    "Two men pushed carts through the woods.",
    "A man is riding a white horse on an enclosed ground.",
    "A monkey is playing drums.",
    "A cheetah is running behind its prey.",
]
corpus_embeddings = embedder.encode(corpus, convert_to_tensor=True)

# Query sentences:
queries = [
    "A man is eating pasta.",
    "Someone in a gorilla costume is playing a set of drums.",
    "A cheetah chases prey on across a field.",
]


# Find the closest 5 sentences of the corpus for each query sentence based on cosine similarity
top_k = min(5, len(corpus))
for query in queries:
    query_embedding = embedder.encode(query, convert_to_tensor=True)

    # We use cosine-similarity and torch.topk to find the highest 5 scores
    cos_scores = util.cos_sim(query_embedding, corpus_embeddings)[0]
    top_results = torch.topk(cos_scores, k=top_k)

    print("\n\n======================\n\n")
    print("Query:", query)
    print("\nTop 5 most similar sentences in corpus:")

    for score, idx in zip(top_results[0], top_results[1]):
        print(corpus[idx], "(Score: {:.4f})".format(score))

    """
    # Alternatively, we can also use util.semantic_search to perform cosine similarty + topk
    hits = util.semantic_search(query_embedding, corpus_embeddings, top_k=5)
    hits = hits[0]      #Get the hits for the first query
    for hit in hits:
        print(corpus[hit['corpus_id']], "(Score: {:.4f})".format(hit['score']))
    """
42/4: from sentence_transformers import SentenceTransformer
43/1:
"""
This is a simple application for sentence embeddings: semantic search

We have a corpus with various sentences. Then, for a given query sentence,
we want to find the most similar sentence in this corpus.

This script outputs for various queries the top 5 most similar sentences in the corpus.
"""
from sentence_transformers import SentenceTransformer, util
import torch

embedder = SentenceTransformer("all-MiniLM-L6-v2")

# Corpus with example sentences
corpus = [
    "A man is eating food.",
    "A man is eating a piece of bread.",
    "The girl is carrying a baby.",
    "A man is riding a horse.",
    "A woman is playing violin.",
    "Two men pushed carts through the woods.",
    "A man is riding a white horse on an enclosed ground.",
    "A monkey is playing drums.",
    "A cheetah is running behind its prey.",
]
corpus_embeddings = embedder.encode(corpus, convert_to_tensor=True)

# Query sentences:
queries = [
    "A man is eating pasta.",
    "Someone in a gorilla costume is playing a set of drums.",
    "A cheetah chases prey on across a field.",
]


# Find the closest 5 sentences of the corpus for each query sentence based on cosine similarity
top_k = min(5, len(corpus))
for query in queries:
    query_embedding = embedder.encode(query, convert_to_tensor=True)

    # We use cosine-similarity and torch.topk to find the highest 5 scores
    cos_scores = util.cos_sim(query_embedding, corpus_embeddings)[0]
    top_results = torch.topk(cos_scores, k=top_k)

    print("\n\n======================\n\n")
    print("Query:", query)
    print("\nTop 5 most similar sentences in corpus:")

    for score, idx in zip(top_results[0], top_results[1]):
        print(corpus[idx], "(Score: {:.4f})".format(score))

    """
    # Alternatively, we can also use util.semantic_search to perform cosine similarty + topk
    hits = util.semantic_search(query_embedding, corpus_embeddings, top_k=5)
    hits = hits[0]      #Get the hits for the first query
    for hit in hits:
        print(corpus[hit['corpus_id']], "(Score: {:.4f})".format(hit['score']))
    """
43/2: pip install sentence-transformers
44/1: pip install faiss-cpu
44/2: !pip install faiss-cpu
45/1:
import os
import getpass

os.environ['OPENAI_API_KEY'] = getpass.getpass('OpenAI API Key:')
45/2:
from langchain_community.document_loaders import TextLoader
from langchain_openai import OpenAIEmbeddings
from langchain.text_splitter import CharacterTextSplitter
from langchain_community.vectorstores import FAISS

# Load the document, split it into chunks, embed each chunk and load it into the vector store.
raw_documents = TextLoader('../../../state_of_the_union.txt').load()
text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)
documents = text_splitter.split_documents(raw_documents)
db = FAISS.from_documents(documents, OpenAIEmbeddings())
45/3: !pip install langchain
45/4:
from langchain_community.document_loaders import TextLoader
from langchain_openai import OpenAIEmbeddings
from langchain.text_splitter import CharacterTextSplitter
from langchain_community.vectorstores import FAISS

# Load the document, split it into chunks, embed each chunk and load it into the vector store.
raw_documents = TextLoader('../../../state_of_the_union.txt').load()
text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)
documents = text_splitter.split_documents(raw_documents)
db = FAISS.from_documents(documents, OpenAIEmbeddings())
45/5: !pip install langchain_community
45/6:
from platform import python_version
print(python_version())
46/1:
from platform import python_version
print(python_version())
46/2: !pip install langchain_community
46/3: !pip install langchain
46/4: --user
46/5: !pip install langchain
46/6:
from langchain_community.document_loaders import TextLoader
from langchain_openai import OpenAIEmbeddings
from langchain.text_splitter import CharacterTextSplitter
from langchain_community.vectorstores import FAISS

# Load the document, split it into chunks, embed each chunk and load it into the vector store.
raw_documents = TextLoader('../../../state_of_the_union.txt').load()
text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)
documents = text_splitter.split_documents(raw_documents)
db = FAISS.from_documents(documents, OpenAIEmbeddings())
46/7: !pip install 'pwd'
46/8: !pip install pwd
46/9: pip install pwd
46/10:
import os
import getpass

os.environ['OPENAI_API_KEY'] = getpass.getpass('OpenAI API Key:')
46/11: !pip install faiss-gpu
46/12:
import os
import getpass

os.environ['OPENAI_API_KEY'] = getpass.getpass('OpenAI API Key:')
46/13:
from langchain_community.document_loaders import TextLoader
from langchain_openai import OpenAIEmbeddings
from langchain.text_splitter import CharacterTextSplitter
from langchain_community.vectorstores import FAISS

# Load the document, split it into chunks, embed each chunk and load it into the vector store.
raw_documents = TextLoader('../../../state_of_the_union.txt').load()
text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)
documents = text_splitter.split_documents(raw_documents)
db = FAISS.from_documents(documents, OpenAIEmbeddings())
46/14:
# from langchain_community.document_loaders import TextLoader
from langchain_openai import OpenAIEmbeddings
from langchain.text_splitter import CharacterTextSplitter
from langchain_community.vectorstores import FAISS

# Load the document, split it into chunks, embed each chunk and load it into the vector store.
raw_documents = TextLoader('../../../state_of_the_union.txt').load()
text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)
documents = text_splitter.split_documents(raw_documents)
db = FAISS.from_documents(documents, OpenAIEmbeddings())
47/1:
from langchain_community.document_loaders import TextLoader
from langchain_openai import OpenAIEmbeddings
from langchain.text_splitter import CharacterTextSplitter
from langchain_community.vectorstores import FAISS

# Load the document, split it into chunks, embed each chunk and load it into the vector store.
raw_documents = TextLoader('../../../state_of_the_union.txt').load()
text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)
documents = text_splitter.split_documents(raw_documents)
db = FAISS.from_documents(documents, OpenAIEmbeddings())
47/2:
import os
import getpass

os.environ['OPENAI_API_KEY'] = getpass.getpass('OpenAI API Key:')
47/3:
from langchain_community.document_loaders import TextLoader
from langchain_openai import OpenAIEmbeddings
from langchain.text_splitter import CharacterTextSplitter
from langchain_community.vectorstores import FAISS

# Load the document, split it into chunks, embed each chunk and load it into the vector store.
raw_documents = TextLoader('../../../state_of_the_union.txt').load()
text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)
documents = text_splitter.split_documents(raw_documents)
db = FAISS.from_documents(documents, OpenAIEmbeddings())
48/1:
from langchain_community.document_loaders import TextLoader
from langchain_openai import OpenAIEmbeddings
from langchain.text_splitter import CharacterTextSplitter
from langchain_community.vectorstores import FAISS

# Load the document, split it into chunks, embed each chunk and load it into the vector store.
raw_documents = TextLoader('../../../state_of_the_union.txt').load()
text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)
documents = text_splitter.split_documents(raw_documents)
db = FAISS.from_documents(documents, OpenAIEmbeddings())
49/1:
import os
import openai
import sys
sys.path.append('../..')

from dotenv import load_dotenv, find_dotenv
_ = load_dotenv(find_dotenv()) # read local .env file

openai.api_key  = os.environ['OPENAI_API_KEY']
49/2: pip install openai
49/3:
import os
import openai
import sys
sys.path.append('../..')

from dotenv import load_dotenv, find_dotenv
_ = load_dotenv(find_dotenv()) # read local .env file

openai.api_key  = os.environ['OPENAI_API_KEY']
49/4: pip install dotenv
50/1:
import os
import openai
import sys
sys.path.append('../..')

from dotenv import load_dotenv, find_dotenv
_ = load_dotenv(find_dotenv()) # read local .env file

openai.api_key  = os.environ['OPENAI_API_KEY']
50/2: !pip3 install dotenv
50/3: pip install python-dotenv
50/4:
import os
import openai
import sys
sys.path.append('../..')

from dotenv import load_dotenv, find_dotenv
_ = load_dotenv(find_dotenv()) # read local .env file

openai.api_key  = os.environ['OPENAI_API_KEY']
50/5: pip install chromadb
50/6: !pip install chromadb
50/7: pip install pytorch
51/1: pip install sentence-transformers
51/2:
import os
import getpass

os.environ['OPENAI_API_KEY'] = getpass.getpass('OpenAI API Key:')
51/3:
import os
import openai
import sys
sys.path.append('../..')

from dotenv import load_dotenv, find_dotenv
_ = load_dotenv(find_dotenv()) # read local .env file

openai.api_key  = os.environ['OPENAI_API_KEY']
51/4:
import os
import openai
import sys
sys.path.append('../..')

# from dotenv import load_dotenv, find_dotenv
# _ = load_dotenv(find_dotenv()) # read local .env file

openai.api_key  = os.environ['OPENAI_API_KEY']
51/5:
import os
import openai
import sys
sys.path.append('../..')

# from dotenv import load_dotenv, find_dotenv
# _ = load_dotenv(find_dotenv()) # read local .env file
os.environ['OPENAI_API_KEY'] = getpass.getpass('OpenAI API Key:')
openai.api_key  = os.environ['OPENAI_API_KEY']
51/6:
import datetime
current_date = datetime.datetime.now().date()
if current_date < datetime.date(2023, 9, 2):
    llm_name = "gpt-3.5-turbo-0301"
else:
    llm_name = "gpt-3.5-turbo"
print(llm_name)
51/7:
from langchain.vectorstores import Chroma
from langchain.embeddings.openai import OpenAIEmbeddings
persist_directory = 'docs/chroma/'
embedding = OpenAIEmbeddings()
vectordb = Chroma(persist_directory=persist_directory, embedding_function=embedding)
51/8:
from langchain.vectorstores import Chroma
from langchain.embeddings.openai import OpenAIEmbeddings
persist_directory = '.docs/chroma/'
embedding = OpenAIEmbeddings()
vectordb = Chroma(persist_directory=persist_directory, embedding_function=embedding)
51/9: python --version
51/10: !python --version
52/1: !python --version
52/2:
from langchain.vectorstores import Chroma
from langchain.embeddings.openai import OpenAIEmbeddings
persist_directory = '.docs/chroma/'
embedding = OpenAIEmbeddings()
vectordb = Chroma(persist_directory=persist_directory, embedding_function=embedding)
52/3:
import os
import openai
import sys
sys.path.append('../..')

# from dotenv import load_dotenv, find_dotenv
# _ = load_dotenv(find_dotenv()) # read local .env file
os.environ['OPENAI_API_KEY'] = getpass.getpass('OpenAI API Key:')
openai.api_key  = os.environ['OPENAI_API_KEY']
52/4: pip install openai
52/5:
import os
import openai
import sys
sys.path.append('../..')

# from dotenv import load_dotenv, find_dotenv
# _ = load_dotenv(find_dotenv()) # read local .env file
os.environ['OPENAI_API_KEY'] = getpass.getpass('OpenAI API Key:')
openai.api_key  = os.environ['OPENAI_API_KEY']
52/6:
import os
import openai
import sys
import getpass
sys.path.append('../..')

# from dotenv import load_dotenv, find_dotenv
# _ = load_dotenv(find_dotenv()) # read local .env file
os.environ['OPENAI_API_KEY'] = getpass.getpass('OpenAI API Key:')
openai.api_key  = os.environ['OPENAI_API_KEY']
52/7:
import datetime
current_date = datetime.datetime.now().date()
if current_date < datetime.date(2023, 9, 2):
    llm_name = "gpt-3.5-turbo-0301"
else:
    llm_name = "gpt-3.5-turbo"
print(llm_name)
53/1:
import os
import getpass

os.environ['OPENAI_API_KEY'] = getpass.getpass('OpenAI API Key:')
53/2:
from langchain_community.document_loaders import TextLoader
from langchain_openai import OpenAIEmbeddings
from langchain.text_splitter import CharacterTextSplitter
from langchain_community.vectorstores import Chroma

# Load the document, split it into chunks, embed each chunk and load it into the vector store.
raw_documents = TextLoader('../../../state_of_the_union.txt').load()
text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)
documents = text_splitter.split_documents(raw_documents)
db = Chroma.from_documents(documents, OpenAIEmbeddings())
53/3: !pip3 install chromadb
53/4:
import chromadb
import pandas as pd
import numpy as np
53/5:
import chromadb
import pandas as pd
import numpy as np
55/1:
import chromadb
import pandas as pd
import numpy as np
55/2: !pip install chromadb
55/3:
import chromadb
import pandas as pd
import numpy as np
55/4:
import chromadb
import pandas as pd
import numpy as np
57/1:
import chromadb
import pandas as pd
import numpy as np
59/1: !pip install chromadb
59/2:
import chromadb
import pandas as pd
import numpy as np
59/3: !python --version
61/1:
import chromadb
import pandas as pd
import numpy as np
61/2: !pip install chromadb
61/3:
import chromadb
import pandas as pd
import numpy as np
61/4: !pip install chromadb
61/5: !python --version
63/1:
"""
This is a simple application for sentence embeddings: semantic search

We have a corpus with various sentences. Then, for a given query sentence,
we want to find the most similar sentence in this corpus.

This script outputs for various queries the top 5 most similar sentences in the corpus.
"""
from sentence_transformers import SentenceTransformer, util
import torch

embedder = SentenceTransformer("all-MiniLM-L6-v2")

# Corpus with example sentences
corpus = [
    "A man is eating food.",
    "A man is eating a piece of bread.",
    "The girl is carrying a baby.",
    "A man is riding a horse.",
    "A woman is playing violin.",
    "Two men pushed carts through the woods.",
    "A man is riding a white horse on an enclosed ground.",
    "A monkey is playing drums.",
    "A cheetah is running behind its prey.",
]
corpus_embeddings = embedder.encode(corpus, convert_to_tensor=True)

# Query sentences:
queries = [
    "A man is eating pasta.",
    "Someone in a gorilla costume is playing a set of drums.",
    "A cheetah chases prey on across a field.",
]


# Find the closest 5 sentences of the corpus for each query sentence based on cosine similarity
top_k = min(5, len(corpus))
for query in queries:
    query_embedding = embedder.encode(query, convert_to_tensor=True)

    # We use cosine-similarity and torch.topk to find the highest 5 scores
    cos_scores = util.cos_sim(query_embedding, corpus_embeddings)[0]
    top_results = torch.topk(cos_scores, k=top_k)

    print("\n\n======================\n\n")
    print("Query:", query)
    print("\nTop 5 most similar sentences in corpus:")

    for score, idx in zip(top_results[0], top_results[1]):
        print(corpus[idx], "(Score: {:.4f})".format(score))

    """
    # Alternatively, we can also use util.semantic_search to perform cosine similarty + topk
    hits = util.semantic_search(query_embedding, corpus_embeddings, top_k=5)
    hits = hits[0]      #Get the hits for the first query
    for hit in hits:
        print(corpus[hit['corpus_id']], "(Score: {:.4f})".format(hit['score']))
    """
63/2: !python --version
63/3:
import chromadb
import pandas as pd
import numpy as np
64/1:
import chromadb
import pandas as pd
import numpy as np
64/2: !python --version
66/1:
import chromadb
import pandas as pd
import numpy as np
66/2:
import chromadb
import pandas as pd
import numpy as np
66/3: pip install chromadb
67/1:
import chromadb
import pandas as pd
import numpy as np
66/4:
import chromadb
import pandas as pd
import numpy as np
68/1: scrapy fetch('https://kodeksy-kz.com/ka/ugolovnyj_kodeks/1.htm')
68/2: fetch('https://kodeksy-kz.com/ka/ugolovnyj_kodeks/1.htm')
68/3: response.css('div.statya.h1')
68/4: response.css('div.statya')
68/5: response.css('div#statya')
68/6: response.css('div#statya.h1')
68/7: response.css('div#statya').css('id)
68/8: response.css('div#statya').css('p')
68/9: response.css('div#statya')get()
68/10: response.css('div#statya').get()
68/11: response.css('div#statya::text').get()
68/12: response.css('div#statya::text').getall()
68/13: response.css('div#statya h1').getall()
68/14: response.css('div#statya h1::text').getall()
68/15: response.css('div#statya h1').getall()
68/16: response.css('div#statya h1 br').getall()
68/17: response.css('div#statya h1 br::text').getall()
68/18: response.css('div#statya h1 br::text')
68/19: response.css('div#statya h1 br')
68/20: response.css('div#statya br')
68/21: response.css('div#statya br::text')
68/22: response.css('div#statya h1getall()').
68/23: response.css('div#statya h1').getall()
68/24: response.css('div#statya h1 br').getall()
68/25: 
68/26: 
69/1: fetch('https://kodeksy-kz.com/ka/ugolovnyj_kodeks/1.htm')
69/2: response.xpath('//div[@id="statya"]//text()').getall()
69/3: response.css('div#statya h1 br ::text').getall()
69/4: response.css('div#statya h1 br ').getall()
69/5: response.css('div#statya h1::text').getall()
69/6: response.css('div#statya ').getall()
69/7: response.css('div#statya::text ').getall()
69/8: response.xpath('//div[@id="statya"]//text()').getall()
69/9: response.css('div#statya::text ').getall()
69/10: response.xpath('//div[@id="statya"]//text()').getall()
69/11: response.xpath('//div[@id="statya"]//text()').get()
69/12: response.xpath('//div[@id="statya"]//text()').getall()
69/13: response.css('div#statya ').getall()
69/14: response.css('div#statya::text ').getall()
69/15: response.css('div#statya br::text ').getall()
69/16: response.css('div#statya br').getall()
69/17: response.css('div#statya::text').getall()
69/18: 
70/1: fetch('https://kodeksy-kz.com/ka/ugolovnyj_kodeks/45.htm')
70/2: response.css('div#statya h1::text').getall()
70/3: response.css('div#statya::text').getall()
70/4: response.css('div#statya::text').getall()
70/5: response.css('div#statya h1::text').getall()
70/6: response.css('div#statya h1::text').getall()
70/7: response.css('div#statya h1::text').getall()
70/8: response.css('div#statya::text').getall()
70/9: response.xpath('//div[@id="statya"]//text()').getall()
71/1: fetch('https://kodeksy-kz.com/ka/ugolovnyj_kodeks/44.htm')
71/2: response.xpath('//div[@id="statya"]//text()').getall()
71/3: response.xpath('//div[@id="statya"]//text()').getal
71/4: eixt
72/1: response.xpath('//div[@id="statya"]//text()').getall
72/2: response.xpath('//div[@id="statya"]//text()').getall()
72/3: response.xpath('//div[@id="statya"]//text()').getall()
72/4: response.xpath('//div[@id="statya"]//text()').getall()
72/5: response.xpath('//div[@id="statya"]//text()').getall()
72/6: fetch('https://kodeksy-kz.com/ka/ugolovnyj_kodeks/44.htm')
72/7: response.xpath('//div[@id="statya"]//text()').getall()
72/8: response.xpath('//div[@id="statya"//*[not(self::h1]//text()').getall()
72/9: response.xpath('//div[@id="statya"*[not(self::h1]//text()').getall()
72/10: response.xpath('//div[@id="statya"*[not(self::h1]//text()').getall()response.xpath('//div[@id="statya"]//*[not(self::h1)]/text()').getall()
72/11: response.xpath('//div[@id="statya"]//*[not(self::h1)]/text()').getall()
73/1: fetch('https://adilet.zan.kz/rus/docs/K1400000226')
73/2: response.css('div.gs_12 h3')
73/3: response.css('div.gs_12 h3::text')
73/4: response.css('div.gs_12 h3::text').getall()
73/5: response.css('div.gs_12 p::text').getall()
73/6: response.css('div.gs_12 h3::text').getall()
73/7: response.css('div.gs_12::text').getall()
73/8: response.css('div.gs_12').getall()
75/1: response.css('div.gs_12 h3::text').getall()
75/2: fetch('https://adilet.zan.kz/rus/docs/K1400000226')
75/3: response.css('div.gs_12 h3::text').getall()
75/4: response.css('div.gs_12 h3 p::text').getall()
75/5: response.css('div.gs_12 h3 p::text').getall()
75/6: response.css('div.gs_12 h3 ').getall()
75/7: response.css('div.gs_12 h3 ::text').getall()
75/8: response.css('div.gs_12 h3 p::text').getall()
75/9: response.css('div.gs_12 p::text').getall()
75/10: response.css('div.gs_12 h3, p::text').getall()
75/11: response.css('div.gs_12 h3 p::text').getall()        paragraphs = response.xpath('//p/text()').getall()
75/12:         paragraphs = response.xpath('//p/text()').getall()
75/13: response.xpath('//p/text()').getall()
75/14: response.xpath('//h3/text()').getall()
75/15:  paragraphs = response.xpath('//p/text()').getall()
75/16:  paragraphs = response.xpath('//p/text()').getall()response.xpath('//p/text()').getall()
75/17: response.xpath('//p/text()').getall()
75/18: response.xpath('//p/text()').get()
75/19: response.xpath('//p/text()').get()
75/20: response.xpath('//h3')
75/21: response.xpath('//h3::text')
75/22: response.xpath('//h3/text')
75/23: response.xpath('//h3//text')
75/24: response.xpath('//h3')
75/25: 
76/1:
print('PyDev console: using IPython 8.21.0\n')

import sys; print('Python %s on %s' % (sys.version, sys.platform))
sys.path.extend(['C:\\Users\\aliha\\Desktop\\NLP'])
76/2:
import scrapy

import requests

class AdiletzanSpider(scrapy.Spider):
    name = "AdiletZan"
    # allowed_domains = [""]
    # start_urls = ["https://kodeksy-kz.com/ka/ugolovnyj_kodeks/1.htm"]
    start_urls = ["https://adilet.zan.kz/rus/docs/K1400000226"]

    def parse(self, response):
        h3_tags = response.xpath('//h3')
        temp = 0
        for h3_tag in h3_tags:
            # Extract text from the current h3 tag
            if temp == 3:
                break
            heading = h3_tag.xpath('./text()').get()
            print(heading)
            following_ps = []
            p_tags = h3_tag.xpath('./following-sibling::p')
            for p_tag in p_tags:
                paragraph = p_tag.xpath('./text()').get()
                if paragraph:
                    following_ps.append(paragraph.strip())
            temp += 1
            print(following_ps)
        # text_content = response.xpath('//div[@class="gs_12"]//text()').getall()
        # cleaned_text = ' '.join(text_content).strip()
        # data = {
        #     'text': cleaned_text
        # }
        #
        # for page in range(1, 468):
        #     # print(page)
        #     yield response.follow(
        #         f'https://kodeksy-kz.com/ka/ugolovnyj_kodeks/{page}.htm',
        #         callback=self.parse_article,
        #         # priority=page
        #     )
        # yield data

    def parse_article(self, response):
        # text_content = response.xpath('//div[@id="statya"]//text()').getall()
        cleaned_text = ' '.join(text_content).strip()
        data = {
            'text':cleaned_text
        }
        # pass
        yield data
77/1: fetch('https://adilet.zan.kz/rus/docs/K1400000226')
77/2: response.xpath('//h3/text()').extract()
78/1: fetch('https://adilet.zan.kz/rus/docs/K1400000226')
78/2: response.xpath('//h3//text()').getall()
78/3: 
79/1: fetch('https://adilet.zan.kz/rus/docs/K1400000226')
79/2: response.xpath('//h3')
79/3: response.xpath('//h3//text')
79/4: response.xpath('//h3/text')
79/5: response.xpath('//h3/text()')
79/6: response.xpath('//h3/text()').get()
79/7: response.xpath('//h3/text()').getall()
79/8: response.xpath('//h3/text()').getall()
80/1:
print('PyDev console: using IPython 8.21.0\n')

import sys; print('Python %s on %s' % (sys.version, sys.platform))
sys.path.extend(['C:\\Users\\aliha\\Desktop\\NLP'])
80/2:
import json

def parse_texts(texts):
    result = {}
    current_section = None
    current_subsection = None
    current_article = None

    for text in texts:
        parts = text.strip().split(" ")
        print(parts)
        if len(parts) == 1:  # It's a section or subsection
            current_section = parts[0]
            result[current_section] = {}
            current_subsection = None
        else:  # It's an article or content
            if parts[1].startswith("РАЗДЕЛ"):
                current_subsection = parts[1]
                result[current_section][current_subsection] = {}
            elif parts[1].startswith("Статья"):
                current_article = parts[1]
                result[current_section][current_subsection][current_article] = []
            else:  # It's content for the current article
                result[current_section][current_subsection][current_article].append(parts[1])

    return result

text1 = [
    "ОБЩАЯ ЧАСТЬ",
    "РАЗДЕЛ 1. УГОЛОВНЫЙ ЗАКОН",
    "Статья 1. Уголовное законодательство Республики Казахстан",
    "Статья 2. Задачи Уголовного кодекса"
]

text2 = """ОБЩАЯ ЧАСТЬ РАЗДЕЛ 1. УГОЛОВНЫЙ ЗАКОН  Статья 1. Уголовное законодательство Республики Казахстан 1. Уголовное законодательство Республики Казахстан состоит из настоящего Уголовного кодекса Республики Казахстан. Иные законы, предусматривающие уголовную ответственность, подлежат применению только после их включения в настоящий Кодекс. 2. Настоящий Кодекс основывается на  Конституции  Республики Казахстан и общепризнанных принципах и нормах международного права.  Конституция  Республики Казахстан имеет высшую юридическую силу и прямое действие на всей территории Республики. В случае противоречий между нормами настоящего Кодекса и Конституции Республики Казахстан действуют положения  Конституции . Нормы настоящего Кодекса, признанные неконституционными, в том числе ущемляющими закрепленные Конституцией Республики Казахстан права и свободы человека и гражданина, утрачивают юридическую силу и не подлежат применению. Нормативные постановления Конституционного Суда и Верховного Суда Республики Казахстан являются составной частью уголовного законодательства Республики Казахстан. 3. Международные договоры, ратифицированные Республикой Казахстан, имеют приоритет перед настоящим Кодексом. Порядок и условия действия на территории Республики Казахстан международных договоров, участником которых является Республика Казахстан, определяются законодательством Республики Казахстан. Сноска. Статья 1 с изменением, внесенным Законом РК от 27.12.2018  № 205-VI  (вводится в действие по истечении десяти календарных дней после дня его первого официального опубликования); от 05.11.2022  № 157-VII  (вводится в действие с 01.01.2023). Статья 2. Задачи Уголовного кодекса 1. Задачами настоящего Кодекса являются: защита прав, свобод и законных интересов человека и гражданина, собственности, прав и законных интересов организаций, общественного порядка и безопасности, окружающей среды, конституционного строя и территориальной целостности Республики Казахстан, охраняемых законом интересов общества и государства от общественно опасных посягательств, охрана мира и безопасности человечества, а также предупреждение уголовных правонарушений. 2. Для осуществления этих задач настоящий Кодекс устанавливает основания уголовной ответственности, определяет, какие опасные для личности, общества или государства деяния являются уголовными правонарушениями, то есть преступлениями или уголовными проступками, устанавливает наказания и иные меры уголовно-правового воздействия за их совершение. Статья 3. Разъяснение некоторых понятий, содержащихся в настоящем Кодексе Содержащиеся в настоящем Кодексе понятия имеют, если нет особых указаний в законе, следующие значения: 1) эксплуатация человека – использование виновным принудительного труда, то есть любой работы или службы, требуемой от лица путем применения насилия или угрозы его применения, для выполнения которой это лицо не предложило добровольно своих услуг, за исключением случаев, предусмотренных законами Республики Казахстан; использование виновным занятия проституцией другим лицом или иных оказываемых им услуг в целях присвоения полученных доходов, а равно принуждение лица к оказанию услуг сексуального характера без преследования виновным этой цели; принуждение лица к занятию попрошайничеством, то есть к совершению антиобщественного деяния, связанного с выпрашиванием у других лиц денег и (или) иного имущества; иные действия, связанные с осуществлением виновным полномочий собственника в отношении лица, которое по не зависящим от него причинам не может отказаться от выполнения работ и (или) услуг для виновного и (или) другого лица; 2) значительный ущерб и значительный размер – в статьях:  198  и  199  – размер ущерба или стоимость прав на использование объектов интеллектуальной собственности либо стоимость экземпляров объектов авторского права и (или) смежных прав или товаров, содержащих изобретения, полезные модели, промышленные образцы, селекционные достижения или топологии интегральных микросхем, в двести  раз превышающие месячный расчетный показатель;  202  и  269-1 – ущерб на сумму, в двести раз превышающую месячный расчетный показатель;  214  – количество товаров, стоимость которых превышает две тысячи месячных расчетных показателей; 217-1 – ущерб на сумму, в двести раз превышающую месячный расчетный показатель;  233  – ущерб, причиненный на сумму,  в одну тысячу раз превышающую месячный расчетный показатель;  325 ,  326 , 328, 334, 335, 337 и 342 – стоимостное выражение затрат, необходимых для устранения экологического ущерба или восстановления потребительских свойств природных ресурсов, в размере, превышающем сто месячных расчетных показателей; 340 – стоимостное выражение затрат, необходимых для устранения экологического ущерба или восстановления потребительских свойств природных ресурсов, в размере, превышающем пятьдесят месячных расчетных показателей; в иных статьях – размер ущерба на сумму, в двести раз превышающую месячный расчетный показатель;  366  и  367  – сумма денег, стоимость ценных бумаг, иного имущества или выгоды имущественного характера от пятидесяти до трех тысяч месячных расчетных показателей; 3) особо крупный ущерб и особо крупный размер – в статьях:  188 , 188-1,  191 ,  192  и 295-1 – стоимость имущества или размер ущерба, в две тысячи раз превышающие месячный расчетный показатель;  189 ,  190 ,  194 , 195,  197 ,  202 ,  204  и  425  – стоимость имущества или размер ущерба, в четыре тысячи раз превышающие месячный расчетный показатель;  214  – доход, сумма которого превышает двадцать тысяч месячных расчетных показателей;  216  – ущерб, причиненный гражданину на сумму, в пять тысяч раз превышающую месячный расчетный показатель, либо ущерб, причиненный организации или государству на сумму, превышающую пятьдесят тысяч месячных расчетных показателей;  217  – доход, сумма которого превышает пять тысяч месячных расчетных показателей;  218-1  – деньги, права на имущество и (или) иное имущество на сумму, превышающую двадцать тысяч месячных расчетных показателей;  229 ,  230  – ущерб на сумму, в двадцать тысяч  раз превышающую месячный расчетный показатель;  234  – стоимость перемещенных товаров, превышающая двадцать тысяч месячных  расчетных показателей;  235-1  – сумма незаконно вывезенной, пересланной или переведенной суммы валютных ценностей, превышающая сто тысяч месячных расчетных показателей;  245  – сумма не поступивших платежей в бюджет, превышающая семьдесят пять тысяч месячных расчетных показателей за проверенный период, при условии, что сумма налогов и других обязательных платежей в бюджет, начисленных по результатам налоговой проверки за один календарный год из проверяемого периода, превышает десять процентов от суммы всех налогов и других обязательных платежей в бюджет, исчисленных налогоплательщиком за этот календарный год;  253  – сумма денег, стоимость ценных бумаг, иного имущества или  выгоды имущественного характера, превышающие две тысячи месячных расчетных показателей;  307  – доход, сумма которого превышает пять тысяч месячных расчетных показателей;  324 ,  325 ,  326 ,  328 ,  329 ,  330 ,  332 ,  333 ,  334 ,  337 ,  339 , 342 и  343  – стоимостное выражение затрат, необходимых для устранения экологического ущерба или восстановления потребительских свойств природных ресурсов, в размере, превышающем двадцать тысяч месячных расчетных показателей;  365  – ущерб, причиненный гражданину на сумму, в две тысячи раз превышающую месячный расчетный показатель, либо ущерб, причиненный организации или государству на сумму, в десять тысяч раз превышающую месячный расчетный показатель;  366  и  367  – сумма денег, стоимость ценных бумаг, иного имущества или выгоды имущественного характера свыше десяти тысяч месячных расчетных показателей; в иных статьях – размер ущерба на сумму, в четыре тысячи раз превышающую месячный расчетный показатель; 4) тяжкие последствия – следующие последствия в случаях, когда они не указаны в качестве признака состава уголовного правонарушения, предусмотренного настоящим Кодексом: смерть человека; смерть двух или более лиц; самоубийство потерпевшего (потерпевшей) или его (ее) близкого (близких); причинение тяжкого вреда здоровью; причинение тяжкого вреда здоровью двум или более лицам; массовое заболевание, заражение, облучение или отравление людей; ухудшение состояния здоровья населения и окружающей среды; наступление нежелательной беременности; наступление техногенного или экологического бедствия, чрезвычайной экологической ситуации; причинение крупного или особо крупного ущерба; срыв исполнения поставленных высшим командованием задач; создание угрозы безопасности государства, катастрофы или аварии; длительное снижение уровня боевой готовности и боеспособности воинских частей и подразделений; срыв выполнения боевой задачи; вывод из строя боевой техники; иные последствия, свидетельствующие о тяжести причиненного вреда; 5) административно-хозяйственные функции – предоставленное в установленном законом Республики Казахстан порядке право управления и распоряжения имуществом, находящимся на балансе организации; 6) воинские уголовные правонарушения – деяния, предусмотренные  главой 18  настоящего Кодекса, направленные против установленного порядка несения воинской службы, совершенные военнослужащими, проходящими воинскую службу по призыву либо по контракту в Вооруженных Силах Республики Казахстан, других войсках и воинских формированиях Республики Казахстан, а также гражданами, пребывающими в запасе, во время прохождения ими сборов; 7) банда – организованная группа, преследующая цель нападения на граждан или организации с применением или угрозой применения оружия либо предметов, используемых в качестве оружия; 8) другие механические транспортные средства – тракторы, мотоциклы, самоходные машины (экскаваторы, автокраны, грейдеры, катки); 9) представитель власти – лицо, находящееся на государственной службе, наделенное в установленном законом Республики Казахстан порядке распорядительными полномочиями в отношении лиц, не находящихся от него в служебной зависимости, в том числе сотрудник правоохранительного или специального государственного органа, военнослужащий органа военной полиции, военнослужащий, участвующий в обеспечении общественного порядка; 10) незначительный размер – в  статье 187  – стоимость имущества, принадлежащего организации, не превышающая десяти месячных расчетных показателей, или имущества, принадлежащего физическому лицу, не превышающая двух месячных расчетных показателей; 11) тяжкий вред здоровью – вред здоровью человека, опасный для его жизни, либо иной вред здоровью, повлекший за собой: потерю зрения, речи, слуха или какого-либо органа; утрату органом его функций; неизгладимое обезображивание лица; расстройство здоровья, соединенное со значительной стойкой утратой общей трудоспособности не менее чем на одну треть; полную утрату профессиональной трудоспособности; прерывание беременности; психическое, поведенческое расстройство (заболевание), в том числе связанное с употреблением психоактивных веществ; 12) средней тяжести вред здоровью – вред здоровью человека, не опасный для его жизни, вызвавший длительное расстройство здоровья (на срок более двадцати одного дня) или значительную стойкую утрату общей трудоспособности (менее чем на одну треть); 13) легкий вред здоровью – вред здоровью человека, повлекший кратковременное расстройство здоровья (на срок не более двадцати одного дня) или незначительную стойкую утрату общей трудоспособности (менее чем на одну десятую часть); 14) существенный вред – следующие последствия в случаях, когда они не указаны в качестве признака состава уголовного правонарушения, предусмотренного настоящим Кодексом: нарушение конституционных прав и свобод человека и гражданина, прав и законных интересов организаций, охраняемых законом интересов общества и государства; причинение значительного ущерба; возникновение трудной жизненной ситуации у потерпевшего лица; нарушение нормальной работы организаций или государственных органов; срыв важных воинских мероприятий либо кратковременное снижение уровня боевой готовности и боеспособности воинских частей и подразделений; несвоевременное обнаружение или отражение нападения вооруженных групп или отдельных вооруженных лиц, сухопутной, воздушной или морской боевой техники, допущение беспрепятственного незаконного перехода через Государственную границу Республики Казахстан лиц и транспортных средств, перемещения контрабандных грузов, попустительство действиям, наносящим ущерб пограничным сооружениям, техническим средствам охраны границы; иные последствия, свидетельствующие о существенности причиненного вреда; 15) наемник – лицо, специально завербованное для участия в вооруженном конфликте, военных действиях или иных насильственных действиях, направленных на свержение или подрыв конституционного строя либо нарушение территориальной целостности государства, действующее в целях получения материального вознаграждения или иной личной выгоды, которое не является гражданином стороны, находящейся в конфликте, или государства, против которого направлены указанные действия, не входит в личный состав вооруженных сил стороны, находящейся в конфликте, не послано другим государством для выполнения официальных обязанностей в качестве лица, входящего в состав его вооруженных сил; 16) лицо, занимающее ответственную государственную должность, – лицо, занимающее должность, которая установлена Конституцией Республики Казахстан, конституционными и иными законами Республики Казахстан для непосредственного исполнения функций государства и полномочий государственных органов, в том числе депутат Парламента Республики Казахстан, судья Конституционного Суда Республики Казахстан, судья, Уполномоченный по правам человека в Республике Казахстан, а равно лицо, занимающее согласно законодательству Республики Казахстан о государственной службе политическую государственную должность либо административную государственную должность корпуса "А"; 17) хищение – совершенные с корыстной целью противоправные безвозмездные изъятие и (или) обращение чужого имущества в пользу виновного или других лиц, причинившие ущерб собственнику или иному владельцу этого имущества; 18) незаконное военизированное формирование – не предусмотренное законодательством Республики Казахстан формирование (объединение, отряд, дружина или иная группа, состоящая из трех и более человек), имеющее организационную структуру военизированного типа, обладающее единоначалием, боеспособностью, жесткой дисциплиной; 19) лицо, выполняющее управленческие функции в коммерческой или иной организации, – лицо, постоянно, временно либо по специальному полномочию выполняющее организационно-распорядительные или административно-хозяйственные обязанности в организации, не являющейся государственным органом, органом местного самоуправления, государственной организацией либо субъектом квазигосударственного сектора; 20) транспорт – железнодорожный, автомобильный, морской, внутренний водный, в том числе морские и речные маломерные суда, воздушный, городской электрический, в том числе метрополитен, а также находящийся на территории Республики Казахстан магистральный трубопроводный транспорт; 20-1) иной тяжкий вред жизненно важным интересам Республики Казахстан – вред, причиненный в результате совершения деяний, предусмотренных частью второй  статьи 160 ,  статьей 163 , частью второй  статьи 164 ,  статьями 168 ,  169 ,  175 , частью третьей  статьи 179 , частью третьей  статьи 180 ,  статьей 181 , частью третьей  статьи 182 ,  статьей 455  настоящего Кодекса; 21) лидер общественного объединения – руководитель общественного объединения, а также иной участник общественного объединения, способный посредством своего влияния и авторитета единолично оказывать управляющее воздействие на деятельность этого общественного объединения; 22) координация преступных действий – согласование между организованными группами (преступными организациями) в целях совместного совершения преступлений (создание устойчивых связей между руководителями или иными участниками организованных групп (преступных организаций), разработка планов, условий для совершения преступлений, а также раздел сфер преступного влияния, доходов от преступной деятельности); 23) преступное сообщество – объединение двух или более преступных организаций, вступивших в сговор для совместного совершения одного или нескольких уголовных правонарушений, а равно создания условий для самостоятельного совершения одного или нескольких уголовных правонарушений любой из этих преступных организаций; 24) преступная группа – организованная группа, преступная организация, преступное сообщество, транснациональная организованная группа, транснациональная преступная организация, транснациональное преступное сообщество, террористическая группа, экстремистская группа, банда, незаконное военизированное формирование; 25) преступная организация – организованная группа, участники которой распределены по организационно, функционально и (или) территориально обособленным группам (структурным подразделениям); 26) должностное лицо – лицо, постоянно, временно или по специальному полномочию осуществляющее функции представителя власти либо выполняющее организационно-распорядительные или административно-хозяйственные функции в государственных органах, органах местного самоуправления, а также в Вооруженных Силах Республики Казахстан, других войсках и воинских формированиях Республики Казахстан; 26-1) принудительный платеж – это обязанность лица, совершившего уголовное правонарушение, по уплате фиксированной денежной суммы, взыскиваемой по обвинительному приговору суда в соответствии с законодательством Республики Казахстан о Фонде компенсации потерпевшим; 27) лицо, уполномоченное на выполнение государственных функций, – лицо, находящееся на государственной службе, депутат маслихата, лицо, временно исполняющее обязанности, предусмотренные государственной должностью, до назначения его на государственную службу, а также лицо, временно назначенное на воинскую должность военнослужащего по контракту офицерского состава или временно исполняющее его обязанности; Примечание ИЗПИ! В подпункт 28) предусмотрено изменение Законом РК от 23.12.2023  № 50-VIII  (вводится в действие с 01.01.2025). 28) лицо, приравненное к лицам, уполномоченным на выполнение государственных функций, – лицо, избранное в органы местного самоуправления; гражданин, зарегистрированный в установленном законом Республики Казахстан порядке в качестве кандидата в Президенты Республики Казахстан, депутаты Парламента Республики Казахстан или маслихатов, акимы районов, городов областного значения, городов районного значения, поселков, сел, сельских округов, а также в члены выборного органа местного самоуправления; служащий, постоянно или временно работающий в органе местного самоуправления, оплата труда которого производится из средств государственного бюджета Республики Казахстан; лицо, исполняющее управленческие функции в государственной организации или субъекте квазигосударственного сектора, а также лицо, уполномоченное на принятие решений по организации и проведению закупок, в том числе государственных, либо ответственное за отбор и реализацию проектов, финансируемых из средств государственного бюджета и Национального фонда Республики Казахстан, занимающее должность не ниже руководителя самостоятельного структурного подразделения в указанных организациях; служащие Национального Банка Республики Казахстан и его ведомств; служащие уполномоченной организации в сфере гражданской авиации, действующей в соответствии с законодательством Республики Казахстан об использовании воздушного пространства Республики Казахстан и деятельности авиации; служащие уполномоченного органа по регулированию, контролю и надзору финансового рынка и финансовых организаций; 28-1) официальный документ – документ, созданный физическим или юридическим лицом, оформленный и удостоверенный в порядке, установленном законодательством Республики Казахстан; 29) коррупционные преступления – деяния, предусмотренные  статьями 189  (пунктом 2) части третьей, частью четвертой в случае наличия признаков, предусмотренных пунктом 2) части третьей),  190  (пунктом 2) части третьей, частью четвертой в случае наличия признаков, предусмотренных пунктом 2) части третьей),  218  (пунктом 1) части третьей),  218-1  (пунктом 1) части четвертой),  234  (пунктом 1) части третьей),  249  (пунктом 2) части третьей),  361 ,  362  (пунктом 3) части четвертой),  364 ,  365 ,  366 ,  367 ,  368 ,  369 ,  370 ,  450 ,  451  (пунктом 2) части второй) и  452  настоящего Кодекса; 30) террористические преступления – деяния, предусмотренные статьями  170 ,  171 ,  173 ,  177 ,  178 ,  184 ,  255 ,  256 ,  257 ,  258 ,  259 ,  260 ,  261 ,  269  и  270  настоящего Кодекса; 31) террористическая группа – организованная группа, преследующая цель совершения одного или нескольких террористических преступлений; 32) лицо, занимающее лидирующее положение, – лицо, наделенное руководителями организованных групп (преступных организаций) полномочиями по координации преступных действий, либо лицо, за которым члены группы признают право брать на себя наиболее ответственные решения, затрагивающие их интересы и определяющие направление и характер их преступной деятельности; 33) транснациональное преступное сообщество – объединение двух или более транснациональных преступных организаций; 34) транснациональная преступная организация – преступная организация, преследующая цель совершения одного или нескольких уголовных правонарушений на территории двух или более государств либо одного государства, при организации совершения деяния или руководстве его исполнением с территории другого государства, а равно при участии граждан другого государства; 35) транснациональная организованная группа – организованная группа, преследующая цель совершения одного или нескольких уголовных правонарушений на территории двух или более государств либо одного государства, при организации совершения деяния или руководстве его исполнением с территории другого государства, а равно при участии граждан другого государства; 36) организованная группа – устойчивая группа двух или более лиц, заранее объединившихся с целью совершения одного или нескольких уголовных правонарушений; 37) организационно-распорядительные функции – предоставленное в установленном законом Республики Казахстан порядке право издавать приказы и распоряжения, обязательные для исполнения подчиненными по службе лицами, а также применять меры поощрения и дисциплинарные взыскания в отношении подчиненных; 37-1) средства, полученные из иностранных источников, – деньги и (или) иное имущество, предоставленные иностранными государствами, международными и иностранными организациями, иностранцами, лицами без гражданства; 38) крупный ущерб и крупный размер – в статьях:  185 ,  186  и  458  – ущерб на сумму, в пятьсот раз превышающую месячный расчетный показатель;  188 , 188-1,  191  и  192  – стоимость имущества или размер ущерба,  в пятьсот раз превышающие месячный расчетный показатель;  189 ,  190 ,  194 ,  195 ,  196 ,  197 ,  200 ,  202 ,  204 ,  269-1 ,  295-1  и  425  – стоимость имущества или размер  ущерба, в одну тысячу раз превышающие месячный расчетный показатель;  198  и  199  – размер ущерба или стоимость прав на использование объектов интеллектуальной собственности либо стоимость экземпляров объектов авторского права и (или) смежных прав или товаров, содержащих изобретения, полезные модели, промышленные образцы, селекционные достижения или топологии интегральных микросхем, в одну тысячу раз превышающие месячный расчетный показатель;  214  – доход, сумма  которого превышает десять тысяч месячных расчетных показателей;  214 ,  221 ,  237 ,  238  (часть первая),  239  (части первая и вторая),  242 ,  243  и  250  (часть вторая) – ущерб, причиненный гражданину на сумму, в две тысячи раз превышающую месячный расчетный показатель, либо ущерб, причиненный организации или государству на сумму, в десять тысяч раз превышающую месячный расчетный показатель; 216 – ущерб, причиненный гражданину на сумму, в две тысячи раз превышающую месячный расчетный показатель, либо ущерб, причиненный организации или государству на сумму, превышающую двадцать тысяч месячных расчетных показателей;  217  – доход, сумма которого превышает одну тысячу месячных расчетных показателей;  218  – деньги и (или) иное имущество, полученные преступным путем, на сумму, превышающую двадцать тысяч месячных расчетных показателей;  218-1  – деньги, права на имущество и (или) иное имущество, на сумму, превышающую десять тысяч месячных расчетных показателей;  219 ,  222 ,  223 ,  224 ,  225 ,  226 ,  227 ,  228  и  241  – ущерб, причиненный гражданину на сумму, в двести раз превышающую месячный расчетный показатель, либо ущерб, причиненный организации или государству на сумму, в две тысячи раз превышающую месячный расчетный показатель;  220 ,  229 ,  230  – ущерб на сумму, в десять тысяч  раз превышающую месячный расчетный показатель;  221  – доход,  сумма которого превышает двадцать тысяч месячных расчетных показателей;  231  – стоимость банкнот, монет, ценных бумаг, иностранной валюты,  в отношении которых совершена подделка, в пятьсот раз превышающая месячный расчетный показатель;  234  – стоимость перемещенных товаров, превышающая десять тысяч месячных расчетных показателей;  235-1  – сумма незаконно вывезенной, пересланной или переведенной суммы валютных ценностей, превышающая сорок пять тысяч месячных расчетных показателей;  236  – стоимость неуплаченных таможенных пошлин, таможенных сборов, налогов, специальных, антидемпинговых, компенсационных пошлин, превышающая пять тысяч месячных расчетных показателей;  238  (часть вторая) – ущерб, причиненный субъекту среднего предпринимательства на сумму, в двадцать тысяч раз превышающую месячный расчетный показатель, либо субъекту крупного предпринимательства на сумму, в сорок тысяч раз превышающую  месячный расчетный показатель;  244  – сумма не поступивших платежей в бюджет, превышающая двадцать тысяч месячных расчетных показателей;  245  – сумма не поступивших платежей в бюджет, превышающая пятьдесят тысяч месячных расчетных показателей за проверенный период, при условии, что сумма налогов и других обязательных платежей в бюджет, начисленных по результатам налоговой проверки за один календарный год из проверяемого периода, превышает десять процентов от суммы всех налогов и других обязательных платежей в бюджет, исчисленных налогоплательщиком за этот календарный год;  247  – полученная лицом сумма или стоимость оказанных ему услуг, превышающая триста месячных расчетных показателей;  253  – сумма денег, стоимость ценных бумаг, иного имущества или выгоды имущественного характера, превышающие пятьсот месячных расчетных показателей;  258  – сумма денег, стоимость имущества, выгоды имущественного характера, оказанных услуг, превышающие одну тысячу месячных расчетных показателей;  274  – ущерб, причиненный гражданину на сумму, в две тысячи раз превышающую месячный расчетный показатель, либо ущерб, причиненный организации или государству на сумму, в десять тысяч раз превышающую месячный расчетный показатель;  292  – ущерб, причиненный физическому лицу на сумму, в одну тысячу раз превышающую месячный расчетный показатель, либо ущерб, причиненный организации или государству на сумму, в две тысячи раз превышающую месячный расчетный показатель;  307  – доход, сумма которого превышает одну тысячу месячных расчетных показателей;  323  – стоимость лекарственных средств и медицинских изделий, в отношении которых совершена фальсификация, превышающая одну тысячу месячных расчетных показателей;  324 ,  325 ,  326 ,  328 ,  329 ,  330 ,  332 ,  333 ,  334 ,  335 ,  337 ,  338 ,  339 , 341, 342 и 343 – стоимостное выражение затрат, необходимых для устранения экологического ущерба или восстановления потребительских свойств природных ресурсов, в размере, превышающем одну тысячу месячных расчетных показателей; 340 – стоимостное выражение затрат, необходимых для устранения экологического ущерба или восстановления потребительских свойств природных ресурсов, в размере, превышающем пятьсот месячных расчетных показателей;  344  – размер ущерба, превышающий две тысячи месячных расчетных показателей;  350 ,  354 ,  355  и  356  – ущерб, причиненный гражданину  в размере, в двести раз превышающем месячный расчетный показатель,  либо ущерб, причиненный организации или государству в размере, в одну тысячу раз превышающем месячный расчетный показатель;  365  – ущерб, причиненный гражданину на сумму, в двести раз превышающую месячный расчетный показатель, либо ущерб, причиненный организации или государству на сумму, в две тысячи раз превышающую месячный расчетный показатель;  366  и  367  – сумма денег, стоимость ценных бумаг, иного имущества или выгоды имущественного характера свыше трех тысяч и  до десяти тысяч месячных расчетных показателей;  399  – стоимость специальных технических средств, превышающая пять тысяч месячных расчетных показателей; в иных статьях – размер ущерба на сумму, в одну тысячу раз превышающую месячный расчетный показатель; 39) экстремистские преступления – деяния, предусмотренные статьями  174 ,  179 ,  180 ,  181 ,  182 ,  184 ,  258 ,  259 ,  260 ,  267 ,  404  (частями второй и третьей) и  405  настоящего Кодекса; 40) экстремистская группа – организованная группа, преследующая цель совершения одного или нескольких экстремистских преступлений; 41) электронный носитель – материальный носитель, предназначенный для хранения информации в электронной форме, а также записи или ее воспроизведения с помощью технических средств; 42) преступления против половой неприкосновенности несовершеннолетних – деяния, предусмотренные  статьями 120  (изнасилование),  121  (насильственные действия сексуального характера),  122  (половое сношение или иные действия сексуального характера с лицом, не достигшим 16-летнего возраста),  123  (понуждение к половому сношению, мужеложству, лесбиянству или иным действиям сексуального характера),  124  (развращение малолетних) ,  134  (вовлечение несовершеннолетнего в занятие проституцией),  144  (вовлечение несовершеннолетних в изготовление продукции эротического содержания), частями второй и третьей  статьи 312  (изготовление и оборот материалов или предметов с порнографическими изображениями несовершеннолетних либо их привлечение для участия в зрелищных мероприятиях порнографического характера) настоящего Кодекса, совершенные в отношении малолетних и несовершеннолетних. Сноска. Статья 3 с изменениями, внесенными законами РК от 07.11.2014  № 248-V  (вводится в действие с 01.01.2015); от 24.11.2015  № 419-V  ( вводится  в действие с 01.01.2016); от 24.11.2015  № 422-V  ( вводится  в действие с 01.01.2016); от 08.04.2016  № 489-V  (вводится в действие по истечении десяти календарных дней после дня его первого официального опубликования); от 09.04.2016  № 501-V  ( вводится  в действие по истечении десяти календарных дней после дня его первого официального опубликования); от 26.07.2016  № 12-VІ  (вводится в действие по истечении двух месяцев после дня его первого официального опубликования); от 03.07.2017  № 84-VI  (вводится в действие по истечении десяти календарных дней после дня его первого официального опубликования); от 11.07.2017  № 91-VI  (вводится в действие по истечении десяти календарных дней после дня его первого официального опубликования); от 26.12.2017  № 124-VI  (вводится в действие с 01.01.2018); от 10.01.2018  № 132-VI  (вводится в действие с 01.07.2018); от 12.07.2018  № 180-VІ  (вводится в действие по истечении десяти календарных дней после дня его первого официального опубликования); от 28.12.2018  № 211-VI  (вводится в действие по истечении десяти календарных дней после дня его первого официального опубликования); от 21.01.2019  № 217-VI  (вводится в действие по истечении десяти календарных дней после дня его первого официального опубликования); от 19.04.2019  № 249-VI  (вводится в действие с 01.08.2019); от 03.07.2019  № 262-VI  (вводится в действие с 01.01.2020); от 28.10.2019  № 268-VI  (вводится в действие по истечении десяти календарных дней после дня его первого официального опубликования); от 27.12.2019  № 290-VІ  (вводится в действие по истечении десяти календарных дней после дня его первого официального опубликования); от 27.12.2019  № 292-VІ  (порядок введения в действие см.  ст.2 ); от 07.07.2020  № 361-VI  (вводится в действие по истечении десяти календарных дней после дня его первого официального опубликования); от 06.10.2020  № 365-VI  (вводится в действие по истечении десяти календарных дней после дня его первого официального опубликования); от 30.12.2020  № 393-VI  (вводится в действие по истечении десяти календарных дней после дня его первого официального опубликования); от 02.01.2021  № 401-VI  (вводится в действие с 01.07.2021); от 02.07.2021  № 62-VII  (вводится в действие по истечении шестидесяти календарных дней после дня его первого официального опубликования); от 12.07.2022  № 139-VII  (вводится в действие по истечении шестидесяти календарных дней после дня его первого официального опубликования); от 05.11.2022  № 157-VII  (порядок введения в действие см.  ст. 3 ); от 03.01.2023  № 186-VII  (вводится в действие по истечении шестидесяти календарных дней после дня его первого официального опубликования); от 03.01.2023  № 188-VII  (вводится в действие по истечении шестидесяти календарных дней после дня его первого официального опубликования); от 14.03.2023  № 206-VII  (вводится в действие по истечении десяти календарных дней после дня его первого официального опубликования); от 12.07.2023  № 23-VIII  (вводится в действие по истечении шестидесяти календарных дней после дня его первого официального опубликования). Статья 4. Основание уголовной ответственности Единственным основанием уголовной ответственности является совершение уголовного правонарушения, то есть деяния, содержащего все признаки состава преступления либо уголовного проступка, предусмотренного настоящим Кодексом. Никто не может быть подвергнут повторно уголовной ответственности за одно и то же уголовное правонарушение. Применение уголовного закона по аналогии не допускается. Статья 5. Действие уголовного закона во времени Преступность и наказуемость деяния определяются законом, действовавшим во время совершения этого деяния. Временем совершения уголовного правонарушения признается время осуществления общественно опасного действия (бездействия) независимо от времени наступления последствий. Статья 6. Обратная сила уголовного закона 1. Закон, устраняющий преступность или наказуемость деяния, смягчающий ответственность или наказание, или иным образом улучшающий положение лица, совершившего уголовное правонарушение, имеет обратную силу, то есть распространяется на лиц, совершивших соответствующее деяние до введения такого закона в действие, в том числе на лиц, отбывающих наказание или отбывших наказание, но имеющих судимость. 2. Если новый уголовный закон смягчает наказуемость деяния, за которое лицо отбывает наказание, то назначенное наказание подлежит сокращению в пределах санкции вновь изданного уголовного закона. 3. Закон, устанавливающий преступность или наказуемость деяния, усиливающий ответственность или наказание, или иным образом ухудшающий положение лица, совершившего это деяние, обратной силы не имеет. Статья 7. Действие уголовного закона в отношении лиц, совершивших уголовное правонарушение на территории Республики Казахстан 1. Лицо, совершившее уголовное правонарушение на территории Республики Казахстан, подлежит ответственности по настоящему Кодексу. 2. Уголовным правонарушением, совершенным на территории Республики Казахстан, признается деяние, которое начато или продолжилось либо было окончено на территории Республики Казахстан. Действие настоящего Кодекса распространяется также на уголовные правонарушения, совершенные на континентальном шельфе и в исключительной экономической зоне Республики Казахстан. 3. Лицо, совершившее уголовное правонарушение на судне, приписанном к порту Республики Казахстан и находящемся в открытом водном или воздушном пространстве вне пределов Республики Казахстан, подлежит уголовной ответственности по настоящему Кодексу, если иное не предусмотрено международным договором Республики Казахстан. По настоящему Кодексу уголовную ответственность несет также лицо, совершившее уголовное правонарушение на военном корабле или военном воздушном судне Республики Казахстан, независимо от места его нахождения. 4. Вопрос об уголовной ответственности дипломатических представителей иностранных государств и иных граждан, которые пользуются иммунитетом, в случае совершения этими лицами уголовного правонарушения на территории Республики Казахстан разрешается в соответствии с нормами международного права. Статья 8. Действие уголовного закона в отношении лиц, совершивших уголовное правонарушение за пределами Республики Казахстан 1. Граждане Республики Казахстан, совершившие уголовное правонарушение за пределами Республики Казахстан, подлежат уголовной ответственности по настоящему Кодексу, если совершенное ими деяние признано уголовно наказуемым в государстве, на территории которого оно было совершено, и если эти лица не были осуждены в другом государстве. При осуждении указанных лиц наказание не может превышать верхнего предела санкции, предусмотренной законом того государства, на территории которого было совершено уголовное правонарушение. На тех же основаниях несут ответственность иностранцы и лица без гражданства, находящиеся на территории Республики Казахстан, в случаях, когда они не могут быть выданы иностранному государству для привлечения к уголовной ответственности или отбывания наказания в соответствии с международным договором Республики Казахстан. Положения настоящего Кодекса применяются независимо от места совершения преступления в отношении граждан Республики Казахстан, лиц без гражданства, постоянно проживающих на территории Республики Казахстан, в случаях совершения террористического или экстремистского преступления либо преступления против мира и безопасности человечества либо за причинение иного тяжкого вреда жизненно важным интересам Республики Казахстан, если иное не установлено международным договором Республики Казахстан. 2. Судимость и иные уголовно-правовые последствия совершения лицом уголовно наказуемого деяния на территории другого государства не имеют уголовно-правового значения для решения вопроса об уголовной ответственности этого лица за уголовное правонарушение, совершенное на территории Республики Казахстан, если иное не предусмотрено международным договором Республики Казахстан или если совершенное на территории другого государства уголовно наказуемое деяние не затрагивало национальных интересов Республики Казахстан. 3. Военнослужащие воинских частей Республики Казахстан, дислоцирующихся за ее пределами, за уголовные правонарушения, совершенные на территории иностранного государства, несут уголовную ответственность по настоящему Кодексу, если иное не предусмотрено международным договором Республики Казахстан. 4. Иностранцы, а также лица без гражданства, постоянно не проживающие на территории Республики Казахстан, совершившие преступление за пределами Республики Казахстан, подлежат уголовной ответственности по настоящему Кодексу в случаях, если это деяние направлено против интересов Республики Казахстан, совершения коррупционного преступления или преступления в сфере экономической деятельности и в случаях, предусмотренных международным договором Республики Казахстан, если они не были осуждены в другом государстве и привлекаются к уголовной ответственности на территории Республики Казахстан. Сноска. Статья 8 с изменениями, внесенными законами РК от 11.07.2017  № 91-VI  (вводится в действие по истечении десяти календарных дней после дня его первого официального опубликования); от 12.07.2023  № 23-VIII  (вводится в действие по истечении шестидесяти календарных дней после дня его первого официального опубликования). Статья 9. Выдача лиц, совершивших уголовное правонарушение 1. Граждане Республики Казахстан, совершившие уголовное правонарушение на территории другого государства, не подлежат выдаче, если иное не установлено международным договором Республики Казахстан. 2. Иностранцы и лица без гражданства, совершившие преступление за пределами Республики Казахстан и находящиеся на территории Республики Казахстан, могут быть выданы иностранному государству для привлечения к уголовной ответственности или отбывания наказания в соответствии с международным договором Республики Казахстан. 3. Никто не может быть выдан иностранному государству, если существуют серьезные основания полагать, что ему в этом государстве может угрожать применение пыток, насилие, другое жестокое или унижающее человеческое достоинство обращение или наказание, а также в случае угрозы применения смертной казни, если иное не предусмотрено международными договорами Республики Казахстан. РАЗДЕЛ 2. УГОЛОВНЫЕ ПРАВОНАРУШЕНИЯ Статья 10. Понятия преступления и уголовного проступка 1. Уголовные правонарушения в зависимости от степени общественной опасности и наказуемости подразделяются на преступления и уголовные проступки."""
import re

text = text2

# Define the pattern to match sections starting with "РАЗДЕЛ"
pattern = re.compile(r'(?<=\b(РАЗДЕЛ \d\.|(?<!РАЗДЕЛ )\b\w(?= статья \d\.)))')


# Split the text based on the pattern
sections = pattern.split(text)
print(sections)
# Print the sections
# for section in sections:
#     # print()
#     print(section.strip())
81/1:
print('PyDev console: using IPython 8.21.0\n')

import sys; print('Python %s on %s' % (sys.version, sys.platform))
sys.path.extend(['C:\\Users\\aliha\\Desktop\\NLP'])
81/2:
import json
import re


with open('new.txt', 'r',encoding='utf-8') as f:
    content = f.read()


text = content
# Define the pattern to match sections starting with "РАЗДЕЛ"
# sections = re.split(r'(?=ЧАСТЬ \d+\.)', text)

# print(text)
sections = re.split(r'(?=ОБЩАЯ ЧАСТЬ \d+\.)', text)
# print(sections)
# Extracting words before each "Статья"
for section in sections:
    subsections = re.split(r'(?=РАЗДЕЛ \d+\.)', section)
    text = subsections[1]
    # print(text)
    inner_sections = re.split(r'(?=РАЗДЕЛ \d+\.)', text)

    for inner_section in inner_sections:
        subsections = re.split(r'(?=Статья \d+\.)', inner_section)
        # print(subsections)
    # print(subsections)
    # for i in range(1, len(subsections)):
    #     words_before_st = re.findall(r'\b\w+\b(?= Статья)', subsections[i-1])
    #     if words_before_st:
    #         print(' '.join(words_before_st))
    # print(subsections[i])
# Define the phrases to split the text

text =content
def split_text_by_phrases(text, phrase1, phrase2):
    # Find the indices where the phrases occur
    index1 = text.find(phrase1)
    index2 = text.find(phrase2)

    # Ensure both phrases are found
    if index1 != -1 and index2 != -1:
        # Extract the sections based on the phrase indices
        first_section = text[:index2].strip()  # section before the second phrase
        second_section = text[index2:].strip()  # section from the second phrase onwards
        return first_section, second_section
    else:
        # Phrases not found
        return None, None
phrase1 = "ОБЩАЯ ЧАСТЬ"
phrase2 = "ОСОБЕННАЯ ЧАСТЬ"

section1, section2 = split_text_by_phrases(text, phrase1, phrase2)

print(section1)
text = section1
sections = re.split(r'(?=РАЗДЕЛ \d+\.)', text)
# print(sections)
# Extracting words before each "Статья"
for section in sections:
    subsections = re.split(r'(?=Статья \d+\.)', section)
    # print(subsections)
    for i in range(1, len(subsections)):
        words_before_st = re.findall(r'\b\w+\b(?= Статья)', subsections[i-1])
        # if words_before_st:
        #     print(' '.join(words_before_st))
        # print(subsections[i])
85/1:
"""
This is a simple application for sentence embeddings: semantic search

We have a corpus with various sentences. Then, for a given query sentence,
we want to find the most similar sentence in this corpus.

This script outputs for various queries the top 5 most similar sentences in the corpus.
"""
from sentence_transformers import SentenceTransformer, util
import torch

embedder = SentenceTransformer("all-MiniLM-L6-v2")

# Corpus with example sentences
corpus = [
    "A man is eating food.",
    "A man is eating a piece of bread.",
    "The girl is carrying a baby.",
    "A man is riding a horse.",
    "A woman is playing violin.",
    "Two men pushed carts through the woods.",
    "A man is riding a white horse on an enclosed ground.",
    "A monkey is playing drums.",
    "A cheetah is running behind its prey.",
]
corpus_embeddings = embedder.encode(corpus, convert_to_tensor=True)

# Query sentences:
queries = [
    "A man is eating pasta.",
    "Someone in a gorilla costume is playing a set of drums.",
    "A cheetah chases prey on across a field.",
]


# Find the closest 5 sentences of the corpus for each query sentence based on cosine similarity
top_k = min(5, len(corpus))
for query in queries:
    query_embedding = embedder.encode(query, convert_to_tensor=True)

    # We use cosine-similarity and torch.topk to find the highest 5 scores
    cos_scores = util.cos_sim(query_embedding, corpus_embeddings)[0]
    top_results = torch.topk(cos_scores, k=top_k)

    print("\n\n======================\n\n")
    print("Query:", query)
    print("\nTop 5 most similar sentences in corpus:")

    for score, idx in zip(top_results[0], top_results[1]):
        print(corpus[idx], "(Score: {:.4f})".format(score))

    """
    # Alternatively, we can also use util.semantic_search to perform cosine similarty + topk
    hits = util.semantic_search(query_embedding, corpus_embeddings, top_k=5)
    hits = hits[0]      #Get the hits for the first query
    for hit in hits:
        print(corpus[hit['corpus_id']], "(Score: {:.4f})".format(hit['score']))
    """
85/2: pip install sentence-transformers
87/1:
import chromadb
import pandas as pd
import numpy as np
87/2:
"""
This is a simple application for sentence embeddings: semantic search

We have a corpus with various sentences. Then, for a given query sentence,
we want to find the most similar sentence in this corpus.

This script outputs for various queries the top 5 most similar sentences in the corpus.
"""
from sentence_transformers import SentenceTransformer, util
import torch

embedder = SentenceTransformer("all-MiniLM-L6-v2")

# Corpus with example sentences
corpus = [
    "A man is eating food.",
    "A man is eating a piece of bread.",
    "The girl is carrying a baby.",
    "A man is riding a horse.",
    "A woman is playing violin.",
    "Two men pushed carts through the woods.",
    "A man is riding a white horse on an enclosed ground.",
    "A monkey is playing drums.",
    "A cheetah is running behind its prey.",
]
corpus_embeddings = embedder.encode(corpus, convert_to_tensor=True)

# Query sentences:
queries = [
    "A man is eating pasta.",
    "Someone in a gorilla costume is playing a set of drums.",
    "A cheetah chases prey on across a field.",
]


# Find the closest 5 sentences of the corpus for each query sentence based on cosine similarity
top_k = min(5, len(corpus))
for query in queries:
    query_embedding = embedder.encode(query, convert_to_tensor=True)

    # We use cosine-similarity and torch.topk to find the highest 5 scores
    cos_scores = util.cos_sim(query_embedding, corpus_embeddings)[0]
    top_results = torch.topk(cos_scores, k=top_k)

    print("\n\n======================\n\n")
    print("Query:", query)
    print("\nTop 5 most similar sentences in corpus:")

    for score, idx in zip(top_results[0], top_results[1]):
        print(corpus[idx], "(Score: {:.4f})".format(score))

    """
    # Alternatively, we can also use util.semantic_search to perform cosine similarty + topk
    hits = util.semantic_search(query_embedding, corpus_embeddings, top_k=5)
    hits = hits[0]      #Get the hits for the first query
    for hit in hits:
        print(corpus[hit['corpus_id']], "(Score: {:.4f})".format(hit['score']))
    """
87/3: !python --version
87/4: pip install sentence-transformers
87/5:
"""
This is a simple application for sentence embeddings: semantic search

We have a corpus with various sentences. Then, for a given query sentence,
we want to find the most similar sentence in this corpus.

This script outputs for various queries the top 5 most similar sentences in the corpus.
"""
from sentence_transformers import SentenceTransformer, util
import torch

embedder = SentenceTransformer("all-MiniLM-L6-v2")

# Corpus with example sentences
corpus = [
    "A man is eating food.",
    "A man is eating a piece of bread.",
    "The girl is carrying a baby.",
    "A man is riding a horse.",
    "A woman is playing violin.",
    "Two men pushed carts through the woods.",
    "A man is riding a white horse on an enclosed ground.",
    "A monkey is playing drums.",
    "A cheetah is running behind its prey.",
]
corpus_embeddings = embedder.encode(corpus, convert_to_tensor=True)

# Query sentences:
queries = [
    "A man is eating pasta.",
    "Someone in a gorilla costume is playing a set of drums.",
    "A cheetah chases prey on across a field.",
]


# Find the closest 5 sentences of the corpus for each query sentence based on cosine similarity
top_k = min(5, len(corpus))
for query in queries:
    query_embedding = embedder.encode(query, convert_to_tensor=True)

    # We use cosine-similarity and torch.topk to find the highest 5 scores
    cos_scores = util.cos_sim(query_embedding, corpus_embeddings)[0]
    top_results = torch.topk(cos_scores, k=top_k)

    print("\n\n======================\n\n")
    print("Query:", query)
    print("\nTop 5 most similar sentences in corpus:")

    for score, idx in zip(top_results[0], top_results[1]):
        print(corpus[idx], "(Score: {:.4f})".format(score))

    """
    # Alternatively, we can also use util.semantic_search to perform cosine similarty + topk
    hits = util.semantic_search(query_embedding, corpus_embeddings, top_k=5)
    hits = hits[0]      #Get the hits for the first query
    for hit in hits:
        print(corpus[hit['corpus_id']], "(Score: {:.4f})".format(hit['score']))
    """
87/6: import json
87/7:
with open('test.json', encoding='utf-8') as f:
    data = json.load(f)
87/8: data
87/9:
articles = []
for x in data:
    print(x)
#     for k in data[x]:
#         for a in data[x][k]:
#             questions.append(a['вопрос'])
#             answers.append(a['ответ'])
87/10:
articles = []
for x in data:
    
    for k in data[x]:
        print(k)
#         for a in data[x][k]:
#             questions.append(a['вопрос'])
#             answers.append(a['ответ'])
87/11:
articles = []
for x in data:    
    for k in data[x]:
        for a in data[x][k]:
            print(a)
#             questions.append(a['вопрос'])
#             answers.append(a['ответ'])
87/12:
articles = []
for x in data:    
    for k in data[x]:
        for a in data[x][k]:
#             print(a)
            temp = a + " " +data[x][k][a]
            articles.append(a)
#             questions.append(a['вопрос'])
#             answers.append(a['ответ'])

articles
87/13:
articles = []
for x in data:    
    for k in data[x]:
        for a in data[x][k]:
#             print(a)
            temp = a + " " +data[x][k][a]
            articles.append(temp)
#             questions.append(a['вопрос'])
#             answers.append(a['ответ'])

articles
87/14:
articles = []
for x in data:    
    for k in data[x]:
        for a in data[x][k]:
#             print(a)
            temp = a + " " +data[x][k][a]
            text.replace('\n\xa0\xa0\xa0\xa0\xa0', '')
            articles.append(temp)
#             questions.append(a['вопрос'])
#             answers.append(a['ответ'])

articles
87/15:
articles = []
for x in data:    
    for k in data[x]:
        for a in data[x][k]:
#             print(a)
#             first = a
            second = data[x][k][a]
            second.replace('\n\xa0\xa0\xa0\xa0\xa0', '')
            print(second)
#             articles.append(temp)
#             questions.append(a['вопрос'])
#             answers.append(a['ответ'])

articles
87/16:
articles = []
for x in data:    
    for k in data[x]:
        for a in data[x][k]:
#             print(a)
#             first = a
            second = data[x][k][a]
            second.replace('\n\xa0\xa0\xa0\xa0\xa0', '')
            articles.append(second)
#             questions.append(a['вопрос'])
#             answers.append(a['ответ'])

articles
87/17:
articles = []
for x in data:    
    for k in data[x]:
        for a in data[x][k]:
#             print(a)
#             first = a
            second = data[x][k][a]
            second.replace('\n\xa0\xa0\xa0\xa0\xa0', '').replace('\n', '')
            articles.append(second)
#             questions.append(a['вопрос'])
#             answers.append(a['ответ'])

articles
87/18:
articles = []
for x in data:    
    for k in data[x]:
        for a in data[x][k]:
#             print(a)
#             first = a
            second = data[x][k][a]
            second.replace('\n\xa0\xa0\xa0\xa0\xa0', '').replace('\n', '')
            print(second)
            articles.append(second)
#             questions.append(a['вопрос'])
#             answers.append(a['ответ'])

articles
87/19:
articles = []
for x in data:    
    for k in data[x]:
        for a in data[x][k]:
            print(a)
#             first = a
            second = data[x][k][a]
    
            second.replace('\n\xa0\xa0\xa0\xa0\xa0', '').replace('\n', '')
            articles.append(second)
#             questions.append(a['вопрос'])
#             answers.append(a['ответ'])

articles
87/20:
articles = []
for x in data:    
    for k in data[x]:
        for a in data[x][k]:
#             print(a)
#             first = a
            second = data[x][k][a]
    
            second.replace('\n\xa0\xa0\xa0\xa0\xa0', '').replace('\n', '')
            articles.append(second)
#             questions.append(a['вопрос'])
#             answers.append(a['ответ'])

articles
87/21:
articles = []
for x in data:    
    for k in data[x]:
        for a in data[x][k]:
#             print(a)
#             first = a
            second = data[x][k][a]
            print(second)
            second.replace('\n\xa0\xa0\xa0\xa0\xa0', '').replace('\n', '')
            articles.append(second)
#             questions.append(a['вопрос'])
#             answers.append(a['ответ'])

# articles
87/22:
articles = []
for x in data:    
    for k in data[x]:
        for a in data[x][k]:
            second = data[x][k][a]
            second.replace('\n\xa0\xa0\xa0\xa0\xa0', '').replace('\n', '')
            articles.append(second)

articles
87/23:
articles = []
for x in data:    
    for k in data[x]:
        for a in data[x][k]:
            second = data[x][k][a]
            second.replace('\n\xa0\xa0\xa0\xa0\xa0', '').replace('\n', '')
            articles.append(second)

articles
87/24:
articles = []
import unicodedata

for x in data:    
    for k in data[x]:
        for a in data[x][k]:
            second = data[x][k][a]
            new_str = unicodedata.normalize("NFKD", second)
            print(new_str)
#             second.replace('\n\xa0\xa0\xa0\xa0\xa0', '').replace('\n', '')
            articles.append(second)

articles
87/25:
articles = []
import unicodedata

for x in data:    
    for k in data[x]:
        for a in data[x][k]:
            second = data[x][k][a]
            new_str = unicodedata.normalize("NFKD", second)
#             print(new_str)
#             second.replace('\n\xa0\xa0\xa0\xa0\xa0', '').replace('\n', '')
            articles.append(second)

articles
87/26:
articles = []
import unicodedata

for x in data:    
    for k in data[x]:
        for a in data[x][k]:
            second = data[x][k][a]
            new_str = unicodedata.normalize("NFKD", second)
#             print(new_str)
#             second.replace('\n\xa0\xa0\xa0\xa0\xa0', '').replace('\n', '')
            articles.append(new_str)

articles
87/27:
articles = []
import unicodedata

for x in data:    
    for k in data[x]:
        for a in data[x][k]:
            second = data[x][k][a]
            new_str = unicodedata.normalize("NFKD", second).strip()
#             print(new_str)
#             second.replace('\n\xa0\xa0\xa0\xa0\xa0', '').replace('\n', '')
            articles.append(new_str)

articles
87/28:
articles = []
import unicodedata

for x in data:    
    for k in data[x]:
        for a in data[x][k]:
            second = data[x][k][a]
#             new_str = unicodedata.normalize("NFKD", second).strip()
#             new_str.repla
#             print(new_str)
            second = second.replace('\n\xa0\xa0\xa0\xa0\xa0', '').replace('\n', '')
            print(second)
#             articles.append(new_str)

articles
87/29:
articles = []
import unicodedata

for x in data:    
    for k in data[x]:
        for a in data[x][k]:
            second = data[x][k][a]
#             new_str = unicodedata.normalize("NFKD", second).strip()
#             new_str.repla
#             print(new_str)
            second = second.replace('\n\xa0\xa0\xa0\xa0\xa0', '').replace('\n', '').replace('\n\n', '')
            print(second)
#             articles.append(new_str)

articles
87/30:
articles = []
import unicodedata

for x in data:    
    for k in data[x]:
        for a in data[x][k]:
            second = data[x][k][a]
#             new_str = unicodedata.normalize("NFKD", second).strip()
#             new_str.repla
#             print(new_str)
            second = second.replace('\n\xa0\xa0\xa0\xa0\xa0', '').replace('\n', '').replace('\n\n', '')
#             print(second)
#             articles.append(new_str)

articles
87/31:
articles = []
import unicodedata

for x in data:    
    for k in data[x]:
        for a in data[x][k]:
            second = data[x][k][a]
#             new_str = unicodedata.normalize("NFKD", second).strip()
#             new_str.repla
#             print(new_str)
            second = second.replace('\n\xa0\xa0\xa0\xa0\xa0', '').replace('\n', '').replace('\n\n', '')
#             print(second)
#             articles.append(new_str)

articles
87/32:
articles = []
import unicodedata

for x in data:    
    for k in data[x]:
        for a in data[x][k]:
            second = data[x][k][a]
#             new_str = unicodedata.normalize("NFKD", second).strip()
#             new_str.repla
#             print(new_str)
            second = second.replace('\n\xa0\xa0\xa0\xa0\xa0', '').replace('\n', '').replace('\n\n', '')
#             print(second)
            articles.append(second)

articles
87/33:
articles = []
import unicodedata

for x in data:    
    for k in data[x]:
        for a in data[x][k]:
            second = data[x][k][a]
            new_str = unicodedata.normalize("NFKD", second).strip()
#             new_str.repla
#             print(new_str)
#             second = second.replace('\n\xa0\xa0\xa0\xa0\xa0', '').replace('\n', '').replace('\n\n', '')
#             print(second)
            articles.append(new_str)

articles
87/34:
articles = []
import unicodedata

for x in data:    
    for k in data[x]:
        for a in data[x][k]:
            second = data[x][k][a]
            new_str = unicodedata.normalize("NFKD", second).strip()
#             new_str.repla
#             print(new_str)
#             new_str = new_str.replace('\n', '').replace('\n\n', '')
#             print(second)
            articles.append(new_str)

articles
87/35:
articles = []
import unicodedata

for x in data:    
    for k in data[x]:
        for a in data[x][k]:
            second = data[x][k][a]
            new_str = unicodedata.normalize("NFKD", second).strip()
#             new_str.repla
#             print(new_str)
            new_str = new_str.replace('\n', '').replace('\n\n', '')
#             print(second)
            articles.append(new_str)

articles
87/36:
articles = []
import unicodedata

for x in data:    
    for k in data[x]:
        for a in data[x][k]:
            second = data[x][k][a]
            new_str = unicodedata.normalize("NFKD", second).strip()
#             new_str.repla
#             print(new_str)
            new_str = new_str.replace('\n', '').replace('\n\n', '').strip()
#             print(second)
            articles.append(new_str)

articles
90/1: import json
90/2:
articles = []
import unicodedata

for x in data:    
    for k in data[x]:
        for a in data[x][k]:
            second = data[x][k][a]
            new_str = unicodedata.normalize("NFKD", second).strip()
#             new_str.repla
#             print(new_str)
            new_str = new_str.replace('\n', '').replace('\n\n', '').strip()
#             print(second)
            articles.append(new_str)

articles
90/3: import json
90/4:
with open('test.json', encoding='utf-8') as f:
    data = json.load(f)
90/5: data
90/6:
articles = []
import unicodedata

for x in data:    
    for k in data[x]:
        for a in data[x][k]:
            second = data[x][k][a]
            new_str = unicodedata.normalize("NFKD", second).strip()
#             new_str.repla
#             print(new_str)
            new_str = new_str.replace('\n', '').replace('\n\n', '').strip()
#             print(second)
            articles.append(new_str)

articles
90/7:
with open('test.json', encoding='utf-8') as f:
    data = json.load(f)
90/8:
with open('test.json', encoding='utf-8') as f:
    data = json.load(f)
90/9: data
90/10:
articles = []
import unicodedata

for x in data:    
    for k in data[x]:
        for a in data[x][k]:
            second = data[x][k][a]
            new_str = unicodedata.normalize("NFKD", second).strip()
#             new_str.repla
#             print(new_str)
            new_str = new_str.replace('\n', '').replace('\n\n', '').strip()
#             print(second)
            articles.append(new_str)

articles
90/11: art_tok = [list(tokenize(x.lower())) for x in articles].copy()
90/12: from razdel import tokenize
90/13: art_tok = [list(tokenize(x.lower())) for x in articles].copy()
90/14: pip install natasha
90/15: from razdel import tokenize
90/16: art_tok = [list(tokenize(x.lower())) for x in articles].copy()
90/17:
from natasha import (
    Segmenter,
    MorphVocab,
    
    Doc
)
import gensim
90/18: pip install gensim
90/19:
from natasha import (
    Segmenter,
    MorphVocab,
    
    Doc
)
import gensim
90/20:
import pymorphy2
morph = pymorphy2.MorphAnalyzer()
90/21: morph.parse('буквы')
90/22: sw = "c а алло без белый близко более больше большой будем будет будете будешь будто буду будут будь бы бывает бывь был была были было быть в важная важное важные важный вам вами вас ваш ваша ваше ваши вверх вдали вдруг ведь везде вернуться весь вечер взгляд взять вид видел видеть вместе вне вниз внизу во вода война вокруг вон вообще вопрос восемнадцатый восемнадцать восемь восьмой вот впрочем времени время все все еще всегда всего всем всеми всему всех всею всю всюду вся всё второй вы выйти г где главный глаз говорил говорит говорить год года году голова голос город да давать давно даже далекий далеко дальше даром дать два двадцатый двадцать две двенадцатый двенадцать дверь двух девятнадцатый девятнадцать девятый девять действительно дел делал делать делаю дело день деньги десятый десять для до довольно долго должен должно должный дом дорога друг другая другие других друго другое другой думать душа е его ее ей ему если есть еще ещё ею её ж ждать же жена женщина жизнь жить за занят занята занято заняты затем зато зачем здесь земля знать значит значить и иди идти из или им имеет имел именно иметь ими имя иногда их к каждая каждое каждые каждый кажется казаться как какая какой кем книга когда кого ком комната кому конец конечно которая которого которой которые который которых кроме кругом кто куда лежать лет ли лицо лишь лучше любить люди м маленький мало мать машина между меля менее меньше меня место миллионов мимо минута мир мира мне много многочисленная многочисленное многочисленные многочисленный мной мною мог могу могут мож может может быть можно можхо мои мой мор москва мочь моя моё мы на наверху над надо назад наиболее найти наконец нам нами народ нас начала начать наш наша наше наши не него недавно недалеко нее ней некоторый нельзя нем немного нему непрерывно нередко несколько нет нею неё ни нибудь ниже низко никакой никогда никто никуда ним ними них ничего ничто но новый нога ночь ну нужно нужный нх о об оба обычно один одиннадцатый одиннадцать однажды однако одного одной оказаться окно около он она они оно опять особенно остаться от ответить отец откуда отовсюду отсюда очень первый перед писать плечо по под подойди подумать пожалуйста позже пойти пока пол получить помнить понимать понять пор пора после последний посмотреть посреди потом потому почему почти правда прекрасно при про просто против процентов путь пятнадцатый пятнадцать пятый пять работа работать раз разве рано раньше ребенок решить россия рука русский ряд рядом с с кем сам сама сами самим самими самих само самого самой самом самому саму самый свет свое своего своей свои своих свой свою сделать сеаой себе себя сегодня седьмой сейчас семнадцатый семнадцать семь сидеть сила сих сказал сказала сказать сколько слишком слово случай смотреть сначала снова со собой собою советский совсем спасибо спросить сразу стал старый стать стол сторона стоять страна суть считать т та так такая также таки такие такое такой там твои твой твоя твоё те тебе тебя тем теми теперь тех то тобой тобою товарищ тогда того тоже только том тому тот тою третий три тринадцатый тринадцать ту туда тут ты тысяч у увидеть уж уже улица уметь утро хороший хорошо хотел бы хотеть хоть хотя хочешь час часто часть чаще чего человек чем чему через четвертый четыре четырнадцатый четырнадцать что чтоб чтобы чуть шестнадцатый шестнадцать шестой шесть эта эти этим этими этих это этого этой этом этому этот эту я являюсь".split()
90/23:
import nltk
from nltk.corpus import stopwords
sw_nltk = stopwords.words('russian')
# ans = []
# que = []
# [[w for w in ans[x] if w.lower() not in ] for x in range(len(ans))]
print(sw_nltk)
90/24:
model_ans = gensim.models.Word2Vec (ans, vector_size=100, window=10, min_count=0, workers=10)
model_ans.train(ans,total_examples=len(ans),epochs=10)
90/25: art_tok = [list(tokenize(x.lower())) for x in articles].copy()
90/26: art_tok
90/27:
from natasha import (
    Segmenter,
    MorphVocab,
    
    Doc
)
import gensim
90/28:
import pymorphy2
morph = pymorphy2.MorphAnalyzer()
90/29:
art = []
for sentence in ans_tok:
    temp = []
    for word in sentence:
        m = morph.parse(word.text)[0]
        temp.append(m.normal_form)
    sss = ' '.join(temp)
    tt = gensim.utils.simple_preprocess(sss)
    art.append(tt)

print(art)
90/30:
art = []
for sentence in art_tok:
    temp = []
    for word in sentence:
        m = morph.parse(word.text)[0]
        temp.append(m.normal_form)
    sss = ' '.join(temp)
    tt = gensim.utils.simple_preprocess(sss)
    art.append(tt)

print(art)
90/31:
model_art = gensim.models.Word2Vec (art, vector_size=100, window=10, min_count=0, workers=10)
model_art.train(art,total_examples=len(art),epochs=10)
90/32: model_art.wv.most_similar (positive='закон')
90/33:
art = []
cnt = 0
for sentence in art_tok:
    temp = []
    for word in sentence:
        m = morph.parse(word.text)[0]
        print(m)
        temp.append(m.normal_form)
    if cnt == 10:
        break
    cnt += 1
    sss = ' '.join(temp)
    tt = gensim.utils.simple_preprocess(sss)
    art.append(tt)

print(art)
90/34:
art = []
cnt = 0
for sentence in art_tok:
    temp = []
    for word in sentence:
        print(word)
        m = morph.parse(word.text)[0]
#         print(m)
        temp.append(m.normal_form)
    if cnt == 10:
        break
    cnt += 1
    sss = ' '.join(temp)
    tt = gensim.utils.simple_preprocess(sss)
    art.append(tt)

print(art)
90/35:
art = []
cnt = 0
for sentence in art_tok:
    temp = []
    for word in sentence:
        print(word.text)
        m = morph.parse(word.text)[0]
#         print(m)
        temp.append(m.normal_form)
    if cnt == 10:
        break
    cnt += 1
    sss = ' '.join(temp)
    tt = gensim.utils.simple_preprocess(sss)
    art.append(tt)

print(art)
90/36:
art = []
cnt = 0
for sentence in art_tok:
    temp = []
    for word in sentence:
#         print(word.text)
        m = morph.parse(word.text)[0]
        print(m)
        temp.append(m.normal_form)
    if cnt == 10:
        break
    cnt += 1
    sss = ' '.join(temp)
    tt = gensim.utils.simple_preprocess(sss)
    art.append(tt)

print(art)
90/37:
art = []
cnt = 0
for sentence in art_tok:
    temp = []
    for word in sentence:
#         print(word.text)
        m = morph.parse(word.text)[0]
#         print(m)
        temp.append(m.normal_form)
    print(temp)
    if cnt == 10:
        break
    cnt += 1
    sss = ' '.join(temp)
    tt = gensim.utils.simple_preprocess(sss)
    art.append(tt)

print(art)
90/38:
from sentence_transformers import SentenceTransformer, util

model = SentenceTransformer("all-MiniLM-L6-v2")

# Sentences are encoded by calling model.encode()
emb1 = model.encode("This is a red cat with a hat.")
emb2 = model.encode("Have you seen my red cat?")

cos_sim = util.cos_sim(emb1, emb2)
print("Cosine-Similarity:", cos_sim)
90/39:
from sentence_transformers import SentenceTransformer, util

model = SentenceTransformer("all-MiniLM-L6-v2")

sentences = [
    "A man is eating food.",
    "A man is eating a piece of bread.",
    "The girl is carrying a baby.",
    "A man is riding a horse.",
    "A woman is playing violin.",
    "Two men pushed carts through the woods.",
    "A man is riding a white horse on an enclosed ground.",
    "A monkey is playing drums.",
    "Someone in a gorilla costume is playing a set of drums.",
]

# Encode all sentences
embeddings = model.encode(sentences)

# Compute cosine similarity between all pairs
cos_sim = util.cos_sim(embeddings, embeddings)

# Add all pairs to a list with their cosine similarity score
all_sentence_combinations = []
for i in range(len(cos_sim) - 1):
    for j in range(i + 1, len(cos_sim)):
        all_sentence_combinations.append([cos_sim[i][j], i, j])

# Sort list by the highest cosine similarity score
all_sentence_combinations = sorted(all_sentence_combinations, key=lambda x: x[0], reverse=True)

print("Top-5 most similar pairs:")
for score, i, j in all_sentence_combinations[0:5]:
    print("{} \t {} \t {:.4f}".format(sentences[i], sentences[j], cos_sim[i][j]))
90/40:
from sentence_transformers import SentenceTransformer, util

model = SentenceTransformer("msmarco-MiniLM-L6-cos-v5")

sentences = [
    "A man is eating food.",
    "A man is eating a piece of bread.",
    "The girl is carrying a baby.",
    "A man is riding a horse.",
    "A woman is playing violin.",
    "Two men pushed carts through the woods.",
    "A man is riding a white horse on an enclosed ground.",
    "A monkey is playing drums.",
    "Someone in a gorilla costume is playing a set of drums.",
]

# Encode all sentences
embeddings = model.encode(sentences)

# Compute cosine similarity between all pairs
cos_sim = util.cos_sim(embeddings, embeddings)

# Add all pairs to a list with their cosine similarity score
all_sentence_combinations = []
for i in range(len(cos_sim) - 1):
    for j in range(i + 1, len(cos_sim)):
        all_sentence_combinations.append([cos_sim[i][j], i, j])

# Sort list by the highest cosine similarity score
all_sentence_combinations = sorted(all_sentence_combinations, key=lambda x: x[0], reverse=True)

print("Top-5 most similar pairs:")
for score, i, j in all_sentence_combinations[0:5]:
    print("{} \t {} \t {:.4f}".format(sentences[i], sentences[j], cos_sim[i][j]))
90/41:
from sentence_transformers import SentenceTransformer, util

model = SentenceTransformer("all-MiniLM-L6-v2")

sentences = [
    "A man is eating food.",
    "A man is eating a piece of bread.",
    "The girl is carrying a baby.",
    "A man is riding a horse.",
    "A woman is playing violin.",
    "Two men pushed carts through the woods.",
    "A man is riding a white horse on an enclosed ground.",
    "A monkey is playing drums.",
    "Someone in a gorilla costume is playing a set of drums.",
]

# Encode all sentences
embeddings = model.encode(sentences)

# Compute cosine similarity between all pairs
cos_sim = util.cos_sim(embeddings, embeddings)

# Add all pairs to a list with their cosine similarity score
all_sentence_combinations = []
for i in range(len(cos_sim) - 1):
    for j in range(i + 1, len(cos_sim)):
        all_sentence_combinations.append([cos_sim[i][j], i, j])

# Sort list by the highest cosine similarity score
all_sentence_combinations = sorted(all_sentence_combinations, key=lambda x: x[0], reverse=True)

print("Top-5 most similar pairs:")
for score, i, j in all_sentence_combinations[0:5]:
    print("{} \t {} \t {:.4f}".format(sentences[i], sentences[j], cos_sim[i][j]))
90/42:
from sentence_transformers import SentenceTransformer, util

model = SentenceTransformer("msmarco-MiniLM-L12-cos-v5")

sentences = [
    "A man is eating food.",
    "A man is eating a piece of bread.",
    "The girl is carrying a baby.",
    "A man is riding a horse.",
    "A woman is playing violin.",
    "Two men pushed carts through the woods.",
    "A man is riding a white horse on an enclosed ground.",
    "A monkey is playing drums.",
    "Someone in a gorilla costume is playing a set of drums.",
]

# Encode all sentences
embeddings = model.encode(sentences)

# Compute cosine similarity between all pairs
cos_sim = util.cos_sim(embeddings, embeddings)

# Add all pairs to a list with their cosine similarity score
all_sentence_combinations = []
for i in range(len(cos_sim) - 1):
    for j in range(i + 1, len(cos_sim)):
        all_sentence_combinations.append([cos_sim[i][j], i, j])

# Sort list by the highest cosine similarity score
all_sentence_combinations = sorted(all_sentence_combinations, key=lambda x: x[0], reverse=True)

print("Top-5 most similar pairs:")
for score, i, j in all_sentence_combinations[0:5]:
    print("{} \t {} \t {:.4f}".format(sentences[i], sentences[j], cos_sim[i][j]))
91/1:
from sentence_transformers import SentenceTransformer, util

model = SentenceTransformer("multi-qa-MiniLM-L6-cos-v1")

query_embedding = model.encode("How big is London")
print(query_embedding)
passage_embedding = model.encode([
    "London has 9,787,426 inhabitants at the 2011 census",
    "London is known for its finacial district",
])

print("Similarity:", util.dot_score(query_embedding, passage_embedding))
91/2:
import chromadb
client = chromadb.HttpClient()
collection = client.create_collection("sample_collection")

# Add docs to the collection. Can also update and delete. Row-based API coming soon!
collection.add(
    documents=["This is document1", "This is document2"], # we embed for you, or bring your own
    metadatas=[{"source": "notion"}, {"source": "google-docs"}], # filter on arbitrary metadata!
    ids=["doc1", "doc2"], # must be unique for each doc 
)

results = collection.query(
    query_texts=["This is a query document"],
    n_results=2,
    # where={"metadata_field": "is_equal_to_this"}, # optional filter
    # where_document={"$contains":"search_string"}  # optional filter
)
91/3: chroma run # run the server
91/4:
import chromadb
client = chromadb.HttpClient()
collection = client.create_collection("sample_collection")

# Add docs to the collection. Can also update and delete. Row-based API coming soon!
collection.add(
    documents=["This is document1", "This is document2"], # we embed for you, or bring your own
    metadatas=[{"source": "notion"}, {"source": "google-docs"}], # filter on arbitrary metadata!
    ids=["doc1", "doc2"], # must be unique for each doc 
)

results = collection.query(
    query_texts=["This is a query document"],
    n_results=2,
    # where={"metadata_field": "is_equal_to_this"}, # optional filter
    # where_document={"$contains":"search_string"}  # optional filter
)
91/5: print(results)
91/6:
from sentence_transformers import SentenceTransformer, util

model = SentenceTransformer("msmarco-MiniLM-L6-cos-v5")

sentences = [
    "A man is eating food.",
    "A man is eating a piece of bread.",
    "The girl is carrying a baby.",
    "A man is riding a horse.",
    "A woman is playing violin.",
    "Two men pushed carts through the woods.",
    "A man is riding a white horse on an enclosed ground.",
    "A monkey is playing drums.",
    "Someone in a gorilla costume is playing a set of drums.",
]

# Encode all sentences
embeddings = model.encode(sentences)

# Compute cosine similarity between all pairs
cos_sim = util.cos_sim(embeddings, embeddings)

# Add all pairs to a list with their cosine similarity score
all_sentence_combinations = []
for i in range(len(cos_sim) - 1):
    for j in range(i + 1, len(cos_sim)):
        all_sentence_combinations.append([cos_sim[i][j], i, j])

# Sort list by the highest cosine similarity score
all_sentence_combinations = sorted(all_sentence_combinations, key=lambda x: x[0], reverse=True)

print("Top-5 most similar pairs:")
for score, i, j in all_sentence_combinations[0:5]:
    print("{} \t {} \t {:.4f}".format(sentences[i], sentences[j], cos_sim[i][j]))
91/7: print(embeddings)
91/8: print(len(embeddings))
91/9: len(embeddings)
91/10: embeddings
91/11:
import chromadb
chroma_client = chromadb.Client()
91/12: collection = chroma_client.create_collection(name="my_collection")
91/13:
from sentence_transformers import SentenceTransformer, util

model = SentenceTransformer("msmarco-MiniLM-L6-cos-v5")

# Encode all sentences
embeddings = model.encode(articles)


# similarity = util.cos_sim(query_embedding, passage_embedding))
91/14: import json
91/15:
with open('test.json', encoding='utf-8') as f:
    data = json.load(f)
91/16: data
91/17:
articles = []
import unicodedata

for x in data:    
    for k in data[x]:
        for a in data[x][k]:
            second = data[x][k][a]
            new_str = unicodedata.normalize("NFKD", second).strip()
#             new_str.repla
#             print(new_str)
            new_str = new_str.replace('\n', '').replace('\n\n', '').strip()
#             print(second)
            articles.append(new_str)

articles
91/18:
from sentence_transformers import SentenceTransformer, util

model = SentenceTransformer("msmarco-MiniLM-L6-cos-v5")

# Encode all sentences
embeddings = model.encode(articles)


# similarity = util.cos_sim(query_embedding, passage_embedding))
91/19: embeddings
91/20: articles
91/21: pint(len(articles))
91/22: print(len(articles))
91/23:
ids = list(range(len(articles)))
ids
91/24:
for x in data:    
    for k in data[x]:
        print(k)
91/25:
for x in data:    
    for k in data[x]:
        print(len(data[x][k]))
91/26:
ids = list(range(len(articles)))
metadatas = []
for x in data:    
    for k in data[x]:
        for cnt in range(len(data[x][k])):
            metadatas.append({'Part':k})
metadatas
91/27:
collection.add(
    embeddings=embeddings,
    documents=articles,
    metadatas=metadatas,
    ids=ids
)
91/28:
ids = [str(x) for x in range(len(articles))]
metadatas = []
for x in data:    
    for k in data[x]:
        for cnt in range(len(data[x][k])):
            metadatas.append({'Part':k})
ids
91/29:
ids = [str(x) for x in range(len(articles))]
metadatas = []
for x in data:    
    for k in data[x]:
        for cnt in range(len(data[x][k])):
            metadatas.append({'Part':k})
metadatas
91/30:
collection.add(
    embeddings=embeddings,
    documents=articles,
    metadatas=metadatas,
    ids=ids
)
91/31:
results = collection.query(
    query_texts=["Что будет за убийство?"],
    n_results=2
)
91/32: results
91/33:
results = collection.query(
    query_texts=["Сколько дают за проституцию?"],
    n_results=2
)
91/34: results
91/35:
results = collection.query(
    query_texts=["Убийство?"],
    n_results=2
)
91/36:
results = collection.query(
    query_texts=["За убийство дают сколько лет"],
    n_results=2
)
91/37: results
91/38:
results = collection.query(
    query_texts=["За убийство дают сколько лет"],
    n_results=10
)
91/39: results
91/40:
results = collection.query(
    query_texts=["Штраф за вандализм"],
    n_results=10
)
91/41: results
91/42:
query_embedding = model.encode("Штраф за вандализм")
results = collection.query(
    query_embeddings=query_embedding,
    n_results=10
)
91/43:
query_embedding = model.encode("Штраф за вандализм")
# results = collection.query(
#     query_embeddings=query_embedding,
#     n_results=10
# )
query_embedding
91/44:
collection.add(
    embeddings=embeddings,
#     documents=articles,
    metadatas=metadatas,
    ids=ids
)
91/45:
query_embedding = model.encode("Штраф за вандализм")
# results = collection.query(
#     query_embeddings=query_embedding,
#     n_results=10
# )
query_embedding
91/46:
query_embedding = model.encode("Штраф за вандализм")
results = collection.query(
    query_embeddings=query_embedding,
    n_results=10
)
query_embedding
91/47: embeddings
91/48:
query_embedding = model.encode("Штраф за вандализм")
results = collection.query(
    query_embeddings=[query_embedding],
    n_results=10
)
query_embedding
91/49:
query_embedding = model.encode("Штраф за вандализм")
results = collection.query(
    query_embeddings=[query_embedding],
    n_results=10
)
# query_embedding
91/50:
query_embedding = list[model.encode("Штраф за вандализм")]
results = collection.query(
    query_embeddings=query_embedding,
    n_results=10
)
# query_embedding
91/51: embeddings
91/52:
query_embedding = model.encode(["Штраф за вандализм"])
results = collection.query(
    query_embeddings=query_embedding,
    n_results=10
)
# query_embedding
91/53: results
91/54: similarity = util.cos_sim(query_embedding, embeddings))
91/55: similarity = util.cos_sim(query_embedding, embeddings)
91/56: similarity
91/57:
from langchain.embeddings.openai import OpenAIEmbeddings
embedding = OpenAIEmbeddings()
91/58:
import os
import openai
import sys

openai.api_key  = os.environ['E62X24FIJA23MI5C7LMWKKJ']
91/59:
import os
import openai
import sys

openai.api_key  = os.environ["E62X24FIJA23MI5C7LMWKKJ"]
91/60:
import os
import openai
import sys
sys.path.append('../..')

openai.api_key  = os.environ["E62X24FIJA23MI5C7LMWKKJ"]
91/61:
import os
import openai
import sys
sys.path.append('../..')

openai.api_key  = os.environ["E62X24FIJA23MI5C7LMWKKJ"]
91/62:
import os
import openai
import sys

openai.api_key  = os.environ["E62X24FIJA23MI5C7LMWKKJ"]
91/63:
import os
import openai
import sys

openai.api_key  = os.environ["E62X24FIJA23MI5C7LMWKKJ"]
91/64:
import os
import openai
import sys

openai.api_key  = "E62X24FIJA23MI5C7LMWKKJ"
91/65:
from langchain.embeddings.openai import OpenAIEmbeddings
embedding = OpenAIEmbeddings()
91/66:
from langchain.embeddings.openai import OpenAIEmbeddings
embedding = OpenAIEmbeddings(openai_api_key='E62X24FIJA23MI5C7LMWKKJ')
91/67: query = 'Сколько дают за убийство человека'
91/68: embedding1 = embedding.embed_query(query)
91/69: pip install tiktoken
91/70: embedding1 = embedding.embed_query(query)
91/71:
from langchain.embeddings.openai import OpenAIEmbeddings
embedding = OpenAIEmbeddings(openai_api_key='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')
91/72: query = 'Сколько дают за убийство человека'
91/73: embedding1 = embedding.embed_query(query)
91/74:
sentence1 = "i like dogs"
sentence2 = "i like canines"
sentence3 = "the weather is ugly outside"
91/75:
embedding1 = embedding.embed_query(sentence1)
embedding2 = embedding.embed_query(sentence2)
embedding3 = embedding.embed_query(sentence3)
91/76: import numpy as np
91/77: np.dot(embedding1, embedding2)
91/78: from langchain.vectorstores import Chroma
91/79: persist_directory = './docs/chroma/'
91/80: !rm -rf ./docs/chroma  # remove old database files if any
91/81:
vectordb = Chroma.from_documents(
    documents=splits,
    embedding=embedding,
    persist_directory=persist_directory
)
91/82:
vectordb = Chroma.from_documents(
    documents=articles,
    embedding=embedding,
    persist_directory=persist_directory
)
91/83:
from langchain.text_splitter import RecursiveCharacterTextSplitter

text_splitter = RecursiveCharacterTextSplitter(
    chunk_size = 1500,
    chunk_overlap = 150
)
91/84: splits = RecursiveCharacterTextSplitter(articles)
91/85: splits
91/86: len(splits)
91/87: splits = text_splitter(articles)
91/88:
def generate_tokens(s):
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
    splits = text_splitter.split_text(s)

    return text_splitter.create_documents(splits)
91/89: splits = generate_tokens(articles)
91/90: texts = text_splitter.create_documents([articles])
91/91: texts = text_splitter.create_documents(articles)
91/92: texts
91/93: splits = generate_tokens(texts)
91/94: len(texts)
91/95: splits = text_splitter(texts)
91/96: texts
91/97: splits = text_splitter.split_documents(texts)
91/98: splits
91/99:
from langchain.text_splitter import RecursiveCharacterTextSplitter

text_splitter = RecursiveCharacterTextSplitter(
    chunk_size = 1500,
    chunk_overlap = 150
)
91/100: splits = text_splitter.split_documents(texts)
91/101: splits
91/102: len(splits)
91/103:
vectordb = Chroma.from_documents(
    documents=splits,
    embedding=embedding,
    persist_directory=persist_directory
)
91/104:
from langchain.text_splitter import RecursiveCharacterTextSplitter

text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=100,
    chunk_overlap=20,
)
91/105: splits = text_splitter.split_documents(texts)
91/106: len(splits)
91/107:
vectordb = Chroma.from_documents(
    documents=splits,
    embedding=embedding,
    persist_directory=persist_directory
)
91/108: vectordb.persist()
91/109: question = "what did they say about matlab?"
91/110: docs = vectordb.similarity_search(question,k=5)
91/111: docs[0]
91/112: question = "Что дают за убийство"
91/113: docs = vectordb.similarity_search(question,k=5)
91/114: docs[0]
91/115:
from langchain.text_splitter import RecursiveCharacterTextSplitter

text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=500,
    chunk_overlap=50,
)
91/116: splits = text_splitter.split_documents(texts)
91/117: len(splits)
91/118: texts
91/119:
vectordb = Chroma.from_documents(
    documents=splits,
    embedding=embedding,
    persist_directory=persist_directory
)
91/120:
from langchain.text_splitter import RecursiveCharacterTextSplitter

text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=400,
    chunk_overlap=50,
)
91/121: splits = text_splitter.split_documents(texts)
91/122: len(splits)
91/123:
vectordb = Chroma.from_documents(
    documents=splits,
    embedding=embedding,
    persist_directory=persist_directory
)
91/124:
from langchain.text_splitter import RecursiveCharacterTextSplitter

text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=400,
    chunk_overlap=30,
)
91/125: splits = text_splitter.split_documents(texts)
91/126:
vectordb = Chroma.from_documents(
    documents=splits,
    embedding=embedding,
    persist_directory=persist_directory
)
91/127:
from langchain.text_splitter import RecursiveCharacterTextSplitter

text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=300,
    chunk_overlap=30,
)
91/128: splits = text_splitter.split_documents(texts)
91/129:
vectordb = Chroma.from_documents(
    documents=splits,
    embedding=embedding,
    persist_directory=persist_directory
)
91/130: vectordb.persist()
91/131: question = "Что дают за убийство"
91/132: docs = vectordb.similarity_search(question,k=5)
91/133: docs[0]
91/134:
for doc in docs:
    print(doc.metadata)
91/135:
for doc in docs:
    print(doc)
92/1: from catboost import CatBoostClassifier
92/2: pip install catboost
92/3:
from catboost import CatBoostClassifier
import numpy as np
import pandas as pd
92/4:
from catboost import CatBoostClassifier
import numpy as np
import pandas as pd
92/5: pip install -U setuptools
92/6: pip install catboost
93/1:
from catboost import CatBoostClassifier
import numpy as np
import pandas as pd
93/2: pip install catboost
94/1:
from catboost import CatBoostClassifier
import numpy as np
import pandas as pd
94/2: pip install catboost
93/3: pip install wheel setuptools pip --upgrade
93/4: pip install -U setuptools
94/3: pip install wheel setuptools pip --upgrade
95/1: pip install catboost
95/2: pip install "cython<3.0.0" wheel
95/3: pip install "pyyaml==5.4.1" --no-build-isolation
95/4: pip install catboost
96/1: pip install catboost
96/2: pip install PyYAML==6.0
96/3: pip cache purge
96/4: pip install --upgrade setuptools
96/5:
pip uninstall Cython
pip install Cython
96/6: pip uninstall Cython
96/7: pip install Cython
96/8: pip install PyYAML==6.0
97/1: import numpy as np
97/2: pip install catboost
98/1: import numpy as np
99/1: pip install catboost
100/1: pip install catboost
100/2: pip install --upgrade pip setuptools
100/3: pip install PyYAML==5.4.1
100/4: pip install catboost
101/1: import catboost
101/2: from catboost import CatBoostRegressor
101/3: from catboost import CatBoostRegressor
103/1: import catboost
104/1: import numpy as np
104/2: import catboost
104/3: pip3 install catboost
104/4: pip3 install catboost
104/5: !pip3 install catboost
104/6: import catboost
104/7: !pip3 install catboost
105/1: import catboost
105/2: !pip3 install catboost
105/3: import catboost
105/4: from catboost import CatBoostClassifier
105/5:
# !pip3 install catboost
pip uninstall catboost
105/6: pip uninstall catboost
106/1: !pip3 install catboost
106/2: from catboost import CatBoostClassifier
106/3: !pip install catboost
106/4: pip install catboost
106/5: conda install catboost
106/6: !conda install catboost
106/7: from catboost import CatBoostClassifier
106/8: pip install catboost
106/9: !conda install catboost
107/1: pip install catboost
108/1: from catboost import CatBoostRegressor
109/1: import catboost
110/1: import catboost
111/1: from catboost import CatBoostRegressor
111/2:
import numpy 
from catboost import CatBoostRegressor

dataset = numpy.array([[1,4,5,6],[4,5,6,7],[30,40,50,60],[20,15,85,60]])
train_labels = [1.2,3.4,9.5,24.5]
model = CatBoostRegressor(learning_rate=1, depth=6, loss_function='RMSE')
fit_model = model.fit(dataset, train_labels)

print(fit_model.get_params())
111/3:
import numpy as np
import pandas as pd
111/4: df = pd.read_csv('diabetes.csv')
111/5: df
111/6:
X = df.drop('Outcome', axis=1)
y = df['Outcome']
111/7: X
111/8:
from sklearn.model_selection import train_test_split

X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=0)
111/9: pip install sklearn
111/10: pip install scikit-learn
111/11:
from sklearn.model_selection import train_test_split

X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=0)
111/12:
from catboost import CatBoostClassifier

clf = CatBoostClassifier(
    iterations=5, 
    learning_rate=0.1, 
    #loss_function='CrossEntropy'
)


clf.fit(X_train, y_train, 
        cat_features=cat_features, 
        eval_set=(X_val, y_val), 
        verbose=False
)

print('CatBoost model is fitted: ' + str(clf.is_fitted()))
print('CatBoost model parameters:')
print(clf.get_params())
111/13:
cat_features = list(range(0, X.shape[1]))
print(cat_features)
111/14:
from catboost import CatBoostClassifier

clf = CatBoostClassifier(
    iterations=5, 
    learning_rate=0.1, 
    #loss_function='CrossEntropy'
)


clf.fit(X_train, y_train, 
        cat_features=cat_features, 
        eval_set=(X_val, y_val), 
        verbose=False
)

print('CatBoost model is fitted: ' + str(clf.is_fitted()))
print('CatBoost model parameters:')
print(clf.get_params())
111/15: df.head()
111/16: df.info()
111/17: df.describe()
111/18:
 from sklearn import datasets
    from sklearn import metrics
    from sklearn.model_selection import train_test_split
    import matplotlib.pyplot as plt
    import seaborn as sns

    plt.style.use("ggplot")

    import catboost as ctb
111/19:
from sklearn import datasets
from sklearn import metrics
from sklearn.model_selection import train_test_split
import catboost as ctb
111/20:
dataset = datasets.load_iris()
    X = dataset.data; y = dataset.target
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30)
111/21:
dataset = datasets.load_iris()
X = dataset.data; y = dataset.target
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30)
111/22:
model_CBC = ctb.CatBoostClassifier()
    model_CBC.fit(X_train, y_train)
    print(model_CBC)
111/23:
model_CBC = ctb.CatBoostClassifier()
model_CBC.fit(X_train, y_train)
print(model_CBC)
111/24:
expected_y  = y_test
predicted_y = model_CBC.predict(X_test)
111/25:
print(metrics.classification_report(expected_y, predicted_y))
print(metrics.confusion_matrix(expected_y, predicted_y))
111/26:
model_CBC.plot_tree(
    tree_idx=0,
    pool=pool # "pool" is required parameter for trees with one hot features
)
111/27:
from sklearn import datasets
from sklearn import metrics
from sklearn.model_selection import train_test_split
from catboost import CatBoostRegressor as ctb, CatBoostClassifier, Pool
111/28:
from sklearn import datasets
from sklearn import metrics
from sklearn.model_selection import train_test_split
from catboost import CatBoostClassifier as ctb, Pool
111/29:
dataset = datasets.load_iris()
X = dataset.data; y = dataset.target
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30)
111/30:
model_CBC = ctb.CatBoostClassifier()
model_CBC.fit(X_train, y_train)
print(model_CBC)
111/31:
expected_y  = y_test
predicted_y = model_CBC.predict(X_test)
111/32:
from sklearn import datasets
from sklearn import metrics
from sklearn.model_selection import train_test_split
from catboost import CatBoostClassifier as ctb, Pool
111/33:
from sklearn import datasets
from sklearn import metrics
from sklearn.model_selection import train_test_split
import catboost as ctb
111/34:
dataset = datasets.load_iris()
X = dataset.data; y = dataset.target
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30)
111/35:
model_CBC = ctb.CatBoostClassifier()
model_CBC.fit(X_train, y_train)
print(model_CBC)
111/36:
expected_y  = y_test
predicted_y = model_CBC.predict(X_test)
111/37:
print(metrics.classification_report(expected_y, predicted_y))
print(metrics.confusion_matrix(expected_y, predicted_y))
111/38:
model_CBC.plot_tree(
    tree_idx=0,
    # pool=pool, 
)
111/39:
model = ctb.CatBoostClassifier()
model.fit(X_train, y_train)
print(model)
111/40:
expected_y  = y_test
predicted_y = model.predict(X_test)
111/41:
print(metrics.classification_report(expected_y, predicted_y))
print(metrics.confusion_matrix(expected_y, predicted_y))
111/42:
graph = model.plot_tree(tree_idx=0)
displayHTML(graph.pipe(format="svg").decode("utf-8"))
111/43:
is_cat = (X.dtypes != float)

cat_features_index = np.where(is_cat)[0]
pool = Pool(X, y, cat_features=cat_features_index, feature_names=list(X.columns))
111/44: X
111/45:
import numpy as np
import catboost
from catboost import CatBoost, Pool

from catboost.datasets import titanic
titanic_df = titanic()

X = titanic_df[0].drop('Survived',axis=1)
y = titanic_df[0].Survived

is_cat = (X.dtypes != float)
for feature, feat_is_cat in is_cat.to_dict().items():
    if feat_is_cat:
        X[feature].fillna("NAN", inplace=True)

cat_features_index = np.where(is_cat)[0]
pool = Pool(X, y, cat_features=cat_features_index, feature_names=list(X.columns))

model = CatBoost(
    max_depth=2, verbose=False, max_ctr_complexity=1, iterations=2).fit(pool)

model.plot_tree(
    tree_idx=0,
    pool=pool
)
111/46:
import numpy as np
import catboost
from catboost import CatBoost, Pool

from catboost.datasets import titanic
titanic_df = titanic()

X = titanic_df[0].drop('Survived',axis=1)
y = titanic_df[0].Survived

is_cat = (X.dtypes != float)
for feature, feat_is_cat in is_cat.to_dict().items():
    if feat_is_cat:
        X[feature].fillna("NAN", inplace=True)

cat_features_index = np.where(is_cat)[0]
pool = Pool(X, y, cat_features=cat_features_index, feature_names=list(X.columns))

model = CatBoost(
     verbose=False, max_ctr_complexity=1, iterations=2).fit(pool)

model.plot_tree(
    tree_idx=0,
    pool=pool
)
111/47:
import numpy as np
import catboost
from catboost import CatBoost, Pool

from catboost.datasets import titanic
titanic_df = titanic()

X = titanic_df[0].drop('Survived',axis=1)
y = titanic_df[0].Survived

is_cat = (X.dtypes != float)
for feature, feat_is_cat in is_cat.to_dict().items():
    if feat_is_cat:
        X[feature].fillna("NAN", inplace=True)

cat_features_index = np.where(is_cat)[0]
pool = Pool(X, y, cat_features=cat_features_index, feature_names=list(X.columns))

model = CatBoost(
      max_ctr_complexity=1, iterations=2).fit(pool)

model.plot_tree(
    tree_idx=0,
    pool=pool
)
111/48:
pool = catboost.Pool(
    data=X, 
    label=y
)
111/49:
from sklearn import datasets
from sklearn import metrics
from sklearn.model_selection import train_test_split
import catboost as ctb
111/50:
dataset = datasets.load_iris()
X = dataset.data; y = dataset.target
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30)
111/51:
model = ctb.CatBoostClassifier()
model.fit(X_train, y_train)
print(model)
111/52:
expected_y  = y_test
predicted_y = model.predict(X_test)
111/53:
print(metrics.classification_report(expected_y, predicted_y))
print(metrics.confusion_matrix(expected_y, predicted_y))
111/54:
pool = catboost.Pool(
    data=X, 
    label=y
)
111/55:
model.plot_tree(
    tree_idx=0,
    # pool=pool, 
)
111/56: pip install graphviz
112/1:
from sklearn import datasets
from sklearn import metrics
from sklearn.model_selection import train_test_split
import catboost as ctb
112/2:
dataset = datasets.load_iris()
X = dataset.data; y = dataset.target
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30)
112/3:
model = ctb.CatBoostClassifier()
model.fit(X_train, y_train)
print(model)
112/4:
expected_y  = y_test
predicted_y = model.predict(X_test)
112/5:
print(metrics.classification_report(expected_y, predicted_y))
print(metrics.confusion_matrix(expected_y, predicted_y))
112/6:
pool = catboost.Pool(
    data=X, 
    label=y
)
112/7: pip install graphviz
112/8:
pool = ctb.Pool(
    data=X, 
    label=y
)
112/9: pip install graphviz
112/10:
model.plot_tree(
    tree_idx=0,
    # pool=pool, 
)
112/11:
import os
os.environ["PATH"] += os.pathsep + 'C:/Program Files/Graphviz-10.0.1-win64/in'
112/12:
model.plot_tree(
    tree_idx=0,
    # pool=pool, 
)
112/13:
import os
os.environ["PATH"] += os.pathsep + 'C:/Program Files/Graphviz-10.0.1-win64/bin'
112/14:
model.plot_tree(
    tree_idx=0,
    # pool=pool, 
)
112/15:
model.plot_tree(
    tree_idx=0,
    pool=pool, 
)
112/16:
model.plot_tree(
    tree_idx=0,
    # pool=pool, 
)
112/17:
graph = model.plot_tree(tree_idx=0)
# displayHTML(graph.pipe(format="svg").decode("utf-8"))
112/18:
graph = model.plot_tree(tree_idx=0)
displayHTML(graph.pipe(format="svg").decode("utf-8"))
112/19: from IPython.core.display import display, HTML
112/20:
graph = model.plot_tree(tree_idx=0)
displayHTML(graph.pipe(format="svg").decode("utf-8"))
112/21: from IPython.display import display, HTML
112/22:
graph = model.plot_tree(tree_idx=0)
displayHTML(graph.pipe(format="svg").decode("utf-8"))
112/23:
graph = model.plot_tree(tree_idx=0)
display(HTML(graph.pipe(format="svg").decode("utf-8")))
112/24:
import matplotlib.pyplot as plt

# Assuming 'model' is your DecisionTreeClassifier or DecisionTreeRegressor instance
graph = model.plot_tree(tree_idx=0)
plt.savefig('tree_plot.png')  # Save the plot as an image file
112/25:
output = "savefile.gv"
model.plot_tree(model.best_iteration_,x_train ).save(output)
112/26:
output = "savefile.gv"
model.plot_tree(model.best_iteration_,X_train ).save(output)
112/27:
output = "savefile.gv"
model.plot_tree(tree_idx=0 ).save(output)
112/28:
output = "savefile.png"
model.plot_tree(tree_idx=0 ).save(output)
112/29:
output = "savefile.jpg"
model.plot_tree(tree_idx=0 ).save(output)
112/30:
output = "savefile.gv"
model.plot_tree(tree_idx=0 ).save(output)
112/31:
from graphviz import Source

# Define the path to your .gv file
file_path = "C:/Users/aliha/Desktop/work/savefile.gv"

# Open and render the graph
with open(file_path, "r") as f:
    src = Source(f.read(), filename=file_path, format="png")
    src.render(view=True)
112/32:
from graphviz import Source

# Define the path to your .gv file
file_path = "C:/Users/aliha/Desktop/work/savefile"

# Open and render the graph
with open(file_path, "r") as f:
    src = Source(f.read(), filename=file_path, format="png")
    src.render(view=True)
112/33:
from graphviz import Source

# Define the path to your .gv file
file_path = "C:/Users/aliha/Desktop/work/savefile.gv"

# Open and render the graph
with open(file_path, "r") as f:
    src = Source(f.read(), filename=file_path, format="png")
    src.render(view=True)
112/34:
from graphviz import Source

# Define the path to your .gv file
file_path = "C:/Users/aliha/Desktop/work/savefile.gv"

# Open and render the graph
with open(file_path, "r") as f:
    src = Source(f.read(), filename=file_path, format="png")
    src.render(view=True)
112/35:
from sklearn import datasets
from sklearn import metrics
from sklearn.model_selection import train_test_split
import catboost as ctb
112/36:
import numpy as np
import pandas as pd
112/37: df = pd.read_csv('diabetes.csv')
112/38:
X = df.drop('Outcome', axis=1)
y = df['Outcome']
112/39: df.head()
112/40: df.info()
112/41: df.describe()
112/42:
dataset = datasets.load_iris()
X = dataset.data; y = dataset.target
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30)
112/43:
model = ctb.CatBoostClassifier()
model.fit(X_train, y_train)
print(model)
112/44:
expected_y  = y_test
predicted_y = model.predict(X_test)
112/45:
print(metrics.classification_report(expected_y, predicted_y))
print(metrics.confusion_matrix(expected_y, predicted_y))
112/46:
pool = ctb.Pool(
    data=X, 
    label=y
)
112/47:
import os
os.environ["PATH"] += os.pathsep + 'C:/Program Files/Graphviz-10.0.1-win64/bin'
112/48:
model.plot_tree(
    tree_idx=0,
    # pool=pool, 
)
112/49:
import matplotlib.pyplot as plt

# Assuming 'model' is your DecisionTreeClassifier or DecisionTreeRegressor instance
graph = model.plot_tree(tree_idx=0)
plt.savefig('tree_plot.png')  # Save the plot as an image file
112/50:
output = "savefile.gv"
model.plot_tree(tree_idx=0 ).save(output)
112/51:
from graphviz import Source

# Define the path to your .gv file
file_path = "C:/Users/aliha/Desktop/work/savefile.gv"

# Open and render the graph
with open(file_path, "r") as f:
    src = Source(f.read(), filename=file_path, format="png")
    src.render(view=True)
113/1:
Model = CatboostClassifier()
Model.load_model(catboost_final)
113/2:
Model = ctb.CatboostClassifier()
Model.load_model(catboost_final)
113/3:
from sklearn import datasets
from sklearn import metrics
from sklearn.model_selection import train_test_split
import catboost as ctb
113/4:
import numpy as np
import pandas as pd
113/5: df = pd.read_csv('diabetes.csv')
113/6:
X = df.drop('Outcome', axis=1)
y = df['Outcome']
113/7: df.head()
113/8: df.info()
113/9: df.describe()
113/10:
dataset = datasets.load_iris()
X = dataset.data; y = dataset.target
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30)
113/11:
model = ctb.CatBoostClassifier()
model.fit(X_train, y_train)
print(model)
113/12:
expected_y  = y_test
predicted_y = model.predict(X_test)
113/13:
print(metrics.classification_report(expected_y, predicted_y))
print(metrics.confusion_matrix(expected_y, predicted_y))
113/14:
pool = ctb.Pool(
    data=X, 
    label=y
)
113/15:
import os
os.environ["PATH"] += os.pathsep + 

'C:/Program Files/Graphviz-10.0.1-win64/bin'
113/16:
model.plot_tree(
    tree_idx=0,
    # pool=pool, 
)
113/17:
import os
os.environ["PATH"] += os.pathsep + 'C:/Program Files/Graphviz-10.0.1-win64/bin'
113/18:
model.plot_tree(
    tree_idx=0,
    # pool=pool, 
)
113/19:
output = "savefile.gv"
model.plot_tree(tree_idx=0 ).save(output)
113/20:
from graphviz import Source

# Define the path to your .gv file
file_path = "C:/Users/aliha/Desktop/work/savefile.gv"

# Open and render the graph
with open(file_path, "r") as f:
    src = Source(f.read(), filename=file_path, format="png")
    src.render(view=True)
113/21:
Model = ctb.CatboostClassifier()
Model.load_model(catboost_final)
113/22:
Model = ctb.CatBoostClassifier()
Model.load_model(catboost_final)
113/23:
Model = ctb.CatBoostClassifier()
Model.load_model(catboost_final)
113/24:
Model = ctb.CatBoostClassifier()
Model.load_model('catboost_final')
113/25:
Model.plot_tree(
    tree_idx=0,
    # pool=pool, 
)
113/26:
Model.plot_tree(
    tree_idx=0,
    # pool=pool, 
)
113/27:
output = "savefile.gv"
Model.plot_tree(tree_idx=0 ).save(output)
113/28:
from graphviz import Source

# Define the path to your .gv file
file_path = "C:/Users/aliha/Desktop/work/savefile.gv"

# Open and render the graph
with open(file_path, "r") as f:
    src = Source(f.read(), filename='res', format="png")
    src.render(view=True)
113/29:
from graphviz import Source

# Define the path to your .gv file
file_path = "C:/Users/aliha/Desktop/work/savefile.gv"

# Open and render the graph with increased height
with open(file_path, "r") as f:
    src = Source(f.read(), filename='res', format="png")
    # Set the height parameter (in inches)
    src.render(view=True, format='png', options=['-Gdpi=300'], height='10')
113/30:
from graphviz import Source

# Define the path to your .gv file
file_path = "C:/Users/aliha/Desktop/work/savefile.gv"

# Open and render the graph with increased height
with open(file_path, "r") as f:
    src = Source(f.read(), filename='res', format="png")
    # Set the height parameter (in inches)
    src.render(view=True, format='png', height='10')
116/1: pip install python-docx
116/2:
from docx import Document

# Open the .docx file
doc = Document('1.docx')

# Iterate through each paragraph in the document
for paragraph in doc.paragraphs:
    print(paragraph.text)
116/3:
from docx import Document

document = Document()
116/4: pip install docx
116/5:
from docx import Document

document = Document()
116/6:
from docx import Document

document = Document(1.docx)
116/7:
from docx import Document

document = Document('1.docx')
116/8: document
116/9:
from docx import Document

doc  = Document('1.docx')
116/10:
for paragraph in doc.paragraphs:
    print(paragraph.text)
116/11:
from docx import Document

doc  = Document('8.docx')
116/12:
for paragraph in doc.paragraphs:
    print(paragraph.text)
116/13:
for table in doc.tables:
    # Iterate through each row in the table
    for row in table.rows:
        # Create an empty list to store the data from each cell in the row
        row_data = []
        # Iterate through each cell in the row
        for cell in row.cells:
            # Append the text content of the cell to the row_data list
            row_data.append(cell.text)
        # Print the row data
        print(row_data)
116/14:
from docx import Document

# Create a new Document
doc = Document()

# Add a table with 3 rows and 3 columns
table = doc.add_table(rows=3, cols=3)

# Iterate through each cell in the table and set some sample data
for i in range(3):
    for j in range(3):
        # Access each cell using the row and column index
        cell = table.cell(i, j)
        # Add some sample text to the cell
        cell.text = f"Row {i+1}, Col {j+1}"

# Save the document
doc.save('example_table.docx')
116/15:
from docx import Document

# Create a new Document
doc = Document()

# Add a table with 3 rows and 3 columns
table = doc.add_table(rows=3, cols=3)

# Iterate through each cell in the table and set some sample data
for i in range(3):
    for j in range(3):
        # Access each cell using the row and column index
        cell = table.cell(i, j)
        # Add some sample text to the cell
        cell.text = f"Row {i+1}, Col {j+1}"
        # Add border to each cell
        cell.style.border_top.color = '000000'
        cell.style.border_left.color = '000000'
        cell.style.border_right.color = '000000'
        cell.style.border_bottom.color = '000000'

# Save the document
doc.save('example_table_with_border.docx')
116/16:
from docx import Document 


def create_table(document):
    #this code creates a table with 2 rows and 2 columens 
    table = document.add_table(rows = 2 , cols = 2)
    #adding headers rows 
    hdr_cells = table.rows[0].cells
    hdr_cells[0].text = 'Item'     
    hdr_cells[1].text = 'quantity'
    
    #second riw 
    row_cells = table.add_row().cells 
    row_cells[0].text = 'Apple'
    row_cells[1].text = '10'


document = Document()
create_table(document)
#save the file 
document.save('tableName.docx'
116/17:
from docx import Document 


def create_table(document):
    #this code creates a table with 2 rows and 2 columens 
    table = document.add_table(rows = 2 , cols = 2)
    #adding headers rows 
    hdr_cells = table.rows[0].cells
    hdr_cells[0].text = 'Item'     
    hdr_cells[1].text = 'quantity'
    
    #second riw 
    row_cells = table.add_row().cells 
    row_cells[0].text = 'Apple'
    row_cells[1].text = '10'


document = Document()
create_table(document)
#save the file 
document.save('tableName.docx')
116/18:
from docx import Document 


def create_table(document):
    #this code creates a table with 2 rows and 2 columens 
    table = document.add_table(rows = 2 , cols = 2)
    #adding headers rows 
    hdr_cells = table.rows[0].cells
    hdr_cells[0].text = 'Item'     
    hdr_cells[1].text = 'quantity'
    
    #second riw 
#     row_cells = table.add_row().cells 
    row_cells[0].text = 'Apple'
    row_cells[1].text = '10'


document = Document()
create_table(document)
#save the file 
document.save('tableName.docx')
116/19:
from docx import Document 


def create_table(document):
    #this code creates a table with 2 rows and 2 columens 
    table = document.add_table(rows = 2 , cols = 2)
    #adding headers rows 
    hdr_cells = table.rows[0].cells
    hdr_cells[0].text = 'Item'     
    hdr_cells[1].text = ''
    
    #second riw 
    row_cells = table.add_row().cells 
    row_cells[0].text = 'Apple'
    row_cells[1].text = ''


document = Document()
create_table(document)
#save the file 
document.save('tableName.docx')
116/20:
from docx import Document 


def create_table(document):
    #this code creates a table with 2 rows and 2 columens 
    table = document.add_table(rows = 2 , cols = 2)
    #adding headers rows 
    hdr_cells = table.rows[0].cells
    hdr_cells[0].text = 'Item'     
    hdr_cells[1].text = ''
    
    #second riw 
    row_cells = table.add_row().cells 
    row_cells[0].text = 'Apple'
    row_cells[1].text = ''


document = Document()
create_table(document)
#save the file 
document.save('tableName.docx')
116/21:
from docx import Document

# Create a new Document
doc = Document()

# Add a table with 3 rows and 3 columns
table = doc.add_table(rows=3, cols=3)

# Iterate through each cell in the table and set some sample data
for i in range(3):
    for j in range(3):
        # Access each cell using the row and column index
        cell = table.cell(i, j)
        # Add some sample text to the cell
        cell.text = f"Row {i+1}, Col {j+1}"

# Save the document
doc.save('example_table.docx')
116/22:
from docx import Document

# Create a new Document
doc = Document()

# Add a table with 3 rows and 3 columns
table = doc.add_table(rows=3, cols=3)

# Iterate through each cell in the table and set some sample data
for i in range(3):
    for j in range(2):
        # Access each cell using the row and column index
        cell = table.cell(i, j)
        # Add some sample text to the cell
        cell.text = f"Row {i+1}, Col {j+1}"

# Save the document
doc.save('example_table.docx')
116/23:
from docx import Document

# Create a new Document
doc = Document()

# Add a table with 3 rows and 3 columns
table = doc.add_table(rows=3, cols=3)

# Iterate through each cell in the table and set some sample data
for i in range(3):
        cell = table.cell(i, 0)
        # Add some sample text to the cell
        cell.text = f"Row {i+1}, Col 0"
        cell = table.cell(i, 1)
        cell.text = f"Row {i+1}, Col 1"


# Save the document
doc.save('example_table.docx')
116/24:
from docx import Document

# Create a new Document
doc = Document()

# Add a table with 3 rows and 3 columns
table = doc.add_table(rows=3, cols=3)

# Iterate through each cell in the table and set some sample data
for i in range(3):
        cell = table.cell(i, 0)
        # Add some sample text to the cell
        cell.text = f"Row {i+1}, Col 0"
        cell = table.cell(i, 1)
        cell.text = f"Row {i+1}, Col 1"


# Save the document
doc.save('example_table.docx')
116/25:
from docx import Document

# Create a new Document
doc = Document()

# Add a table with 3 rows and 3 columns
table = doc.add_table(rows=3, cols=3)

# Iterate through each cell in the table and set some sample data
for i in range(3):
        cell = table.cell(i, 0)
        # Add some sample text to the cell
        cell.text = f"Row {i+1}, Col 0"
        cell = table.cell(i, 1)
        cell.text = ""


# Save the document
doc.save('example_table.docx')
116/26:
from docx import Document

# Create a new Document
doc = Document()

# Add a table with 3 rows and 3 columns
table = doc.add_table(rows=3, cols=3)

# Iterate through each cell in the table and set some sample data
for i in range(3):
        cell = table.cell(i, 0)
        # Add some sample text to the cell
        cell.text = f"Row {i+1}, Col 0"
        cell = table.cell(i, 1)
        cell.text = ""


# Save the document
doc.save('example_table.docx')
116/27:
tables = []
for table in doc.tables:
    # Iterate through each row in the table
    for row in table.rows:
        # Create an empty list to store the data from each cell in the row
        row_data = []
        # Iterate through each cell in the row
        for cell in row.cells:
            # Append the text content of the cell to the row_data list
            row_data.append(cell.text)
        # Print the row data
        tables.append(row_data)
116/28:
tables = []
for table in doc.tables:
    # Iterate through each row in the table
    for row in table.rows:
        # Create an empty list to store the data from each cell in the row
        row_data = []
        # Iterate through each cell in the row
        for cell in row.cells:
            # Append the text content of the cell to the row_data list
            row_data.append(cell.text)
        # Print the row data
        tables.append(row_data)

tables
116/29:
tables = []
for table in doc.tables:
    # Iterate through each row in the table
    for row in table.rows:
        # Create an empty list to store the data from each cell in the row
        row_data = []
        # Iterate through each cell in the row
        for cell in row.cells:
            # Append the text content of the cell to the row_data list
            row_data.append(cell.text)
        # Print the row data
        tables.append(row_data)

tables
116/30:
from docx import Document

doc  = Document('8.docx')
116/31:
tables = []
for table in doc.tables:
    # Iterate through each row in the table
    for row in table.rows:
        # Create an empty list to store the data from each cell in the row
        row_data = []
        # Iterate through each cell in the row
        for cell in row.cells:
            # Append the text content of the cell to the row_data list
            row_data.append(cell.text)
        # Print the row data
        tables.append(row_data)

tables
116/32:
tables = []
for table in doc.tables:
    # Iterate through each row in the table
    for row in table.rows:
        # Create an empty list to store the data from each cell in the row
        row_data = []
        # Iterate through each cell in the row
        for cell in row.cells:
            # Append the text content of the cell to the row_data list
            row_data.append(cell.text)
        # Print the row data
        tables.append(row_data)

tables[2]
116/33:
tables = []
for table in doc.tables:
    # Iterate through each row in the table
    for row in table.rows:
        # Create an empty list to store the data from each cell in the row
        row_data = []
        # Iterate through each cell in the row
        for cell in row.cells:
            # Append the text content of the cell to the row_data list
            row_data.append(cell.text)
        # Print the row data
        tables.append(row_data)

tables[2:]
116/34:
tables = []
for table in doc.tables:
    # Iterate through each row in the table
    for row in table.rows:
        # Create an empty list to store the data from each cell in the row
        row_data = []
        # Iterate through each cell in the row
        for cell in row.cells:
            # Append the text content of the cell to the row_data list
            row_data.append(cell.text)
        # Print the row data
        tables.append(row_data)

tables[2:-10]
116/35:
tables = []
for table in doc.tables:
    # Iterate through each row in the table
    for row in table.rows:
        # Create an empty list to store the data from each cell in the row
        row_data = []
        # Iterate through each cell in the row
        for cell in row.cells:
            # Append the text content of the cell to the row_data list
            row_data.append(cell.text)
        # Print the row data
        tables.append(row_data)

tables[2:-10]
116/36:
tables = []
for table in doc.tables:
    # Iterate through each row in the table
    for row in table.rows:
        # Create an empty list to store the data from each cell in the row
        row_data = []
        # Iterate through each cell in the row
        for cell in row.cells:
            # Append the text content of the cell to the row_data list
            row_data.append(cell.text)
        # Print the row data
        tables.append(row_data)

tables[2:-20]
116/37:
tables = []
for table in doc.tables:
    # Iterate through each row in the table
    for row in table.rows:
        # Create an empty list to store the data from each cell in the row
        row_data = []
        # Iterate through each cell in the row
        for cell in row.cells:
            # Append the text content of the cell to the row_data list
            row_data.append(cell.text)
        # Print the row data
        tables.append(row_data)

tables[2:-30]
116/38:
tables = []
for table in doc.tables:
    # Iterate through each row in the table
    for row in table.rows:
        # Create an empty list to store the data from each cell in the row
        row_data = []
        # Iterate through each cell in the row
        for cell in row.cells:
            # Append the text content of the cell to the row_data list
            row_data.append(cell.text)
        # Print the row data
        tables.append(row_data)

tables[2:-40]
116/39:
tables = []
for table in doc.tables:
    # Iterate through each row in the table
    for row in table.rows:
        # Create an empty list to store the data from each cell in the row
        row_data = []
        # Iterate through each cell in the row
        for cell in row.cells:
            # Append the text content of the cell to the row_data list
            row_data.append(cell.text)
        # Print the row data
        tables.append(row_data)

tables[2:-50]
116/40:
tables = []
for table in doc.tables:
    # Iterate through each row in the table
    for row in table.rows:
        # Create an empty list to store the data from each cell in the row
        row_data = []
        # Iterate through each cell in the row
        for cell in row.cells:
            # Append the text content of the cell to the row_data list
            row_data.append(cell.text)
        # Print the row data
        tables.append(row_data)

tables[2:-45]
116/41:
tables = []
for table in doc.tables:
    # Iterate through each row in the table
    for row in table.rows:
        # Create an empty list to store the data from each cell in the row
        row_data = []
        # Iterate through each cell in the row
        for cell in row.cells:
            # Append the text content of the cell to the row_data list
            row_data.append(cell.text)
        # Print the row data
        tables.append(row_data)

tables[2:-40]
116/42:
tables = []
for table in doc.tables:
    # Iterate through each row in the table
    for row in table.rows:
        # Create an empty list to store the data from each cell in the row
        row_data = []
        # Iterate through each cell in the row
        for cell in row.cells:
            # Append the text content of the cell to the row_data list
            row_data.append(cell.text)
        # Print the row data
        tables.append(row_data)

tables[2:-43]
116/43:
tables = []
for table in doc.tables:
    # Iterate through each row in the table
    for row in table.rows:
        # Create an empty list to store the data from each cell in the row
        row_data = []
        # Iterate through each cell in the row
        for cell in row.cells:
            # Append the text content of the cell to the row_data list
            row_data.append(cell.text)
        # Print the row data
        tables.append(row_data)

ans = tables[2:-43]
116/44: ans
116/45:
for x in ans:
    s = ' '.join(x)
    print(x)
116/46:
for x in ans:
    s = ' '.join(x)
    print(s)
116/47: ans
116/48:
import re

# Input string
input_string = 'Гигиена пәніне кіріспе:\n\n1. Қазақстандағы тағам  гигиенасын  негізін қалаушы:\nШарманов Т.Ш.\nАмрин К.Р.;\nСоловьев З.П.\nЧарльз Дарвин\nНеменко Б.А.\n2. Гигиенаның түрлері:\nеңбек гигиенасы;\nэнтомология;\nпаразитология;\nжануарлардың мінез-құлық ғылымы\nэкология.\n3. Биологиялық фактор:\nмикроорганизмдер;\nылғалдылық;\nөсімдіктер, жәндіктер;\nкүн сәулесі.\n4. Гигиена бойынша зерттеу әдістері:\nсанитарлық сипаттама;\nбиохимиялық;\nультрадыбыстық;\nрадиографиялық.'

# Define the pattern for splitting the input string
pattern = r'\d+\..+?(?=\d+\.)'

# Use regex to split the input string based on the pattern
split_strings = re.findall(pattern, input_string, re.DOTALL)

# Print the split strings
for string in split_strings:
    print(string.strip())
116/49:
import re

# Input string
input_string = 'Гигиена пәніне кіріспе:\n\n1. Қазақстандағы тағам  гигиенасын  негізін қалаушы:\nШарманов Т.Ш.\nАмрин К.Р.;\nСоловьев З.П.\nЧарльз Дарвин\nНеменко Б.А.\n2. Гигиенаның түрлері:\nеңбек гигиенасы;\nэнтомология;\nпаразитология;\nжануарлардың мінез-құлық ғылымы\nэкология.\n3. Биологиялық фактор:\nмикроорганизмдер;\nылғалдылық;\nөсімдіктер, жәндіктер;\nкүн сәулесі.\n4. Гигиена бойынша зерттеу әдістері:\nсанитарлық сипаттама;\nбиохимиялық;\nультрадыбыстық;\nрадиографиялық.'

# Define the pattern for splitting the input string
pattern = r'\d+\..+?(?=\d+\.)'

# Use regex to split the input string based on the pattern
split_strings = re.findall(pattern, input_string, re.DOTALL)

# Print the split strings
a = ""
for string in split_strings:
    a = string.strip()
#     break
116/50:
import re

# Input string
input_string = 'Гигиена пәніне кіріспе:\n\n1. Қазақстандағы тағам  гигиенасын  негізін қалаушы:\nШарманов Т.Ш.\nАмрин К.Р.;\nСоловьев З.П.\nЧарльз Дарвин\nНеменко Б.А.\n2. Гигиенаның түрлері:\nеңбек гигиенасы;\nэнтомология;\nпаразитология;\nжануарлардың мінез-құлық ғылымы\nэкология.\n3. Биологиялық фактор:\nмикроорганизмдер;\nылғалдылық;\nөсімдіктер, жәндіктер;\nкүн сәулесі.\n4. Гигиена бойынша зерттеу әдістері:\nсанитарлық сипаттама;\nбиохимиялық;\nультрадыбыстық;\nрадиографиялық.'

# Define the pattern for splitting the input string
pattern = r'\d+\..+?(?=\d+\.)'

# Use regex to split the input string based on the pattern
split_strings = re.findall(pattern, input_string, re.DOTALL)

# Print the split strings
a = ""
for string in split_strings:
    a = string.strip()
#     break
a
116/51:
import re

# Input string
input_string = 'Гигиена пәніне кіріспе:\n\n1. Қазақстандағы тағам  гигиенасын  негізін қалаушы:\nШарманов Т.Ш.\nАмрин К.Р.;\nСоловьев З.П.\nЧарльз Дарвин\nНеменко Б.А.\n2. Гигиенаның түрлері:\nеңбек гигиенасы;\nэнтомология;\nпаразитология;\nжануарлардың мінез-құлық ғылымы\nэкология.\n3. Биологиялық фактор:\nмикроорганизмдер;\nылғалдылық;\nөсімдіктер, жәндіктер;\nкүн сәулесі.\n4. Гигиена бойынша зерттеу әдістері:\nсанитарлық сипаттама;\nбиохимиялық;\nультрадыбыстық;\nрадиографиялық.'

# Define the pattern for splitting the input string
pattern = r'\d+\..+?(?=\d+\.)'

# Use regex to split the input string based on the pattern
split_strings = re.findall(pattern, input_string, re.DOTALL)

# Print the split strings
a = ""
for string in split_strings:
    print(string.strip())
#     break
116/52:
import re

# Input string
input_string = 'Гигиена пәніне кіріспе:\n\n1. Қазақстандағы тағам  гигиенасын  негізін қалаушы:\nШарманов Т.Ш.\nАмрин К.Р.;\nСоловьев З.П.\nЧарльз Дарвин\nНеменко Б.А.\n2. Гигиенаның түрлері:\nеңбек гигиенасы;\nэнтомология;\nпаразитология;\nжануарлардың мінез-құлық ғылымы\nэкология.\n3. Биологиялық фактор:\nмикроорганизмдер;\nылғалдылық;\nөсімдіктер, жәндіктер;\nкүн сәулесі.\n4. Гигиена бойынша зерттеу әдістері:\nсанитарлық сипаттама;\nбиохимиялық;\nультрадыбыстық;\nрадиографиялық.'

# Define the pattern for splitting the input string
pattern = r'\d+\..+?(?=\d+\.)'
for x in ans:
    s = ' '.join(x)
    split_strings = re.findall(pattern, input_string, re.DOTALL)
    for string in split_strings:
        print(string.strip())
# Use regex to split the input string based on the pattern
# split_strings = re.findall(pattern, input_string, re.DOTALL)

# Print the split strings
# a = ""
# for string in split_strings:
#     print(string.strip())
#     break
116/53:
import re

# Input string
input_string = 'Гигиена пәніне кіріспе:\n\n1. Қазақстандағы тағам  гигиенасын  негізін қалаушы:\nШарманов Т.Ш.\nАмрин К.Р.;\nСоловьев З.П.\nЧарльз Дарвин\nНеменко Б.А.\n2. Гигиенаның түрлері:\nеңбек гигиенасы;\nэнтомология;\nпаразитология;\nжануарлардың мінез-құлық ғылымы\nэкология.\n3. Биологиялық фактор:\nмикроорганизмдер;\nылғалдылық;\nөсімдіктер, жәндіктер;\nкүн сәулесі.\n4. Гигиена бойынша зерттеу әдістері:\nсанитарлық сипаттама;\nбиохимиялық;\nультрадыбыстық;\nрадиографиялық.'

# Define the pattern for splitting the input string
pattern = r'\d+\..+?(?=\d+\.)'
for x in ans:
    input_string = ' '.join(x)
    split_strings = re.findall(pattern, input_string, re.DOTALL)
    for string in split_strings:
        print(string.strip())
# Use regex to split the input string based on the pattern
# split_strings = re.findall(pattern, input_string, re.DOTALL)

# Print the split strings
# a = ""
# for string in split_strings:
#     print(string.strip())
#     break
116/54:
import re

# Input string
input_string = 'Гигиена пәніне кіріспе:\n\n1. Қазақстандағы тағам  гигиенасын  негізін қалаушы:\nШарманов Т.Ш.\nАмрин К.Р.;\nСоловьев З.П.\nЧарльз Дарвин\nНеменко Б.А.\n2. Гигиенаның түрлері:\nеңбек гигиенасы;\nэнтомология;\nпаразитология;\nжануарлардың мінез-құлық ғылымы\nэкология.\n3. Биологиялық фактор:\nмикроорганизмдер;\nылғалдылық;\nөсімдіктер, жәндіктер;\nкүн сәулесі.\n4. Гигиена бойынша зерттеу әдістері:\nсанитарлық сипаттама;\nбиохимиялық;\nультрадыбыстық;\nрадиографиялық.'

# Define the pattern for splitting the input string
pattern = r'\d+\..+?(?=\d+\.)'
prefixes = ["A)", "B)", "C)", "D)", "E)", "F)", "G)", "H)", "I)", "J)", "K)", "L)", "M)", "N)", "O)", "P)", "Q)", "R)", "S)", "T)", "U)", "V)", "W)", "X)", "Y)", "Z)"]
for x in ans:
    input_string = ' '.join(x)
    split_strings = re.findall(pattern, input_string, re.DOTALL)
    for i, string in enumerate(split_strings[1:], start=1):
        prefix = prefixes[i-1]
        variant_string = string.strip()
        print(f"{prefix} {variant_string}")
#     for string in split_strings:
#         print(string.strip())
# Use regex to split the input string based on the pattern
# split_strings = re.findall(pattern, input_string, re.DOTALL)

# Print the split strings
# a = ""
# for string in split_strings:
#     print(string.strip())
#     break
116/55:
import re

# Input string
input_string = 'Гигиена пәніне кіріспе:\n\n1. Қазақстандағы тағам  гигиенасын  негізін қалаушы:\nШарманов Т.Ш.\nАмрин К.Р.;\nСоловьев З.П.\nЧарльз Дарвин\nНеменко Б.А.\n2. Гигиенаның түрлері:\nеңбек гигиенасы;\nэнтомология;\nпаразитология;\nжануарлардың мінез-құлық ғылымы\nэкология.\n3. Биологиялық фактор:\nмикроорганизмдер;\nылғалдылық;\nөсімдіктер, жәндіктер;\nкүн сәулесі.\n4. Гигиена бойынша зерттеу әдістері:\nсанитарлық сипаттама;\nбиохимиялық;\nультрадыбыстық;\nрадиографиялық.'

# Define the pattern for splitting the input string
pattern = r'\d+\..+?(?=\d+\.)'
prefixes = ["A)", "B)", "C)", "D)", "E)", "F)", "G)", "H)", "I)", "J)", "K)", "L)", "M)", "N)", "O)", "P)", "Q)", "R)", "S)", "T)", "U)", "V)", "W)", "X)", "Y)", "Z)"]
for x in ans:
    input_string = ' '.join(x)
    split_strings = re.findall(pattern, input_string, re.DOTALL)
    for i, string in enumerate(split_strings[1:], start=1):
        prefix = prefixes[i-1]
        entries = string.strip().split("\n")
        for entry in entries:
            print(f"{prefix}. {entry}")
#     for string in split_strings:
#         print(string.strip())
# Use regex to split the input string based on the pattern
# split_strings = re.findall(pattern, input_string, re.DOTALL)

# Print the split strings
# a = ""
# for string in split_strings:
#     print(string.strip())
#     break
116/56:
import re

# Input string
input_string = 'Гигиена пәніне кіріспе:\n\n1. Қазақстандағы тағам  гигиенасын  негізін қалаушы:\nШарманов Т.Ш.\nАмрин К.Р.;\nСоловьев З.П.\nЧарльз Дарвин\nНеменко Б.А.\n2. Гигиенаның түрлері:\nеңбек гигиенасы;\nэнтомология;\nпаразитология;\nжануарлардың мінез-құлық ғылымы\nэкология.\n3. Биологиялық фактор:\nмикроорганизмдер;\nылғалдылық;\nөсімдіктер, жәндіктер;\nкүн сәулесі.\n4. Гигиена бойынша зерттеу әдістері:\nсанитарлық сипаттама;\nбиохимиялық;\nультрадыбыстық;\nрадиографиялық.'

# Define the pattern for splitting the input string
pattern = r'\d+\..+?(?=\d+\.)'
prefixes = ["A", "B", "C", "D", "E", "F", "G", "H", "I", "J", "K", "L", "M", "N", "O", "P", "Q", "R", "S", "T", "U", "V", "W", "X", "Y", "Z"]
for x in ans:
    input_string = ' '.join(x)
    split_strings = re.findall(pattern, input_string, re.DOTALL)
    for i, string in enumerate(split_strings[1:], start=1):
        prefix = prefixes[i-1]
        entries = string.strip().split("\n")
        for entry in entries:
            print(f"{prefix}. {entry}")
#     for string in split_strings:
#         print(string.strip())
# Use regex to split the input string based on the pattern
# split_strings = re.findall(pattern, input_string, re.DOTALL)

# Print the split strings
# a = ""
# for string in split_strings:
#     print(string.strip())
#     break
116/57:
import re

# Input string
input_string = 'Гигиена пәніне кіріспе:\n\n1. Қазақстандағы тағам  гигиенасын  негізін қалаушы:\nШарманов Т.Ш.\nАмрин К.Р.;\nСоловьев З.П.\nЧарльз Дарвин\nНеменко Б.А.\n2. Гигиенаның түрлері:\nеңбек гигиенасы;\nэнтомология;\nпаразитология;\nжануарлардың мінез-құлық ғылымы\nэкология.\n3. Биологиялық фактор:\nмикроорганизмдер;\nылғалдылық;\nөсімдіктер, жәндіктер;\nкүн сәулесі.\n4. Гигиена бойынша зерттеу әдістері:\nсанитарлық сипаттама;\nбиохимиялық;\nультрадыбыстық;\nрадиографиялық.'

# Define the pattern for splitting the input string
pattern = r'\d+\..+?(?=\d+\.)'
prefixes = ["A", "B", "C", "D", "E", "F", "G", "H", "I", "J", "K", "L", "M", "N", "O", "P", "Q", "R", "S", "T", "U", "V", "W", "X", "Y", "Z"]
for x in ans:
    input_string = ' '.join(x)
    split_strings = re.findall(pattern, input_string, re.DOTALL)
    for i, string in enumerate(split_strings[1:], start=1):
        prefix = prefixes[i-1]
        entries = string.strip().split("\n")
        for entry in entries:
            print(f"{prefix}. {entry}")
#     for string in split_strings:
#         print(string.strip())
# Use regex to split the input string based on the pattern
# split_strings = re.findall(pattern, input_string, re.DOTALL)

# Print the split strings
# a = ""
# for string in split_strings:
#     print(string.strip())
#     break
116/58:
import re

# Input string
input_string = 'Гигиена пәніне кіріспе:\n\n1. Қазақстандағы тағам  гигиенасын  негізін қалаушы:\nШарманов Т.Ш.\nАмрин К.Р.;\nСоловьев З.П.\nЧарльз Дарвин\nНеменко Б.А.\n2. Гигиенаның түрлері:\nеңбек гигиенасы;\nэнтомология;\nпаразитология;\nжануарлардың мінез-құлық ғылымы\nэкология.\n3. Биологиялық фактор:\nмикроорганизмдер;\nылғалдылық;\nөсімдіктер, жәндіктер;\nкүн сәулесі.\n4. Гигиена бойынша зерттеу әдістері:\nсанитарлық сипаттама;\nбиохимиялық;\nультрадыбыстық;\nрадиографиялық.'

# Define the pattern for splitting the input string
pattern = r'\d+\..+?(?=\d+\.)'
prefixes = ["A", "B", "C", "D", "E", "F", "G", "H", "I", "J", "K", "L", "M", "N", "O", "P", "Q", "R", "S", "T", "U", "V", "W", "X", "Y", "Z"]
for x in ans:
    input_string = ' '.join(x)
    split_strings = re.findall(pattern, input_string, re.DOTALL)
#     for i, string in enumerate(split_strings[1:], start=1):
#         prefix = prefixes[i-1]
#         entries = string.strip().split("\n")
#         for entry in entries:
#             print(f"{prefix}. {entry}")
    for string in split_strings:
        print(string.strip())
# Use regex to split the input string based on the pattern
# split_strings = re.findall(pattern, input_string, re.DOTALL)

# Print the split strings
# a = ""
# for string in split_strings:
#     print(string.strip())
#     break
116/59:
import re

# Input string
input_string = 'Гигиена пәніне кіріспе:\n\n1. Қазақстандағы тағам  гигиенасын  негізін қалаушы:\nШарманов Т.Ш.\nАмрин К.Р.;\nСоловьев З.П.\nЧарльз Дарвин\nНеменко Б.А.\n2. Гигиенаның түрлері:\nеңбек гигиенасы;\nэнтомология;\nпаразитология;\nжануарлардың мінез-құлық ғылымы\nэкология.\n3. Биологиялық фактор:\nмикроорганизмдер;\nылғалдылық;\nөсімдіктер, жәндіктер;\nкүн сәулесі.\n4. Гигиена бойынша зерттеу әдістері:\nсанитарлық сипаттама;\nбиохимиялық;\nультрадыбыстық;\nрадиографиялық.'

# Define the pattern for splitting the input string
pattern = r'\d+\..+?(?=\d+\.)'
prefixes = ["A", "B", "C", "D", "E", "F", "G", "H", "I", "J", "K", "L", "M", "N", "O", "P", "Q", "R", "S", "T", "U", "V", "W", "X", "Y", "Z"]
for x in ans:
    input_string = ' '.join(x)
    split_strings = re.findall(pattern, input_string, re.DOTALL)
    print(split_strings)
    break
    for string in split_strings:
        print(string.strip())
# Use regex to split the input string based on the pattern
# split_strings = re.findall(pattern, input_string, re.DOTALL)

# Print the split strings
# a = ""
# for string in split_strings:
#     print(string.strip())
#     break
116/60:
import re

# Input string
input_string = 'Гигиена пәніне кіріспе:\n\n1. Қазақстандағы тағам  гигиенасын  негізін қалаушы:\nШарманов Т.Ш.\nАмрин К.Р.;\nСоловьев З.П.\nЧарльз Дарвин\nНеменко Б.А.\n2. Гигиенаның түрлері:\nеңбек гигиенасы;\nэнтомология;\nпаразитология;\nжануарлардың мінез-құлық ғылымы\nэкология.\n3. Биологиялық фактор:\nмикроорганизмдер;\nылғалдылық;\nөсімдіктер, жәндіктер;\nкүн сәулесі.\n4. Гигиена бойынша зерттеу әдістері:\nсанитарлық сипаттама;\nбиохимиялық;\nультрадыбыстық;\nрадиографиялық.'

# Define the pattern for splitting the input string
pattern = r'\d+\..+?(?=\d+\.)'
prefixes = ["A", "B", "C", "D", "E", "F", "G", "H", "I", "J", "K", "L", "M", "N", "O", "P", "Q", "R", "S", "T", "U", "V", "W", "X", "Y", "Z"]
for x in ans:
    input_string = ' '.join(x)
    split_strings = re.findall(pattern, input_string, re.DOTALL)
    print(split_strings.replace('\n', f'\n{prefices[0]}'))
    break
    for string in split_strings:
        print(string.strip())
# Use regex to split the input string based on the pattern
# split_strings = re.findall(pattern, input_string, re.DOTALL)

# Print the split strings
# a = ""
# for string in split_strings:
#     print(string.strip())
#     break
116/61:
import re

# Input string
input_string = 'Гигиена пәніне кіріспе:\n\n1. Қазақстандағы тағам  гигиенасын  негізін қалаушы:\nШарманов Т.Ш.\nАмрин К.Р.;\nСоловьев З.П.\nЧарльз Дарвин\nНеменко Б.А.\n2. Гигиенаның түрлері:\nеңбек гигиенасы;\nэнтомология;\nпаразитология;\nжануарлардың мінез-құлық ғылымы\nэкология.\n3. Биологиялық фактор:\nмикроорганизмдер;\nылғалдылық;\nөсімдіктер, жәндіктер;\nкүн сәулесі.\n4. Гигиена бойынша зерттеу әдістері:\nсанитарлық сипаттама;\nбиохимиялық;\nультрадыбыстық;\nрадиографиялық.'

# Define the pattern for splitting the input string
pattern = r'\d+\..+?(?=\d+\.)'
prefixes = ["A", "B", "C", "D", "E", "F", "G", "H", "I", "J", "K", "L", "M", "N", "O", "P", "Q", "R", "S", "T", "U", "V", "W", "X", "Y", "Z"]
for x in ans:
    input_string = ' '.join(x)
    split_strings = re.findall(pattern, input_string, re.DOTALL)
    print(split_strings[0].replace('\n', f'\n{prefices[0]}'))
    break
    for string in split_strings:
        print(string.strip())
# Use regex to split the input string based on the pattern
# split_strings = re.findall(pattern, input_string, re.DOTALL)

# Print the split strings
# a = ""
# for string in split_strings:
#     print(string.strip())
#     break
116/62:
import re

# Input string
input_string = 'Гигиена пәніне кіріспе:\n\n1. Қазақстандағы тағам  гигиенасын  негізін қалаушы:\nШарманов Т.Ш.\nАмрин К.Р.;\nСоловьев З.П.\nЧарльз Дарвин\nНеменко Б.А.\n2. Гигиенаның түрлері:\nеңбек гигиенасы;\nэнтомология;\nпаразитология;\nжануарлардың мінез-құлық ғылымы\nэкология.\n3. Биологиялық фактор:\nмикроорганизмдер;\nылғалдылық;\nөсімдіктер, жәндіктер;\nкүн сәулесі.\n4. Гигиена бойынша зерттеу әдістері:\nсанитарлық сипаттама;\nбиохимиялық;\nультрадыбыстық;\nрадиографиялық.'

# Define the pattern for splitting the input string
pattern = r'\d+\..+?(?=\d+\.)'
prefixes = ["A", "B", "C", "D", "E", "F", "G", "H", "I", "J", "K", "L", "M", "N", "O", "P", "Q", "R", "S", "T", "U", "V", "W", "X", "Y", "Z"]
for x in ans:
    input_string = ' '.join(x)
    split_strings = re.findall(pattern, input_string, re.DOTALL)
    print(split_strings[0].replace('\n', f'\n{prefixes[0]}'))
    break
    for string in split_strings:
        print(string.strip())
# Use regex to split the input string based on the pattern
# split_strings = re.findall(pattern, input_string, re.DOTALL)

# Print the split strings
# a = ""
# for string in split_strings:
#     print(string.strip())
#     break
116/63:
import re

# Input string
input_string = 'Гигиена пәніне кіріспе:\n\n1. Қазақстандағы тағам  гигиенасын  негізін қалаушы:\nШарманов Т.Ш.\nАмрин К.Р.;\nСоловьев З.П.\nЧарльз Дарвин\nНеменко Б.А.\n2. Гигиенаның түрлері:\nеңбек гигиенасы;\nэнтомология;\nпаразитология;\nжануарлардың мінез-құлық ғылымы\nэкология.\n3. Биологиялық фактор:\nмикроорганизмдер;\nылғалдылық;\nөсімдіктер, жәндіктер;\nкүн сәулесі.\n4. Гигиена бойынша зерттеу әдістері:\nсанитарлық сипаттама;\nбиохимиялық;\nультрадыбыстық;\nрадиографиялық.'

# Define the pattern for splitting the input string
pattern = r'\d+\..+?(?=\d+\.)'
prefixes = ["A", "B", "C", "D", "E", "F", "G", "H", "I", "J", "K", "L", "M", "N", "O", "P", "Q", "R", "S", "T", "U", "V", "W", "X", "Y", "Z"]
for x in ans:
    input_string = ' '.join(x)
    split_strings = re.findall(pattern, input_string, re.DOTALL)
    print(split_strings[0].replace('\n', f'\n{prefixes[0]}. '))
    break
    for string in split_strings:
        print(string.strip())
# Use regex to split the input string based on the pattern
# split_strings = re.findall(pattern, input_string, re.DOTALL)

# Print the split strings
# a = ""
# for string in split_strings:
#     print(string.strip())
#     break
116/64:
import re

# Input string
input_string = 'Гигиена пәніне кіріспе:\n\n1. Қазақстандағы тағам  гигиенасын  негізін қалаушы:\nШарманов Т.Ш.\nАмрин К.Р.;\nСоловьев З.П.\nЧарльз Дарвин\nНеменко Б.А.\n2. Гигиенаның түрлері:\nеңбек гигиенасы;\nэнтомология;\nпаразитология;\nжануарлардың мінез-құлық ғылымы\nэкология.\n3. Биологиялық фактор:\nмикроорганизмдер;\nылғалдылық;\nөсімдіктер, жәндіктер;\nкүн сәулесі.\n4. Гигиена бойынша зерттеу әдістері:\nсанитарлық сипаттама;\nбиохимиялық;\nультрадыбыстық;\nрадиографиялық.'

# Define the pattern for splitting the input string
pattern = r'\d+\..+?(?=\d+\.)'
prefixes = ["A", "B", "C", "D", "E", "F", "G", "H", "I", "J", "K", "L", "M", "N", "O", "P", "Q", "R", "S", "T", "U", "V", "W", "X", "Y", "Z"]
for x in ans:
    input_string = ' '.join(x)
    split_strings = re.findall(pattern, input_string, re.DOTALL)
    print()
    cnt = 0
    for string in split_strings:
        string = string[:-1]
        split_strings[0].replace('\n', f'\n{prefixes[cnt]}. ')
        cnt += 1
        print(string.strip())
# Use regex to split the input string based on the pattern
# split_strings = re.findall(pattern, input_string, re.DOTALL)

# Print the split strings
# a = ""
# for string in split_strings:
#     print(string.strip())
#     break
116/65:
import re

# Input string
input_string = 'Гигиена пәніне кіріспе:\n\n1. Қазақстандағы тағам  гигиенасын  негізін қалаушы:\nШарманов Т.Ш.\nАмрин К.Р.;\nСоловьев З.П.\nЧарльз Дарвин\nНеменко Б.А.\n2. Гигиенаның түрлері:\nеңбек гигиенасы;\nэнтомология;\nпаразитология;\nжануарлардың мінез-құлық ғылымы\nэкология.\n3. Биологиялық фактор:\nмикроорганизмдер;\nылғалдылық;\nөсімдіктер, жәндіктер;\nкүн сәулесі.\n4. Гигиена бойынша зерттеу әдістері:\nсанитарлық сипаттама;\nбиохимиялық;\nультрадыбыстық;\nрадиографиялық.'

# Define the pattern for splitting the input string
pattern = r'\d+\..+?(?=\d+\.)'
prefixes = ["A", "B", "C", "D", "E", "F", "G", "H", "I", "J", "K", "L", "M", "N", "O", "P", "Q", "R", "S", "T", "U", "V", "W", "X", "Y", "Z"]
for x in ans:
    input_string = ' '.join(x)
    split_strings = re.findall(pattern, input_string, re.DOTALL)
    print()
    cnt = 0
    for string in split_strings:
        string = string[:-1]
        string.replace('\n', f'\n{prefixes[cnt]}. ')
        cnt += 1
        print(string.strip())
# Use regex to split the input string based on the pattern
# split_strings = re.findall(pattern, input_string, re.DOTALL)

# Print the split strings
# a = ""
# for string in split_strings:
#     print(string.strip())
#     break
116/66:
import re

# Input string
input_string = 'Гигиена пәніне кіріспе:\n\n1. Қазақстандағы тағам  гигиенасын  негізін қалаушы:\nШарманов Т.Ш.\nАмрин К.Р.;\nСоловьев З.П.\nЧарльз Дарвин\nНеменко Б.А.\n2. Гигиенаның түрлері:\nеңбек гигиенасы;\nэнтомология;\nпаразитология;\nжануарлардың мінез-құлық ғылымы\nэкология.\n3. Биологиялық фактор:\nмикроорганизмдер;\nылғалдылық;\nөсімдіктер, жәндіктер;\nкүн сәулесі.\n4. Гигиена бойынша зерттеу әдістері:\nсанитарлық сипаттама;\nбиохимиялық;\nультрадыбыстық;\nрадиографиялық.'

# Define the pattern for splitting the input string
pattern = r'\d+\..+?(?=\d+\.)'
prefixes = ["A", "B", "C", "D", "E", "F", "G", "H", "I", "J", "K", "L", "M", "N", "O", "P", "Q", "R", "S", "T", "U", "V", "W", "X", "Y", "Z"]
for x in ans:
    input_string = ' '.join(x)
    split_strings = re.findall(pattern, input_string, re.DOTALL)
    print()
    cnt = 0
    for string in split_strings:
        string = string[:-1]
        string = string.replace('\n', f'\n{prefixes[cnt]}. ')
        cnt += 1
        print(string.strip())
# Use regex to split the input string based on the pattern
# split_strings = re.findall(pattern, input_string, re.DOTALL)

# Print the split strings
# a = ""
# for string in split_strings:
#     print(string.strip())
#     break
116/67:
import re

# Input string
input_string = 'Гигиена пәніне кіріспе:\n\n1. Қазақстандағы тағам  гигиенасын  негізін қалаушы:\nШарманов Т.Ш.\nАмрин К.Р.;\nСоловьев З.П.\nЧарльз Дарвин\nНеменко Б.А.\n2. Гигиенаның түрлері:\nеңбек гигиенасы;\nэнтомология;\nпаразитология;\nжануарлардың мінез-құлық ғылымы\nэкология.\n3. Биологиялық фактор:\nмикроорганизмдер;\nылғалдылық;\nөсімдіктер, жәндіктер;\nкүн сәулесі.\n4. Гигиена бойынша зерттеу әдістері:\nсанитарлық сипаттама;\nбиохимиялық;\nультрадыбыстық;\nрадиографиялық.'

# Define the pattern for splitting the input string
pattern = r'\d+\..+?(?=\d+\.)'
prefixes = ["A", "B", "C", "D", "E", "F", "G", "H", "I", "J", "K", "L", "M", "N", "O", "P", "Q", "R", "S", "T", "U", "V", "W", "X", "Y", "Z"]
for x in ans:
    input_string = ' '.join(x)
    split_strings = re.findall(pattern, input_string, re.DOTALL)
    print()
    cnt = 0
    for string in split_strings:
        string = string[:-1]
        for s in string:
            if s=='\n':
                print('sf')
        cnt += 1
        print(string.strip())
# Use regex to split the input string based on the pattern
# split_strings = re.findall(pattern, input_string, re.DOTALL)

# Print the split strings
# a = ""
# for string in split_strings:
#     print(string.strip())
#     break
116/68:
import re

# Input string
input_string = 'Гигиена пәніне кіріспе:\n\n1. Қазақстандағы тағам  гигиенасын  негізін қалаушы:\nШарманов Т.Ш.\nАмрин К.Р.;\nСоловьев З.П.\nЧарльз Дарвин\nНеменко Б.А.\n2. Гигиенаның түрлері:\nеңбек гигиенасы;\nэнтомология;\nпаразитология;\nжануарлардың мінез-құлық ғылымы\nэкология.\n3. Биологиялық фактор:\nмикроорганизмдер;\nылғалдылық;\nөсімдіктер, жәндіктер;\nкүн сәулесі.\n4. Гигиена бойынша зерттеу әдістері:\nсанитарлық сипаттама;\nбиохимиялық;\nультрадыбыстық;\nрадиографиялық.'

# Define the pattern for splitting the input string
pattern = r'\d+\..+?(?=\d+\.)'
prefixes = ["A", "B", "C", "D", "E", "F", "G", "H", "I", "J", "K", "L", "M", "N", "O", "P", "Q", "R", "S", "T", "U", "V", "W", "X", "Y", "Z"]
for x in ans:
    input_string = ' '.join(x)
    split_strings = re.findall(pattern, input_string, re.DOTALL)
    print()
    cnt = 0
    for string in split_strings:
        string = string[:-1]
        for s in range(len(string)-1):
            if s[i]=='\n':
                string = string[:i] + f'\nprefixes[cnt]' + string[i+1:]
                cnt += 1
                
        print(string.strip())
# Use regex to split the input string based on the pattern
# split_strings = re.findall(pattern, input_string, re.DOTALL)

# Print the split strings
# a = ""
# for string in split_strings:
#     print(string.strip())
#     break
116/69:
import re

# Input string
input_string = 'Гигиена пәніне кіріспе:\n\n1. Қазақстандағы тағам  гигиенасын  негізін қалаушы:\nШарманов Т.Ш.\nАмрин К.Р.;\nСоловьев З.П.\nЧарльз Дарвин\nНеменко Б.А.\n2. Гигиенаның түрлері:\nеңбек гигиенасы;\nэнтомология;\nпаразитология;\nжануарлардың мінез-құлық ғылымы\nэкология.\n3. Биологиялық фактор:\nмикроорганизмдер;\nылғалдылық;\nөсімдіктер, жәндіктер;\nкүн сәулесі.\n4. Гигиена бойынша зерттеу әдістері:\nсанитарлық сипаттама;\nбиохимиялық;\nультрадыбыстық;\nрадиографиялық.'

# Define the pattern for splitting the input string
pattern = r'\d+\..+?(?=\d+\.)'
prefixes = ["A", "B", "C", "D", "E", "F", "G", "H", "I", "J", "K", "L", "M", "N", "O", "P", "Q", "R", "S", "T", "U", "V", "W", "X", "Y", "Z"]
for x in ans:
    input_string = ' '.join(x)
    split_strings = re.findall(pattern, input_string, re.DOTALL)
    print()
    cnt = 0
    for string in split_strings:
        string = string[:-1]
        for i in range(len(string)-1):
            if string[i]=='\n':
                string = string[:i] + f'\nprefixes[cnt]' + string[i+1:]
                cnt += 1
                
        print(string.strip())
# Use regex to split the input string based on the pattern
# split_strings = re.findall(pattern, input_string, re.DOTALL)

# Print the split strings
# a = ""
# for string in split_strings:
#     print(string.strip())
#     break
116/70:
import re

# Input string
input_string = 'Гигиена пәніне кіріспе:\n\n1. Қазақстандағы тағам  гигиенасын  негізін қалаушы:\nШарманов Т.Ш.\nАмрин К.Р.;\nСоловьев З.П.\nЧарльз Дарвин\nНеменко Б.А.\n2. Гигиенаның түрлері:\nеңбек гигиенасы;\nэнтомология;\nпаразитология;\nжануарлардың мінез-құлық ғылымы\nэкология.\n3. Биологиялық фактор:\nмикроорганизмдер;\nылғалдылық;\nөсімдіктер, жәндіктер;\nкүн сәулесі.\n4. Гигиена бойынша зерттеу әдістері:\nсанитарлық сипаттама;\nбиохимиялық;\nультрадыбыстық;\nрадиографиялық.'

# Define the pattern for splitting the input string
pattern = r'\d+\..+?(?=\d+\.)'
prefixes = ["A", "B", "C", "D", "E", "F", "G", "H", "I", "J", "K", "L", "M", "N", "O", "P", "Q", "R", "S", "T", "U", "V", "W", "X", "Y", "Z"]
for x in ans:
    input_string = ' '.join(x)
    split_strings = re.findall(pattern, input_string, re.DOTALL)
    print()
    cnt = 0
    for string in split_strings:
        string = string[:-1]
        for i in range(len(string)-1):
            if string[i]=='\n':
                string = string[:i] + f'\n{prefixes[cnt]}' + string[i+1:]
                cnt += 1
                
        print(string.strip())
# Use regex to split the input string based on the pattern
# split_strings = re.findall(pattern, input_string, re.DOTALL)

# Print the split strings
# a = ""
# for string in split_strings:
#     print(string.strip())
#     break
116/71:
import re

# Input string
input_string = 'Гигиена пәніне кіріспе:\n\n1. Қазақстандағы тағам  гигиенасын  негізін қалаушы:\nШарманов Т.Ш.\nАмрин К.Р.;\nСоловьев З.П.\nЧарльз Дарвин\nНеменко Б.А.\n2. Гигиенаның түрлері:\nеңбек гигиенасы;\nэнтомология;\nпаразитология;\nжануарлардың мінез-құлық ғылымы\nэкология.\n3. Биологиялық фактор:\nмикроорганизмдер;\nылғалдылық;\nөсімдіктер, жәндіктер;\nкүн сәулесі.\n4. Гигиена бойынша зерттеу әдістері:\nсанитарлық сипаттама;\nбиохимиялық;\nультрадыбыстық;\nрадиографиялық.'

# Define the pattern for splitting the input string
pattern = r'\d+\..+?(?=\d+\.)'
prefixes = ["A", "B", "C", "D", "E", "F", "G", "H", "I", "J", "K", "L", "M", "N", "O", "P", "Q", "R", "S", "T", "U", "V", "W", "X", "Y", "Z"]
for x in ans:
    input_string = ' '.join(x)
    split_strings = re.findall(pattern, input_string, re.DOTALL)
    print()
    cnt = 0
    for string in split_strings:
        string = string[:-1]
        for i in range(len(string)-1):
            if string[i]=='\n':
                string = string[:i] + f'\n{prefixes[cnt]}. ' + string[i+1:]
                cnt += 1
                
        print(string.strip())
# Use regex to split the input string based on the pattern
# split_strings = re.findall(pattern, input_string, re.DOTALL)

# Print the split strings
# a = ""
# for string in split_strings:
#     print(string.strip())
#     break
116/72:
import re

# Input string
input_string = 'Гигиена пәніне кіріспе:\n\n1. Қазақстандағы тағам  гигиенасын  негізін қалаушы:\nШарманов Т.Ш.\nАмрин К.Р.;\nСоловьев З.П.\nЧарльз Дарвин\nНеменко Б.А.\n2. Гигиенаның түрлері:\nеңбек гигиенасы;\nэнтомология;\nпаразитология;\nжануарлардың мінез-құлық ғылымы\nэкология.\n3. Биологиялық фактор:\nмикроорганизмдер;\nылғалдылық;\nөсімдіктер, жәндіктер;\nкүн сәулесі.\n4. Гигиена бойынша зерттеу әдістері:\nсанитарлық сипаттама;\nбиохимиялық;\nультрадыбыстық;\nрадиографиялық.'

# Define the pattern for splitting the input string
pattern = r'\d+\..+?(?=\d+\.)'
prefixes = ["A", "B", "C", "D", "E", "F", "G", "H", "I", "J", "K", "L", "M", "N", "O", "P", "Q", "R", "S", "T", "U", "V", "W", "X", "Y", "Z"]
for x in ans:
    input_string = ' '.join(x)
    split_strings = re.findall(pattern, input_string, re.DOTALL)
    print(split_strings)
#     cnt = 0
#     for string in split_strings:
#         string = string[:-1]
#         for i in range(len(string)-1):
#             if string[i]=='\n':
#                 string = string[:i] + f'\n{prefixes[cnt]}. ' + string[i+1:]
#                 cnt += 1
                
#         print(string.strip())
# Use regex to split the input string based on the pattern
# split_strings = re.findall(pattern, input_string, re.DOTALL)

# Print the split strings
# a = ""
# for string in split_strings:
#     print(string.strip())
#     break
116/73:
import re

# Input string
input_string = 'Гигиена пәніне кіріспе:\n\n1. Қазақстандағы тағам  гигиенасын  негізін қалаушы:\nШарманов Т.Ш.\nАмрин К.Р.;\nСоловьев З.П.\nЧарльз Дарвин\nНеменко Б.А.\n2. Гигиенаның түрлері:\nеңбек гигиенасы;\nэнтомология;\nпаразитология;\nжануарлардың мінез-құлық ғылымы\nэкология.\n3. Биологиялық фактор:\nмикроорганизмдер;\nылғалдылық;\nөсімдіктер, жәндіктер;\nкүн сәулесі.\n4. Гигиена бойынша зерттеу әдістері:\nсанитарлық сипаттама;\nбиохимиялық;\nультрадыбыстық;\nрадиографиялық.'

# Define the pattern for splitting the input string
pattern = r'\d+\..+?(?=\d+\.)'
prefixes = ["A", "B", "C", "D", "E", "F", "G", "H", "I", "J", "K", "L", "M", "N", "O", "P", "Q", "R", "S", "T", "U", "V", "W", "X", "Y", "Z"]
for x in ans:
    input_string = ' '.join(x)
    split_strings = re.findall(pattern, input_string, re.DOTALL)
    print(split_strings)
    for string in split_strings:
        string = string[:-1]
        cnt = 0
        for i in range(len(string)-1):
            if string[i]=='\n':
                string = string[:i] + f'\n{prefixes[cnt]}. ' + string[i+1:]
                cnt += 1
                
        print(string.strip())
# Use regex to split the input string based on the pattern
# split_strings = re.findall(pattern, input_string, re.DOTALL)

# Print the split strings
# a = ""
# for string in split_strings:
#     print(string.strip())
#     break
116/74:
import re

# Input string
input_string = 'Гигиена пәніне кіріспе:\n\n1. Қазақстандағы тағам  гигиенасын  негізін қалаушы:\nШарманов Т.Ш.\nАмрин К.Р.;\nСоловьев З.П.\nЧарльз Дарвин\nНеменко Б.А.\n2. Гигиенаның түрлері:\nеңбек гигиенасы;\nэнтомология;\nпаразитология;\nжануарлардың мінез-құлық ғылымы\nэкология.\n3. Биологиялық фактор:\nмикроорганизмдер;\nылғалдылық;\nөсімдіктер, жәндіктер;\nкүн сәулесі.\n4. Гигиена бойынша зерттеу әдістері:\nсанитарлық сипаттама;\nбиохимиялық;\nультрадыбыстық;\nрадиографиялық.'

# Define the pattern for splitting the input string
pattern = r'\d+\..+?(?=\d+\.)'
prefixes = ["A", "B", "C", "D", "E", "F", "G", "H", "I", "J", "K", "L", "M", "N", "O", "P", "Q", "R", "S", "T", "U", "V", "W", "X", "Y", "Z"]
for x in ans:
    input_string = ' '.join(x)
    split_strings = re.findall(pattern, input_string, re.DOTALL)
#     print(split_strings)
    for string in split_strings:
        string = string[:-1]
        cnt = 0
        for i in range(len(string)-1):
            if string[i]=='\n':
                string = string[:i] + f'\n{prefixes[cnt]}. ' + string[i+1:]
                cnt += 1
                
        print(string.strip())
# Use regex to split the input string based on the pattern
# split_strings = re.findall(pattern, input_string, re.DOTALL)

# Print the split strings
# a = ""
# for string in split_strings:
#     print(string.strip())
#     break
116/75:
import re

# Input string
input_string = 'Гигиена пәніне кіріспе:\n\n1. Қазақстандағы тағам  гигиенасын  негізін қалаушы:\nШарманов Т.Ш.\nАмрин К.Р.;\nСоловьев З.П.\nЧарльз Дарвин\nНеменко Б.А.\n2. Гигиенаның түрлері:\nеңбек гигиенасы;\nэнтомология;\nпаразитология;\nжануарлардың мінез-құлық ғылымы\nэкология.\n3. Биологиялық фактор:\nмикроорганизмдер;\nылғалдылық;\nөсімдіктер, жәндіктер;\nкүн сәулесі.\n4. Гигиена бойынша зерттеу әдістері:\nсанитарлық сипаттама;\nбиохимиялық;\nультрадыбыстық;\nрадиографиялық.'

# Define the pattern for splitting the input string
pattern = r'\d+\..+?(?=\d+\.)'
prefixes = ["A", "B", "C", "D", "E", "F", "G", "H", "I", "J", "K", "L", "M", "N", "O", "P", "Q", "R", "S", "T", "U", "V", "W", "X", "Y", "Z"]
for x in ans:
    input_string = ' '.join(x)
    split_strings = re.findall(pattern, input_string, re.DOTALL)
#     print(split_strings)
    for string in split_strings:
#         string = string[:-1]
        cnt = 0
        for i in range(len(string)-1):
            if string[i]=='\n':
                string = string[:i] + f'\n{prefixes[cnt]}. ' + string[i+1:]
                cnt += 1
                
        print(string.strip())
# Use regex to split the input string based on the pattern
# split_strings = re.findall(pattern, input_string, re.DOTALL)

# Print the split strings
# a = ""
# for string in split_strings:
#     print(string.strip())
#     break
116/76:
import re

# Input string
input_string = 'Гигиена пәніне кіріспе:\n\n1. Қазақстандағы тағам  гигиенасын  негізін қалаушы:\nШарманов Т.Ш.\nАмрин К.Р.;\nСоловьев З.П.\nЧарльз Дарвин\nНеменко Б.А.\n2. Гигиенаның түрлері:\nеңбек гигиенасы;\nэнтомология;\nпаразитология;\nжануарлардың мінез-құлық ғылымы\nэкология.\n3. Биологиялық фактор:\nмикроорганизмдер;\nылғалдылық;\nөсімдіктер, жәндіктер;\nкүн сәулесі.\n4. Гигиена бойынша зерттеу әдістері:\nсанитарлық сипаттама;\nбиохимиялық;\nультрадыбыстық;\nрадиографиялық.'

# Define the pattern for splitting the input string
pattern = r'\d+\..+?(?=\d+\.)'
prefixes = ["A", "B", "C", "D", "E", "F", "G", "H", "I", "J", "K", "L", "M", "N", "O", "P", "Q", "R", "S", "T", "U", "V", "W", "X", "Y", "Z"]
for x in ans:
    input_string = ' '.join(x)
    split_strings = re.findall(pattern, input_string, re.DOTALL)
#     print(split_strings)
    for string in split_strings:
#         string = string[:-1]
        cnt = 0
        for i in range(len(string)):
            if string[i]=='\n':
                string = string[:i] + f'\n{prefixes[cnt]}. ' + string[i+1:]
                cnt += 1
                
        print(string.strip())
# Use regex to split the input string based on the pattern
# split_strings = re.findall(pattern, input_string, re.DOTALL)

# Print the split strings
# a = ""
# for string in split_strings:
#     print(string.strip())
#     break
116/77:
import re

# Input string
input_string = 'Гигиена пәніне кіріспе:\n\n1. Қазақстандағы тағам  гигиенасын  негізін қалаушы:\nШарманов Т.Ш.\nАмрин К.Р.;\nСоловьев З.П.\nЧарльз Дарвин\nНеменко Б.А.\n2. Гигиенаның түрлері:\nеңбек гигиенасы;\nэнтомология;\nпаразитология;\nжануарлардың мінез-құлық ғылымы\nэкология.\n3. Биологиялық фактор:\nмикроорганизмдер;\nылғалдылық;\nөсімдіктер, жәндіктер;\nкүн сәулесі.\n4. Гигиена бойынша зерттеу әдістері:\nсанитарлық сипаттама;\nбиохимиялық;\nультрадыбыстық;\nрадиографиялық.'

# Define the pattern for splitting the input string
pattern = r'\d+\..+?(?=\d+\.)'
prefixes = ["A", "B", "C", "D", "E", "F", "G", "H", "I", "J", "K", "L", "M", "N", "O", "P", "Q", "R", "S", "T", "U", "V", "W", "X", "Y", "Z"]
for x in ans:
    input_string = ' '.join(x)
    split_strings = re.findall(pattern, input_string, re.DOTALL)
#     print(split_strings)
    for string in split_strings:
#         string = string[:-1]
        cnt = 0
        for i in range(len(string)):
            if string[i]=='\n':
                string = string[:i] + f'\n{prefixes[cnt]}. ' + string[i+1:]
                cnt += 1
                
        print(string.strip())
# Use regex to split the input string based on the pattern
# split_strings = re.findall(pattern, input_string, re.DOTALL)

# Print the split strings
# a = ""
# for string in split_strings:
#     print(string.strip())
#     break
116/78:
import re

# Input string
input_string = 'Гигиена пәніне кіріспе:\n\n1. Қазақстандағы тағам  гигиенасын  негізін қалаушы:\nШарманов Т.Ш.\nАмрин К.Р.;\nСоловьев З.П.\nЧарльз Дарвин\nНеменко Б.А.\n2. Гигиенаның түрлері:\nеңбек гигиенасы;\nэнтомология;\nпаразитология;\nжануарлардың мінез-құлық ғылымы\nэкология.\n3. Биологиялық фактор:\nмикроорганизмдер;\nылғалдылық;\nөсімдіктер, жәндіктер;\nкүн сәулесі.\n4. Гигиена бойынша зерттеу әдістері:\nсанитарлық сипаттама;\nбиохимиялық;\nультрадыбыстық;\nрадиографиялық.'

# Define the pattern for splitting the input string
pattern = r'\d+\..+?(?=\d+\.)'
prefixes = ["A", "B", "C", "D", "E", "F", "G", "H", "I", "J", "K", "L", "M", "N", "O", "P", "Q", "R", "S", "T", "U", "V", "W", "X", "Y", "Z"]
for x in ans:
    input_string = ' '.join(x)
    split_strings = re.findall(pattern, input_string, re.DOTALL)
    print(split_strings)
#     for string in split_strings:
# #         string = string[:-1]
#         cnt = 0
#         for i in range(len(string)):
#             if string[i]=='\n':
#                 string = string[:i] + f'\n{prefixes[cnt]}. ' + string[i+1:]
#                 cnt += 1
                
#         print(string.strip())
# Use regex to split the input string based on the pattern
# split_strings = re.findall(pattern, input_string, re.DOTALL)

# Print the split strings
# a = ""
# for string in split_strings:
#     print(string.strip())
#     break
116/79:
import re

# Input string
input_string = 'Гигиена пәніне кіріспе:\n\n1. Қазақстандағы тағам  гигиенасын  негізін қалаушы:\nШарманов Т.Ш.\nАмрин К.Р.;\nСоловьев З.П.\nЧарльз Дарвин\nНеменко Б.А.\n2. Гигиенаның түрлері:\nеңбек гигиенасы;\nэнтомология;\nпаразитология;\nжануарлардың мінез-құлық ғылымы\nэкология.\n3. Биологиялық фактор:\nмикроорганизмдер;\nылғалдылық;\nөсімдіктер, жәндіктер;\nкүн сәулесі.\n4. Гигиена бойынша зерттеу әдістері:\nсанитарлық сипаттама;\nбиохимиялық;\nультрадыбыстық;\nрадиографиялық.'

# Define the pattern for splitting the input string
pattern = r'\d+\..+?(?=\d+\.)'
prefixes = ["A", "B", "C", "D", "E", "F", "G", "H", "I", "J", "K", "L", "M", "N", "O", "P", "Q", "R", "S", "T", "U", "V", "W", "X", "Y", "Z"]
for x in ans:
    input_string = ' '.join(x)
    split_strings = re.findall(pattern, input_string, re.DOTALL)
#     print(split_strings)
    for string in split_strings:
#         string = string[:-1]
        cnt = 0
        for i in range(len(string)):
            if string[i]=='\n':
                string = string[:i] + f'\n{prefixes[cnt]}. ' + string[i+1:]
                cnt += 1
                
        print(string.strip())
# Use regex to split the input string based on the pattern
# split_strings = re.findall(pattern, input_string, re.DOTALL)

# Print the split strings
# a = ""
# for string in split_strings:
#     print(string.strip())
#     break
116/80:
import re

# Input string
input_string = 'Гигиена пәніне кіріспе:\n\n1. Қазақстандағы тағам  гигиенасын  негізін қалаушы:\nШарманов Т.Ш.\nАмрин К.Р.;\nСоловьев З.П.\nЧарльз Дарвин\nНеменко Б.А.\n2. Гигиенаның түрлері:\nеңбек гигиенасы;\nэнтомология;\nпаразитология;\nжануарлардың мінез-құлық ғылымы\nэкология.\n3. Биологиялық фактор:\nмикроорганизмдер;\nылғалдылық;\nөсімдіктер, жәндіктер;\nкүн сәулесі.\n4. Гигиена бойынша зерттеу әдістері:\nсанитарлық сипаттама;\nбиохимиялық;\nультрадыбыстық;\nрадиографиялық.'

# Define the pattern for splitting the input string
pattern = r'\d+\..+?(?=\d+\.)'
prefixes = ["A", "B", "C", "D", "E", "F", "G", "H", "I", "J", "K", "L", "M", "N", "O", "P", "Q", "R", "S", "T", "U", "V", "W", "X", "Y", "Z"]
for x in ans:
    input_string = ' '.join(x)
    split_strings = re.findall(pattern, input_string, re.DOTALL)
    print(split_strings)
#     for string in split_strings:
# #         string = string[:-1]
#         cnt = 0
#         for i in range(len(string)):
#             if string[i]=='\n':
#                 string = string[:i] + f'\n{prefixes[cnt]}. ' + string[i+1:]
#                 cnt += 1
                
#         print(string.strip())
# Use regex to split the input string based on the pattern
# split_strings = re.findall(pattern, input_string, re.DOTALL)

# Print the split strings
# a = ""
# for string in split_strings:
#     print(string.strip())
#     break
116/81:
import re

# Input string
input_string = 'Гигиена пәніне кіріспе:\n\n1. Қазақстандағы тағам  гигиенасын  негізін қалаушы:\nШарманов Т.Ш.\nАмрин К.Р.;\nСоловьев З.П.\nЧарльз Дарвин\nНеменко Б.А.\n2. Гигиенаның түрлері:\nеңбек гигиенасы;\nэнтомология;\nпаразитология;\nжануарлардың мінез-құлық ғылымы\nэкология.\n3. Биологиялық фактор:\nмикроорганизмдер;\nылғалдылық;\nөсімдіктер, жәндіктер;\nкүн сәулесі.\n4. Гигиена бойынша зерттеу әдістері:\nсанитарлық сипаттама;\nбиохимиялық;\nультрадыбыстық;\nрадиографиялық.'

# Define the pattern for splitting the input string
pattern = r'\d+\..+?(?=\d+\.)'
prefixes = ["A", "B", "C", "D", "E", "F", "G", "H", "I", "J", "K", "L", "M", "N", "O", "P", "Q", "R", "S", "T", "U", "V", "W", "X", "Y", "Z"]
for x in ans:
    input_string = ' '.join(x)
    split_strings = re.findall(pattern, input_string, re.DOTALL)
#     print(split_strings)
    print(x)
    for string in split_strings:
#         string = string[:-1]
        cnt = 0
        for i in range(len(string)):
            if string[i]=='\n':
                string = string[:i] + f'\n{prefixes[cnt]}. ' + string[i+1:]
                cnt += 1
                
        print(string.strip())
# Use regex to split the input string based on the pattern
# split_strings = re.findall(pattern, input_string, re.DOTALL)

# Print the split strings
# a = ""
# for string in split_strings:
#     print(string.strip())
#     break
116/82:
import re

# Input string
input_string = 'Гигиена пәніне кіріспе:\n\n1. Қазақстандағы тағам  гигиенасын  негізін қалаушы:\nШарманов Т.Ш.\nАмрин К.Р.;\nСоловьев З.П.\nЧарльз Дарвин\nНеменко Б.А.\n2. Гигиенаның түрлері:\nеңбек гигиенасы;\nэнтомология;\nпаразитология;\nжануарлардың мінез-құлық ғылымы\nэкология.\n3. Биологиялық фактор:\nмикроорганизмдер;\nылғалдылық;\nөсімдіктер, жәндіктер;\nкүн сәулесі.\n4. Гигиена бойынша зерттеу әдістері:\nсанитарлық сипаттама;\nбиохимиялық;\nультрадыбыстық;\nрадиографиялық.'

# Define the pattern for splitting the input string
pattern = r'\d+\..+?(?=\d+\.)'
prefixes = ["A", "B", "C", "D", "E", "F", "G", "H", "I", "J", "K", "L", "M", "N", "O", "P", "Q", "R", "S", "T", "U", "V", "W", "X", "Y", "Z"]
for x in ans:
    input_string = ''.join(x)
    split_strings = re.findall(pattern, input_string, re.DOTALL)
#     print(split_strings)
#     print(x)
    for string in split_strings:
#         string = string[:-1]
        cnt = 0
        for i in range(len(string)):
            if string[i]=='\n':
                string = string[:i] + f'\n{prefixes[cnt]}. ' + string[i+1:]
                cnt += 1
                
        print(string.strip())
# Use regex to split the input string based on the pattern
# split_strings = re.findall(pattern, input_string, re.DOTALL)

# Print the split strings
# a = ""
# for string in split_strings:
#     print(string.strip())
#     break
116/83:
import re

# Input string
input_string = 'Гигиена пәніне кіріспе:\n\n1. Қазақстандағы тағам  гигиенасын  негізін қалаушы:\nШарманов Т.Ш.\nАмрин К.Р.;\nСоловьев З.П.\nЧарльз Дарвин\nНеменко Б.А.\n2. Гигиенаның түрлері:\nеңбек гигиенасы;\nэнтомология;\nпаразитология;\nжануарлардың мінез-құлық ғылымы\nэкология.\n3. Биологиялық фактор:\nмикроорганизмдер;\nылғалдылық;\nөсімдіктер, жәндіктер;\nкүн сәулесі.\n4. Гигиена бойынша зерттеу әдістері:\nсанитарлық сипаттама;\nбиохимиялық;\nультрадыбыстық;\nрадиографиялық.'

# Define the pattern for splitting the input string
pattern = r'\d+\..+?(?=\d+\.)'
prefixes = ["A", "B", "C", "D", "E", "F", "G", "H", "I", "J", "K", "L", "M", "N", "O", "P", "Q", "R", "S", "T", "U", "V", "W", "X", "Y", "Z"]
for x in ans:
    input_string = ' '.join(x)
    split_strings = re.findall(pattern, input_string, re.DOTALL)
    print(split_strings)
    
#     for string in split_strings:
# #         string = string[:-1]
#         cnt = 0
#         for i in range(len(string)):
#             if string[i]=='\n':
#                 string = string[:i] + f'\n{prefixes[cnt]}. ' + string[i+1:]
#                 cnt += 1
                
#         print(string.strip())
# Use regex to split the input string based on the pattern
# split_strings = re.findall(pattern, input_string, re.DOTALL)

# Print the split strings
# a = ""
# for string in split_strings:
#     print(string.strip())
#     break
116/84:
import re

# Input string
input_string = 'Гигиена пәніне кіріспе:\n\n1. Қазақстандағы тағам  гигиенасын  негізін қалаушы:\nШарманов Т.Ш.\nАмрин К.Р.;\nСоловьев З.П.\nЧарльз Дарвин\nНеменко Б.А.\n2. Гигиенаның түрлері:\nеңбек гигиенасы;\nэнтомология;\nпаразитология;\nжануарлардың мінез-құлық ғылымы\nэкология.\n3. Биологиялық фактор:\nмикроорганизмдер;\nылғалдылық;\nөсімдіктер, жәндіктер;\nкүн сәулесі.\n4. Гигиена бойынша зерттеу әдістері:\nсанитарлық сипаттама;\nбиохимиялық;\nультрадыбыстық;\nрадиографиялық.'

# Define the pattern for splitting the input string
pattern = r'\d+\..+?(?=\d+\.)'
prefixes = ["A", "B", "C", "D", "E", "F", "G", "H", "I", "J", "K", "L", "M", "N", "O", "P", "Q", "R", "S", "T", "U", "V", "W", "X", "Y", "Z"]
for x in ans:
    input_string = ' '.join(x)
    split_strings = re.findall(pattern, input_string, re.DOTALL)
#     print(split_strings)
    
    for string in split_strings:
#         string = string[:-1]
        cnt = 0
#         for i in range(len(string)):
#             if string[i]=='\n':
#                 string = string[:i] + f'\n{prefixes[cnt]}. ' + string[i+1:]
#                 cnt += 1
                
        print(string.strip())
# Use regex to split the input string based on the pattern
# split_strings = re.findall(pattern, input_string, re.DOTALL)

# Print the split strings
# a = ""
# for string in split_strings:
#     print(string.strip())
#     break
116/85:
import re

# Input string
input_string = 'Гигиена пәніне кіріспе:\n\n1. Қазақстандағы тағам  гигиенасын  негізін қалаушы:\nШарманов Т.Ш.\nАмрин К.Р.;\nСоловьев З.П.\nЧарльз Дарвин\nНеменко Б.А.\n2. Гигиенаның түрлері:\nеңбек гигиенасы;\nэнтомология;\nпаразитология;\nжануарлардың мінез-құлық ғылымы\nэкология.\n3. Биологиялық фактор:\nмикроорганизмдер;\nылғалдылық;\nөсімдіктер, жәндіктер;\nкүн сәулесі.\n4. Гигиена бойынша зерттеу әдістері:\nсанитарлық сипаттама;\nбиохимиялық;\nультрадыбыстық;\nрадиографиялық.'

# Define the pattern for splitting the input string
pattern = r'\d+\..+?(?=\d+\.)'
prefixes = ["A", "B", "C", "D", "E", "F", "G", "H", "I", "J", "K", "L", "M", "N", "O", "P", "Q", "R", "S", "T", "U", "V", "W", "X", "Y", "Z"]
for x in ans:
    input_string = ' '.join(x)
    split_strings = re.findall(pattern, input_string, re.DOTALL)
#     print(split_strings)
    
    for string in split_strings:
        print(string)
        cnt = 0
#         for i in range(len(string)):
#             if string[i]=='\n':
#                 string = string[:i] + f'\n{prefixes[cnt]}. ' + string[i+1:]
#                 cnt += 1
                
        print(string.strip())
# Use regex to split the input string based on the pattern
# split_strings = re.findall(pattern, input_string, re.DOTALL)

# Print the split strings
# a = ""
# for string in split_strings:
#     print(string.strip())
#     break
116/86:
import re

# Input string
input_string = 'Гигиена пәніне кіріспе:\n\n1. Қазақстандағы тағам  гигиенасын  негізін қалаушы:\nШарманов Т.Ш.\nАмрин К.Р.;\nСоловьев З.П.\nЧарльз Дарвин\nНеменко Б.А.\n2. Гигиенаның түрлері:\nеңбек гигиенасы;\nэнтомология;\nпаразитология;\nжануарлардың мінез-құлық ғылымы\nэкология.\n3. Биологиялық фактор:\nмикроорганизмдер;\nылғалдылық;\nөсімдіктер, жәндіктер;\nкүн сәулесі.\n4. Гигиена бойынша зерттеу әдістері:\nсанитарлық сипаттама;\nбиохимиялық;\nультрадыбыстық;\nрадиографиялық.'

# Define the pattern for splitting the input string
pattern = r'\d+\..+?(?=\d+\.)'
prefixes = ["A", "B", "C", "D", "E", "F", "G", "H", "I", "J", "K", "L", "M", "N", "O", "P", "Q", "R", "S", "T", "U", "V", "W", "X", "Y", "Z"]
for x in ans:
    input_string = ' '.join(x)
    split_strings = re.findall(pattern, input_string, re.DOTALL)
#     print(split_strings)
    
    for string in split_strings:
        print(string)
        cnt = 0
#         for i in range(len(string)):
#             if string[i]=='\n':
#                 string = string[:i] + f'\n{prefixes[cnt]}. ' + string[i+1:]
#                 cnt += 1
                
#         print(string.strip())
# Use regex to split the input string based on the pattern
# split_strings = re.findall(pattern, input_string, re.DOTALL)

# Print the split strings
# a = ""
# for string in split_strings:
#     print(string.strip())
#     break
116/87:
import re

# Input string
input_string = 'Гигиена пәніне кіріспе:\n\n1. Қазақстандағы тағам  гигиенасын  негізін қалаушы:\nШарманов Т.Ш.\nАмрин К.Р.;\nСоловьев З.П.\nЧарльз Дарвин\nНеменко Б.А.\n2. Гигиенаның түрлері:\nеңбек гигиенасы;\nэнтомология;\nпаразитология;\nжануарлардың мінез-құлық ғылымы\nэкология.\n3. Биологиялық фактор:\nмикроорганизмдер;\nылғалдылық;\nөсімдіктер, жәндіктер;\nкүн сәулесі.\n4. Гигиена бойынша зерттеу әдістері:\nсанитарлық сипаттама;\nбиохимиялық;\nультрадыбыстық;\nрадиографиялық.'

# Define the pattern for splitting the input string
pattern = r'\d+\..+?(?=\d+\.)'
prefixes = ["A", "B", "C", "D", "E", "F", "G", "H", "I", "J", "K", "L", "M", "N", "O", "P", "Q", "R", "S", "T", "U", "V", "W", "X", "Y", "Z"]
for x in ans:
    input_string = ' '.join(x)
    split_strings = re.findall(pattern, input_string, re.DOTALL)
#     print(split_strings)
    
    for string in split_strings:
#         print(string)
        cnt = 0
        for i in range(len(string)):
            if string[i]=='\n':
                string = string[:i] + f'\n{prefixes[cnt]}. ' + string[i+1:]
                cnt += 1
                
        print(string)
# Use regex to split the input string based on the pattern
# split_strings = re.findall(pattern, input_string, re.DOTALL)

# Print the split strings
# a = ""
# for string in split_strings:
#     print(string.strip())
#     break
116/88:
import re

# Input string
input_string = 'Гигиена пәніне кіріспе:\n\n1. Қазақстандағы тағам  гигиенасын  негізін қалаушы:\nШарманов Т.Ш.\nАмрин К.Р.;\nСоловьев З.П.\nЧарльз Дарвин\nНеменко Б.А.\n2. Гигиенаның түрлері:\nеңбек гигиенасы;\nэнтомология;\nпаразитология;\nжануарлардың мінез-құлық ғылымы\nэкология.\n3. Биологиялық фактор:\nмикроорганизмдер;\nылғалдылық;\nөсімдіктер, жәндіктер;\nкүн сәулесі.\n4. Гигиена бойынша зерттеу әдістері:\nсанитарлық сипаттама;\nбиохимиялық;\nультрадыбыстық;\nрадиографиялық.'

# Define the pattern for splitting the input string
pattern = r'\d+\..+?(?=\d+\.)'
prefixes = ["A", "B", "C", "D", "E", "F", "G", "H", "I", "J", "K", "L", "M", "N", "O", "P", "Q", "R", "S", "T", "U", "V", "W", "X", "Y", "Z"]
for x in ans:
    input_string = ' '.join(x)
    split_strings = re.findall(pattern, input_string, re.DOTALL)
#     print(split_strings)
    
    for string in split_strings:
        print(string)
        cnt = 0
        for i in range(len(string)):
            
            if string[i]=='\n':
                string = string[:i] + f'\n{prefixes[cnt]}. ' + string[i+1:]
                cnt += 1
                
#         print(string)
# Use regex to split the input string based on the pattern
# split_strings = re.findall(pattern, input_string, re.DOTALL)

# Print the split strings
# a = ""
# for string in split_strings:
#     print(string.strip())
#     break
116/89:
import re

# Input string
input_string = 'Гигиена пәніне кіріспе:\n\n1. Қазақстандағы тағам  гигиенасын  негізін қалаушы:\nШарманов Т.Ш.\nАмрин К.Р.;\nСоловьев З.П.\nЧарльз Дарвин\nНеменко Б.А.\n2. Гигиенаның түрлері:\nеңбек гигиенасы;\nэнтомология;\nпаразитология;\nжануарлардың мінез-құлық ғылымы\nэкология.\n3. Биологиялық фактор:\nмикроорганизмдер;\nылғалдылық;\nөсімдіктер, жәндіктер;\nкүн сәулесі.\n4. Гигиена бойынша зерттеу әдістері:\nсанитарлық сипаттама;\nбиохимиялық;\nультрадыбыстық;\nрадиографиялық.'

# Define the pattern for splitting the input string
pattern = r'\d+\..+?(?=\d+\.)'
prefixes = ["A", "B", "C", "D", "E", "F", "G", "H", "I", "J", "K", "L", "M", "N", "O", "P", "Q", "R", "S", "T", "U", "V", "W", "X", "Y", "Z"]
for x in ans:
    input_string = ' '.join(x)
    split_strings = re.findall(pattern, input_string, re.DOTALL)
#     print(split_strings)
    
    for string in split_strings:
        print(string)
        cnt = 0
        for i in range(len(string)):
            
            if string[i]=='\n':
                string = string[:i] + f'\n{prefixes[cnt]}. ' + string[i+1:]
                cnt += 1
                
#         print(string)
# Use regex to split the input string based on the pattern
# split_strings = re.findall(pattern, input_string, re.DOTALL)

# Print the split strings
# a = ""
# for string in split_strings:
#     print(string.strip())
#     break
116/90:
import re

# Input string
input_string = 'Гигиена пәніне кіріспе:\n\n1. Қазақстандағы тағам  гигиенасын  негізін қалаушы:\nШарманов Т.Ш.\nАмрин К.Р.;\nСоловьев З.П.\nЧарльз Дарвин\nНеменко Б.А.\n2. Гигиенаның түрлері:\nеңбек гигиенасы;\nэнтомология;\nпаразитология;\nжануарлардың мінез-құлық ғылымы\nэкология.\n3. Биологиялық фактор:\nмикроорганизмдер;\nылғалдылық;\nөсімдіктер, жәндіктер;\nкүн сәулесі.\n4. Гигиена бойынша зерттеу әдістері:\nсанитарлық сипаттама;\nбиохимиялық;\nультрадыбыстық;\nрадиографиялық.'

# Define the pattern for splitting the input string
pattern = r'\d+\..+?(?=\d+\.)'
prefixes = ["A", "B", "C", "D", "E", "F", "G", "H", "I", "J", "K", "L", "M", "N", "O", "P", "Q", "R", "S", "T", "U", "V", "W", "X", "Y", "Z"]
for x in ans:
    input_string = ' '.join(x)
    split_strings = re.findall(pattern, input_string, re.DOTALL)
#     print(split_strings)
    
    for string in split_strings:
        print(string)
        cnt = 0
        for i in range(len(string)):
            
            if string[i]=='\n':
                string = string[:i] + f'\n{prefixes[cnt]}. ' + string[i+1:]
                cnt += 1
                
        print(string)
# Use regex to split the input string based on the pattern
# split_strings = re.findall(pattern, input_string, re.DOTALL)

# Print the split strings
# a = ""
# for string in split_strings:
#     print(string.strip())
#     break
116/91:
import re

# Input string
input_string = 'Гигиена пәніне кіріспе:\n\n1. Қазақстандағы тағам  гигиенасын  негізін қалаушы:\nШарманов Т.Ш.\nАмрин К.Р.;\nСоловьев З.П.\nЧарльз Дарвин\nНеменко Б.А.\n2. Гигиенаның түрлері:\nеңбек гигиенасы;\nэнтомология;\nпаразитология;\nжануарлардың мінез-құлық ғылымы\nэкология.\n3. Биологиялық фактор:\nмикроорганизмдер;\nылғалдылық;\nөсімдіктер, жәндіктер;\nкүн сәулесі.\n4. Гигиена бойынша зерттеу әдістері:\nсанитарлық сипаттама;\nбиохимиялық;\nультрадыбыстық;\nрадиографиялық.'

# Define the pattern for splitting the input string
pattern = r'\d+\..+?(?=\d+\.)'
prefixes = ["A", "B", "C", "D", "E", "F", "G", "H", "I", "J", "K", "L", "M", "N", "O", "P", "Q", "R", "S", "T", "U", "V", "W", "X", "Y", "Z"]
for x in ans:
    input_string = ' '.join(x)
    split_strings = re.findall(pattern, input_string, re.DOTALL)
    print(split_strings)
    
#     for string in split_strings:
#         print(string)
#         cnt = 0
#         for i in range(len(string)):
#             if string[i]=='\n':
#                 string = string[:i] + f'\n{prefixes[cnt]}. ' + string[i+1:]
#                 cnt += 1
                
#         print(string)
# Use regex to split the input string based on the pattern
# split_strings = re.findall(pattern, input_string, re.DOTALL)

# Print the split strings
# a = ""
# for string in split_strings:
#     print(string.strip())
#     break
116/92:
s = "Hello\nWorld"

# Check if the character at index 5 is a newline
if s[5] == '\n':
    print("The character at index 5 is a newline.")
else:
    print("The character at index 5 is not a newline.")
116/93:
s = "Hello\nWorld"

# Check if the character at index 5 is a newline
if s[6] == '\n':
    print("The character at index 5 is a newline.")
else:
    print("The character at index 5 is not a newline.")
116/94:
s = "Hello\nWorld"

# Check if the character at index 5 is a newline
if s[5] == '\n':
    print("The character at index 5 is a newline.")
else:
    print("The character at index 5 is not a newline.")
116/95:
import re

# Input string
input_string = 'Гигиена пәніне кіріспе:\n\n1. Қазақстандағы тағам  гигиенасын  негізін қалаушы:\nШарманов Т.Ш.\nАмрин К.Р.;\nСоловьев З.П.\nЧарльз Дарвин\nНеменко Б.А.\n2. Гигиенаның түрлері:\nеңбек гигиенасы;\nэнтомология;\nпаразитология;\nжануарлардың мінез-құлық ғылымы\nэкология.\n3. Биологиялық фактор:\nмикроорганизмдер;\nылғалдылық;\nөсімдіктер, жәндіктер;\nкүн сәулесі.\n4. Гигиена бойынша зерттеу әдістері:\nсанитарлық сипаттама;\nбиохимиялық;\nультрадыбыстық;\nрадиографиялық.'

# Define the pattern for splitting the input string
pattern = r'\d+\..+?(?=\d+\.)'
prefixes = ["A", "B", "C", "D", "E", "F", "G", "H", "I", "J", "K", "L", "M", "N", "O", "P", "Q", "R", "S", "T", "U", "V", "W", "X", "Y", "Z"]
for x in ans:
    input_string = ' '.join(x)
    split_strings = re.findall(pattern, input_string, re.DOTALL)
    print(split_strings)
    
    for string in split_strings:
        print(string)
        cnt = 0
        for i in range(len(string)):
            if string[i]=='\n':
                string = string[:i] + f'\n{prefixes[cnt]}. ' + string[i+1:]
                cnt += 1
                
#         print(string)
# Use regex to split the input string based on the pattern
# split_strings = re.findall(pattern, input_string, re.DOTALL)

# Print the split strings
# a = ""
# for string in split_strings:
#     print(string.strip())
#     break
116/96:
import re

# Input string
input_string = 'Гигиена пәніне кіріспе:\n\n1. Қазақстандағы тағам  гигиенасын  негізін қалаушы:\nШарманов Т.Ш.\nАмрин К.Р.;\nСоловьев З.П.\nЧарльз Дарвин\nНеменко Б.А.\n2. Гигиенаның түрлері:\nеңбек гигиенасы;\nэнтомология;\nпаразитология;\nжануарлардың мінез-құлық ғылымы\nэкология.\n3. Биологиялық фактор:\nмикроорганизмдер;\nылғалдылық;\nөсімдіктер, жәндіктер;\nкүн сәулесі.\n4. Гигиена бойынша зерттеу әдістері:\nсанитарлық сипаттама;\nбиохимиялық;\nультрадыбыстық;\nрадиографиялық.'

# Define the pattern for splitting the input string
pattern = r'\d+\..+?(?=\d+\.)'
prefixes = ["A", "B", "C", "D", "E", "F", "G", "H", "I", "J", "K", "L", "M", "N", "O", "P", "Q", "R", "S", "T", "U", "V", "W", "X", "Y", "Z"]
for x in ans:
    input_string = ' '.join(x)
    split_strings = re.findall(pattern, input_string, re.DOTALL)
    print(split_strings)
    
#     for string in split_strings:
#         print(string)
#         cnt = 0
#         for i in range(len(string)):
#             if string[i]=='\n':
#                 string = string[:i] + f'\n{prefixes[cnt]}. ' + string[i+1:]
#                 cnt += 1
                
#         print(string)
# Use regex to split the input string based on the pattern
# split_strings = re.findall(pattern, input_string, re.DOTALL)

# Print the split strings
# a = ""
# for string in split_strings:
#     print(string.strip())
#     break
116/97:
import re

# Input string
input_string = 'Гигиена пәніне кіріспе:\n\n1. Қазақстандағы тағам  гигиенасын  негізін қалаушы:\nШарманов Т.Ш.\nАмрин К.Р.;\nСоловьев З.П.\nЧарльз Дарвин\nНеменко Б.А.\n2. Гигиенаның түрлері:\nеңбек гигиенасы;\nэнтомология;\nпаразитология;\nжануарлардың мінез-құлық ғылымы\nэкология.\n3. Биологиялық фактор:\nмикроорганизмдер;\nылғалдылық;\nөсімдіктер, жәндіктер;\nкүн сәулесі.\n4. Гигиена бойынша зерттеу әдістері:\nсанитарлық сипаттама;\nбиохимиялық;\nультрадыбыстық;\nрадиографиялық.'

# Define the pattern for splitting the input string
pattern = r'\d+\..+?(?=\d+\.)'
prefixes = ["A", "B", "C", "D", "E", "F", "G", "H", "I", "J", "K", "L", "M", "N", "O", "P", "Q", "R", "S", "T", "U", "V", "W", "X", "Y", "Z"]
for x in ans:
    input_string = ' '.join(x)
    split_strings = re.findall(pattern, input_string, re.DOTALL)
    print(split_strings)
    
    for string in split_strings:
        print(string)
    break
#         cnt = 0
#         for i in range(len(string)):
#             if string[i]=='\n':
#                 string = string[:i] + f'\n{prefixes[cnt]}. ' + string[i+1:]
#                 cnt += 1
                
#         print(string)
# Use regex to split the input string based on the pattern
# split_strings = re.findall(pattern, input_string, re.DOTALL)

# Print the split strings
# a = ""
# for string in split_strings:
#     print(string.strip())
#     break
116/98:
import re

# Input string
input_string = 'Гигиена пәніне кіріспе:\n\n1. Қазақстандағы тағам  гигиенасын  негізін қалаушы:\nШарманов Т.Ш.\nАмрин К.Р.;\nСоловьев З.П.\nЧарльз Дарвин\nНеменко Б.А.\n2. Гигиенаның түрлері:\nеңбек гигиенасы;\nэнтомология;\nпаразитология;\nжануарлардың мінез-құлық ғылымы\nэкология.\n3. Биологиялық фактор:\nмикроорганизмдер;\nылғалдылық;\nөсімдіктер, жәндіктер;\nкүн сәулесі.\n4. Гигиена бойынша зерттеу әдістері:\nсанитарлық сипаттама;\nбиохимиялық;\nультрадыбыстық;\nрадиографиялық.'

# Define the pattern for splitting the input string
pattern = r'\d+\..+?(?=\d+\.)'
prefixes = ["A", "B", "C", "D", "E", "F", "G", "H", "I", "J", "K", "L", "M", "N", "O", "P", "Q", "R", "S", "T", "U", "V", "W", "X", "Y", "Z"]
for x in ans:
    input_string = ' '.join(x)
    split_strings = re.findall(pattern, input_string, re.DOTALL)
#     print(split_strings)
    
    for string in split_strings:
        print(string)
    break
#         cnt = 0
#         for i in range(len(string)):
#             if string[i]=='\n':
#                 string = string[:i] + f'\n{prefixes[cnt]}. ' + string[i+1:]
#                 cnt += 1
                
#         print(string)
# Use regex to split the input string based on the pattern
# split_strings = re.findall(pattern, input_string, re.DOTALL)

# Print the split strings
# a = ""
# for string in split_strings:
#     print(string.strip())
#     break
116/99:
import re

# Input string
input_string = 'Гигиена пәніне кіріспе:\n\n1. Қазақстандағы тағам  гигиенасын  негізін қалаушы:\nШарманов Т.Ш.\nАмрин К.Р.;\nСоловьев З.П.\nЧарльз Дарвин\nНеменко Б.А.\n2. Гигиенаның түрлері:\nеңбек гигиенасы;\nэнтомология;\nпаразитология;\nжануарлардың мінез-құлық ғылымы\nэкология.\n3. Биологиялық фактор:\nмикроорганизмдер;\nылғалдылық;\nөсімдіктер, жәндіктер;\nкүн сәулесі.\n4. Гигиена бойынша зерттеу әдістері:\nсанитарлық сипаттама;\nбиохимиялық;\nультрадыбыстық;\nрадиографиялық.'

# Define the pattern for splitting the input string
pattern = r'\d+\..+?(?=\d+\.)'
prefixes = ["A", "B", "C", "D", "E", "F", "G", "H", "I", "J", "K", "L", "M", "N", "O", "P", "Q", "R", "S", "T", "U", "V", "W", "X", "Y", "Z"]
for x in ans:
    input_string = ' '.join(x)
    split_strings = re.findall(pattern, input_string, re.DOTALL)
#     print(split_strings)
    
    for string in split_strings:
        print(string)
        break
    break
#         cnt = 0
#         for i in range(len(string)):
#             if string[i]=='\n':
#                 string = string[:i] + f'\n{prefixes[cnt]}. ' + string[i+1:]
#                 cnt += 1
                
#         print(string)
# Use regex to split the input string based on the pattern
# split_strings = re.findall(pattern, input_string, re.DOTALL)

# Print the split strings
# a = ""
# for string in split_strings:
#     print(string.strip())
#     break
116/100:
import re

# Input string
input_string = 'Гигиена пәніне кіріспе:\n\n1. Қазақстандағы тағам  гигиенасын  негізін қалаушы:\nШарманов Т.Ш.\nАмрин К.Р.;\nСоловьев З.П.\nЧарльз Дарвин\nНеменко Б.А.\n2. Гигиенаның түрлері:\nеңбек гигиенасы;\nэнтомология;\nпаразитология;\nжануарлардың мінез-құлық ғылымы\nэкология.\n3. Биологиялық фактор:\nмикроорганизмдер;\nылғалдылық;\nөсімдіктер, жәндіктер;\nкүн сәулесі.\n4. Гигиена бойынша зерттеу әдістері:\nсанитарлық сипаттама;\nбиохимиялық;\nультрадыбыстық;\nрадиографиялық.'

# Define the pattern for splitting the input string
pattern = r'\d+\..+?(?=\d+\.)'
prefixes = ["A", "B", "C", "D", "E", "F", "G", "H", "I", "J", "K", "L", "M", "N", "O", "P", "Q", "R", "S", "T", "U", "V", "W", "X", "Y", "Z"]
for x in ans:
    input_string = ' '.join(x)
    split_strings = re.findall(pattern, input_string, re.DOTALL)
#     print(split_strings)
    
    for string in split_strings:
        print(string)
        s = ''.join(string)
        print(s)
        break
    break
#         cnt = 0
#         for i in range(len(string)):
#             if string[i]=='\n':
#                 string = string[:i] + f'\n{prefixes[cnt]}. ' + string[i+1:]
#                 cnt += 1
                
#         print(string)
# Use regex to split the input string based on the pattern
# split_strings = re.findall(pattern, input_string, re.DOTALL)

# Print the split strings
# a = ""
# for string in split_strings:
#     print(string.strip())
#     break
116/101:
import re

# Input string
input_string = 'Гигиена пәніне кіріспе:\n\n1. Қазақстандағы тағам  гигиенасын  негізін қалаушы:\nШарманов Т.Ш.\nАмрин К.Р.;\nСоловьев З.П.\nЧарльз Дарвин\nНеменко Б.А.\n2. Гигиенаның түрлері:\nеңбек гигиенасы;\nэнтомология;\nпаразитология;\nжануарлардың мінез-құлық ғылымы\nэкология.\n3. Биологиялық фактор:\nмикроорганизмдер;\nылғалдылық;\nөсімдіктер, жәндіктер;\nкүн сәулесі.\n4. Гигиена бойынша зерттеу әдістері:\nсанитарлық сипаттама;\nбиохимиялық;\nультрадыбыстық;\nрадиографиялық.'

# Define the pattern for splitting the input string
pattern = r'\d+\..+?(?=\d+\.)'
prefixes = ["A", "B", "C", "D", "E", "F", "G", "H", "I", "J", "K", "L", "M", "N", "O", "P", "Q", "R", "S", "T", "U", "V", "W", "X", "Y", "Z"]
for x in ans:
    input_string = ' '.join(x)
    split_strings = re.findall(pattern, input_string, re.DOTALL)
#     print(split_strings)
    
    for string in split_strings:
        print(string)
        s = ''.join(string)
        string = s
#         print(s)
#         break
#     break
        cnt = 0
        for i in range(len(string)):
            if string[i]=='\n':
                string = string[:i] + f'\n{prefixes[cnt]}. ' + string[i+1:]
                cnt += 1
                
        print(string)
# Use regex to split the input string based on the pattern
# split_strings = re.findall(pattern, input_string, re.DOTALL)

# Print the split strings
# a = ""
# for string in split_strings:
#     print(string.strip())
#     break
116/102:
import re

# Input string
input_string = 'Гигиена пәніне кіріспе:\n\n1. Қазақстандағы тағам  гигиенасын  негізін қалаушы:\nШарманов Т.Ш.\nАмрин К.Р.;\nСоловьев З.П.\nЧарльз Дарвин\nНеменко Б.А.\n2. Гигиенаның түрлері:\nеңбек гигиенасы;\nэнтомология;\nпаразитология;\nжануарлардың мінез-құлық ғылымы\nэкология.\n3. Биологиялық фактор:\nмикроорганизмдер;\nылғалдылық;\nөсімдіктер, жәндіктер;\nкүн сәулесі.\n4. Гигиена бойынша зерттеу әдістері:\nсанитарлық сипаттама;\nбиохимиялық;\nультрадыбыстық;\nрадиографиялық.'

# Define the pattern for splitting the input string
pattern = r'\d+\..+?(?=\d+\.)'
prefixes = ["A", "B", "C", "D", "E", "F", "G", "H", "I", "J", "K", "L", "M", "N", "O", "P", "Q", "R", "S", "T", "U", "V", "W", "X", "Y", "Z"]
for x in ans:
    input_string = ' '.join(x)
    split_strings = re.findall(pattern, input_string, re.DOTALL)
#     print(split_strings)
    
    for string in split_strings:
#         print(string)
        s = ''.join(string)
        string = s
#         print(s)
#         break
#     break
        cnt = 0
        for i in range(len(string)):
            if string[i]=='\n':
                string = string[:i] + f'\n{prefixes[cnt]}. ' + string[i+1:]
                cnt += 1
                
        print(string)
# Use regex to split the input string based on the pattern
# split_strings = re.findall(pattern, input_string, re.DOTALL)

# Print the split strings
# a = ""
# for string in split_strings:
#     print(string.strip())
#     break
116/103:
import re

# Input string
input_string = 'Гигиена пәніне кіріспе:\n\n1. Қазақстандағы тағам  гигиенасын  негізін қалаушы:\nШарманов Т.Ш.\nАмрин К.Р.;\nСоловьев З.П.\nЧарльз Дарвин\nНеменко Б.А.\n2. Гигиенаның түрлері:\nеңбек гигиенасы;\nэнтомология;\nпаразитология;\nжануарлардың мінез-құлық ғылымы\nэкология.\n3. Биологиялық фактор:\nмикроорганизмдер;\nылғалдылық;\nөсімдіктер, жәндіктер;\nкүн сәулесі.\n4. Гигиена бойынша зерттеу әдістері:\nсанитарлық сипаттама;\nбиохимиялық;\nультрадыбыстық;\nрадиографиялық.'

# Define the pattern for splitting the input string
pattern = r'\d+\..+?(?=\d+\.)'
prefixes = ["A", "B", "C", "D", "E", "F", "G", "H", "I", "J", "K", "L", "M", "N", "O", "P", "Q", "R", "S", "T", "U", "V", "W", "X", "Y", "Z"]
for x in ans:
    input_string = ' '.join(x)
    split_strings = re.findall(pattern, input_string, re.DOTALL)
    
    for string in split_strings:
        s = ''.join(string)
        string = s
        cnt = 0
        print(s)
        break
        for i in range(len(string)):
            if string[i]=='\n':
                string = string[:i] + f'\n{prefixes[cnt]}. ' + string[i+1:]
                cnt += 1
                
        print(string)
# Use regex to split the input string based on the pattern
# split_strings = re.findall(pattern, input_string, re.DOTALL)

# Print the split strings
# a = ""
# for string in split_strings:
#     print(string.strip())
#     break
116/104:
import re

# Input string
input_string = 'Гигиена пәніне кіріспе:\n\n1. Қазақстандағы тағам  гигиенасын  негізін қалаушы:\nШарманов Т.Ш.\nАмрин К.Р.;\nСоловьев З.П.\nЧарльз Дарвин\nНеменко Б.А.\n2. Гигиенаның түрлері:\nеңбек гигиенасы;\nэнтомология;\nпаразитология;\nжануарлардың мінез-құлық ғылымы\nэкология.\n3. Биологиялық фактор:\nмикроорганизмдер;\nылғалдылық;\nөсімдіктер, жәндіктер;\nкүн сәулесі.\n4. Гигиена бойынша зерттеу әдістері:\nсанитарлық сипаттама;\nбиохимиялық;\nультрадыбыстық;\nрадиографиялық.'

# Define the pattern for splitting the input string
pattern = r'\d+\..+?(?=\d+\.)'
prefixes = ["A", "B", "C", "D", "E", "F", "G", "H", "I", "J", "K", "L", "M", "N", "O", "P", "Q", "R", "S", "T", "U", "V", "W", "X", "Y", "Z"]
for x in ans:
    input_string = ' '.join(x)
    split_strings = re.findall(pattern, input_string, re.DOTALL)
    print(split_strings)
    
    for string in split_strings:
        s = ''.join(string)
        string = s
        cnt = 0
        for i in range(len(string)):
            if string[i]=='\n':
                string = string[:i] + f'\n{prefixes[cnt]}. ' + string[i+1:]
                cnt += 1
                
        print(string)
# Use regex to split the input string based on the pattern
# split_strings = re.findall(pattern, input_string, re.DOTALL)

# Print the split strings
# a = ""
# for string in split_strings:
#     print(string.strip())
#     break
116/105:
import re

# Input string
input_string = 'Гигиена пәніне кіріспе:\n\n1. Қазақстандағы тағам  гигиенасын  негізін қалаушы:\nШарманов Т.Ш.\nАмрин К.Р.;\nСоловьев З.П.\nЧарльз Дарвин\nНеменко Б.А.\n2. Гигиенаның түрлері:\nеңбек гигиенасы;\nэнтомология;\nпаразитология;\nжануарлардың мінез-құлық ғылымы\nэкология.\n3. Биологиялық фактор:\nмикроорганизмдер;\nылғалдылық;\nөсімдіктер, жәндіктер;\nкүн сәулесі.\n4. Гигиена бойынша зерттеу әдістері:\nсанитарлық сипаттама;\nбиохимиялық;\nультрадыбыстық;\nрадиографиялық.'

# Define the pattern for splitting the input string
pattern = r'\d+\..+?(?=\d+\.)'
prefixes = ["A", "B", "C", "D", "E", "F", "G", "H", "I", "J", "K", "L", "M", "N", "O", "P", "Q", "R", "S", "T", "U", "V", "W", "X", "Y", "Z"]
for x in ans:
    input_string = ' '.join(x)
    split_strings = re.findall(pattern, input_string, re.DOTALL)
    print(split_strings)
    
    for string in split_strings:
        s = ''.join(string)
        string = s
        cnt = 0
        for i in range(len(string)):
            if string[i]=='\n':
                string = string[:i] + f'\n{prefixes[cnt]}. ' + string[i+1:]
                cnt += 1
                
#         print(string)
# Use regex to split the input string based on the pattern
# split_strings = re.findall(pattern, input_string, re.DOTALL)

# Print the split strings
# a = ""
# for string in split_strings:
#     print(string.strip())
#     break
116/106:
import re

# Input string
input_string = 'Гигиена пәніне кіріспе:\n\n1. Қазақстандағы тағам  гигиенасын  негізін қалаушы:\nШарманов Т.Ш.\nАмрин К.Р.;\nСоловьев З.П.\nЧарльз Дарвин\nНеменко Б.А.\n2. Гигиенаның түрлері:\nеңбек гигиенасы;\nэнтомология;\nпаразитология;\nжануарлардың мінез-құлық ғылымы\nэкология.\n3. Биологиялық фактор:\nмикроорганизмдер;\nылғалдылық;\nөсімдіктер, жәндіктер;\nкүн сәулесі.\n4. Гигиена бойынша зерттеу әдістері:\nсанитарлық сипаттама;\nбиохимиялық;\nультрадыбыстық;\nрадиографиялық.'

# Define the pattern for splitting the input string
pattern = r'\d+\..+?(?=\d+\.)'
prefixes = ["A", "B", "C", "D", "E", "F", "G", "H", "I", "J", "K", "L", "M", "N", "O", "P", "Q", "R", "S", "T", "U", "V", "W", "X", "Y", "Z"]
count = 1
for x in ans:
    input_string = ' '.join(x)
    split_strings = re.findall(pattern, input_string, re.DOTALL)
    print(split_strings)
    if count != 3:
        count += 1 
        continue
    for string in split_strings:
        s = ''.join(string)
        string = s
        cnt = 0
        for i in range(len(string)):
            if string[i]=='\n':
                string = string[:i] + f'\n{prefixes[cnt]}. ' + string[i+1:]
                cnt += 1
                
#         print(string)
# Use regex to split the input string based on the pattern
# split_strings = re.findall(pattern, input_string, re.DOTALL)

# Print the split strings
# a = ""
# for string in split_strings:
#     print(string.strip())
#     break
116/107:
import re

# Input string
input_string = 'Гигиена пәніне кіріспе:\n\n1. Қазақстандағы тағам  гигиенасын  негізін қалаушы:\nШарманов Т.Ш.\nАмрин К.Р.;\nСоловьев З.П.\nЧарльз Дарвин\nНеменко Б.А.\n2. Гигиенаның түрлері:\nеңбек гигиенасы;\nэнтомология;\nпаразитология;\nжануарлардың мінез-құлық ғылымы\nэкология.\n3. Биологиялық фактор:\nмикроорганизмдер;\nылғалдылық;\nөсімдіктер, жәндіктер;\nкүн сәулесі.\n4. Гигиена бойынша зерттеу әдістері:\nсанитарлық сипаттама;\nбиохимиялық;\nультрадыбыстық;\nрадиографиялық.'

# Define the pattern for splitting the input string
pattern = r'\d+\..+?(?=\d+\.)'
prefixes = ["A", "B", "C", "D", "E", "F", "G", "H", "I", "J", "K", "L", "M", "N", "O", "P", "Q", "R", "S", "T", "U", "V", "W", "X", "Y", "Z"]
count = 1
for x in ans:
    input_string = ' '.join(x)
    split_strings = re.findall(pattern, input_string, re.DOTALL)
    if count != 3:
        count += 1 
        continue
    print(split_strings)
    for string in split_strings:
        s = ''.join(string)
        string = s
        cnt = 0
        print(s)
        for i in range(len(string)):
            if string[i]=='\n':
                string = string[:i] + f'\n{prefixes[cnt]}. ' + string[i+1:]
                cnt += 1
                
#         print(string)
# Use regex to split the input string based on the pattern
# split_strings = re.findall(pattern, input_string, re.DOTALL)

# Print the split strings
# a = ""
# for string in split_strings:
#     print(string.strip())
#     break
116/108:
import re

# Input string
input_string = 'Гигиена пәніне кіріспе:\n\n1. Қазақстандағы тағам  гигиенасын  негізін қалаушы:\nШарманов Т.Ш.\nАмрин К.Р.;\nСоловьев З.П.\nЧарльз Дарвин\nНеменко Б.А.\n2. Гигиенаның түрлері:\nеңбек гигиенасы;\nэнтомология;\nпаразитология;\nжануарлардың мінез-құлық ғылымы\nэкология.\n3. Биологиялық фактор:\nмикроорганизмдер;\nылғалдылық;\nөсімдіктер, жәндіктер;\nкүн сәулесі.\n4. Гигиена бойынша зерттеу әдістері:\nсанитарлық сипаттама;\nбиохимиялық;\nультрадыбыстық;\nрадиографиялық.'

# Define the pattern for splitting the input string
pattern = r'\d+\..+?(?=\d+\.)'
prefixes = ["A", "B", "C", "D", "E", "F", "G", "H", "I", "J", "K", "L", "M", "N", "O", "P", "Q", "R", "S", "T", "U", "V", "W", "X", "Y", "Z"]
count = 1
for x in ans:
    input_string = ' '.join(x)
    split_strings = re.findall(pattern, input_string, re.DOTALL)
    if count != 3:
        count += 1 
        continue
    print(split_strings)
    for string in split_strings:
        s = ''.join(string)
        string = s
        cnt = 0
        print(s)
        for i in range(len(s)):
            if s[i]=='\n':
                s = s[:i] + f'\n{prefixes[cnt]}. ' + s[i+1:]
                cnt += 1
                
        print(s)
# Use regex to split the input string based on the pattern
# split_strings = re.findall(pattern, input_string, re.DOTALL)

# Print the split strings
# a = ""
# for string in split_strings:
#     print(string.strip())
#     break
116/109:
import re

# Input string
input_string = 'Гигиена пәніне кіріспе:\n\n1. Қазақстандағы тағам  гигиенасын  негізін қалаушы:\nШарманов Т.Ш.\nАмрин К.Р.;\nСоловьев З.П.\nЧарльз Дарвин\nНеменко Б.А.\n2. Гигиенаның түрлері:\nеңбек гигиенасы;\nэнтомология;\nпаразитология;\nжануарлардың мінез-құлық ғылымы\nэкология.\n3. Биологиялық фактор:\nмикроорганизмдер;\nылғалдылық;\nөсімдіктер, жәндіктер;\nкүн сәулесі.\n4. Гигиена бойынша зерттеу әдістері:\nсанитарлық сипаттама;\nбиохимиялық;\nультрадыбыстық;\nрадиографиялық.'

# Define the pattern for splitting the input string
pattern = r'\d+\..+?(?=\d+\.)'
prefixes = ["A", "B", "C", "D", "E", "F", "G", "H", "I", "J", "K", "L", "M", "N", "O", "P", "Q", "R", "S", "T", "U", "V", "W", "X", "Y", "Z"]
count = 1
for x in ans:
    input_string = ' '.join(x)
    split_strings = re.findall(pattern, input_string, re.DOTALL)
    if count != 3:
        count += 1 
        continue
    print(split_strings)
    for string in split_strings:
        s = ''.join(string)
        string = s
        cnt = 0
        print(s)
        for i in range(len(s)):
            print(s[i])
#             if s[i]=='\n':
#                 s = s[:i] + f'\n{prefixes[cnt]}. ' + s[i+1:]
#                 cnt += 1
                
        print(s)
# Use regex to split the input string based on the pattern
# split_strings = re.findall(pattern, input_string, re.DOTALL)

# Print the split strings
# a = ""
# for string in split_strings:
#     print(string.strip())
#     break
116/110:
import re

# Input string
input_string = 'Гигиена пәніне кіріспе:\n\n1. Қазақстандағы тағам  гигиенасын  негізін қалаушы:\nШарманов Т.Ш.\nАмрин К.Р.;\nСоловьев З.П.\nЧарльз Дарвин\nНеменко Б.А.\n2. Гигиенаның түрлері:\nеңбек гигиенасы;\nэнтомология;\nпаразитология;\nжануарлардың мінез-құлық ғылымы\nэкология.\n3. Биологиялық фактор:\nмикроорганизмдер;\nылғалдылық;\nөсімдіктер, жәндіктер;\nкүн сәулесі.\n4. Гигиена бойынша зерттеу әдістері:\nсанитарлық сипаттама;\nбиохимиялық;\nультрадыбыстық;\nрадиографиялық.'

# Define the pattern for splitting the input string
pattern = r'\d+\..+?(?=\d+\.)'
prefixes = ["A", "B", "C", "D", "E", "F", "G", "H", "I", "J", "K", "L", "M", "N", "O", "P", "Q", "R", "S", "T", "U", "V", "W", "X", "Y", "Z"]
count = 1
for x in ans:
    input_string = ' '.join(x)
    split_strings = re.findall(pattern, input_string, re.DOTALL)
    if count != 3:
        count += 1 
        continue
#     print(split_strings)
    for string in split_strings:
        s = ''.join(string)
        string = s
        cnt = 0
#         print(s)
        for i in range(len(s)):
            print(s[i])
#             if s[i]=='\n':
#                 s = s[:i] + f'\n{prefixes[cnt]}. ' + s[i+1:]
#                 cnt += 1
                
        print(s)
# Use regex to split the input string based on the pattern
# split_strings = re.findall(pattern, input_string, re.DOTALL)

# Print the split strings
# a = ""
# for string in split_strings:
#     print(string.strip())
#     break
116/111:
import re

# Input string
input_string = 'Гигиена пәніне кіріспе:\n\n1. Қазақстандағы тағам  гигиенасын  негізін қалаушы:\nШарманов Т.Ш.\nАмрин К.Р.;\nСоловьев З.П.\nЧарльз Дарвин\nНеменко Б.А.\n2. Гигиенаның түрлері:\nеңбек гигиенасы;\nэнтомология;\nпаразитология;\nжануарлардың мінез-құлық ғылымы\nэкология.\n3. Биологиялық фактор:\nмикроорганизмдер;\nылғалдылық;\nөсімдіктер, жәндіктер;\nкүн сәулесі.\n4. Гигиена бойынша зерттеу әдістері:\nсанитарлық сипаттама;\nбиохимиялық;\nультрадыбыстық;\nрадиографиялық.'

# Define the pattern for splitting the input string
pattern = r'\d+\..+?(?=\d+\.)'
prefixes = ["A", "B", "C", "D", "E", "F", "G", "H", "I", "J", "K", "L", "M", "N", "O", "P", "Q", "R", "S", "T", "U", "V", "W", "X", "Y", "Z"]
count = 1
for x in ans:
    input_string = ' '.join(x)
    split_strings = re.findall(pattern, input_string, re.DOTALL)
    if count != 3:
        count += 1 
        continue
#     print(split_strings)
    for string in split_strings:
        s = ''.join(string)
        string = s
        cnt = 0
        print(s)
        for i in range(len(s)):
#             print(s[i])
#             if s[i]=='\n':
#                 s = s[:i] + f'\n{prefixes[cnt]}. ' + s[i+1:]
#                 cnt += 1
                
        print(s)
# Use regex to split the input string based on the pattern
# split_strings = re.findall(pattern, input_string, re.DOTALL)

# Print the split strings
# a = ""
# for string in split_strings:
#     print(string.strip())
#     break
116/112:
import re

# Input string
input_string = 'Гигиена пәніне кіріспе:\n\n1. Қазақстандағы тағам  гигиенасын  негізін қалаушы:\nШарманов Т.Ш.\nАмрин К.Р.;\nСоловьев З.П.\nЧарльз Дарвин\nНеменко Б.А.\n2. Гигиенаның түрлері:\nеңбек гигиенасы;\nэнтомология;\nпаразитология;\nжануарлардың мінез-құлық ғылымы\nэкология.\n3. Биологиялық фактор:\nмикроорганизмдер;\nылғалдылық;\nөсімдіктер, жәндіктер;\nкүн сәулесі.\n4. Гигиена бойынша зерттеу әдістері:\nсанитарлық сипаттама;\nбиохимиялық;\nультрадыбыстық;\nрадиографиялық.'

# Define the pattern for splitting the input string
pattern = r'\d+\..+?(?=\d+\.)'
prefixes = ["A", "B", "C", "D", "E", "F", "G", "H", "I", "J", "K", "L", "M", "N", "O", "P", "Q", "R", "S", "T", "U", "V", "W", "X", "Y", "Z"]
count = 1
for x in ans:
    input_string = ' '.join(x)
    split_strings = re.findall(pattern, input_string, re.DOTALL)
    if count != 3:
        count += 1 
        continue
#     print(split_strings)
    for string in split_strings:
        s = ''.join(string)
        string = s
        cnt = 0
        print(s)
        for i in range(len(s)):
#             print(s[i])
#             if s[i]=='\n':
#                 s = s[:i] + f'\n{prefixes[cnt]}. ' + s[i+1:]
#                 cnt += 1
                
        print(s)
# Use regex to split the input string based on the pattern
# split_strings = re.findall(pattern, input_string, re.DOTALL)

# Print the split strings
# a = ""
# for string in split_strings:
#     print(string.strip())
#     break
116/113:
import re

# Input string
input_string = 'Гигиена пәніне кіріспе:\n\n1. Қазақстандағы тағам  гигиенасын  негізін қалаушы:\nШарманов Т.Ш.\nАмрин К.Р.;\nСоловьев З.П.\nЧарльз Дарвин\nНеменко Б.А.\n2. Гигиенаның түрлері:\nеңбек гигиенасы;\nэнтомология;\nпаразитология;\nжануарлардың мінез-құлық ғылымы\nэкология.\n3. Биологиялық фактор:\nмикроорганизмдер;\nылғалдылық;\nөсімдіктер, жәндіктер;\nкүн сәулесі.\n4. Гигиена бойынша зерттеу әдістері:\nсанитарлық сипаттама;\nбиохимиялық;\nультрадыбыстық;\nрадиографиялық.'

# Define the pattern for splitting the input string
pattern = r'\d+\..+?(?=\d+\.)'
prefixes = ["A", "B", "C", "D", "E", "F", "G", "H", "I", "J", "K", "L", "M", "N", "O", "P", "Q", "R", "S", "T", "U", "V", "W", "X", "Y", "Z"]
count = 1
for x in ans:
    input_string = ' '.join(x)
    split_strings = re.findall(pattern, input_string, re.DOTALL)
    if count != 3:
        count += 1 
        continue
#     print(split_strings)
    for string in split_strings:
        s = ''.join(string)
        string = s
        cnt = 0
        print(s)
        for i in range(len(s)):
#             print(s[i])
            if s[i]=='\n':
                s = s[:i] + f'\n{prefixes[cnt]}. ' + s[i+1:]
                cnt += 1
                
        print(s)
# Use regex to split the input string based on the pattern
# split_strings = re.findall(pattern, input_string, re.DOTALL)

# Print the split strings
# a = ""
# for string in split_strings:
#     print(string.strip())
#     break
116/114:
import re

# Input string
input_string = 'Гигиена пәніне кіріспе:\n\n1. Қазақстандағы тағам  гигиенасын  негізін қалаушы:\nШарманов Т.Ш.\nАмрин К.Р.;\nСоловьев З.П.\nЧарльз Дарвин\nНеменко Б.А.\n2. Гигиенаның түрлері:\nеңбек гигиенасы;\nэнтомология;\nпаразитология;\nжануарлардың мінез-құлық ғылымы\nэкология.\n3. Биологиялық фактор:\nмикроорганизмдер;\nылғалдылық;\nөсімдіктер, жәндіктер;\nкүн сәулесі.\n4. Гигиена бойынша зерттеу әдістері:\nсанитарлық сипаттама;\nбиохимиялық;\nультрадыбыстық;\nрадиографиялық.'

# Define the pattern for splitting the input string
pattern = r'\d+\..+?(?=\d+\.)'
prefixes = ["A", "B", "C", "D", "E", "F", "G", "H", "I", "J", "K", "L", "M", "N", "O", "P", "Q", "R", "S", "T", "U", "V", "W", "X", "Y", "Z"]
count = 1
for x in ans:
    input_string = ' '.join(x)
    split_strings = re.findall(pattern, input_string, re.DOTALL)
    if count != 3:
        count += 1 
        continue
    print(split_strings)
    for string in split_strings:
        s = ''.join(string)
        string = s
        cnt = 0
        print(s)
        for i in range(len(s)):
#             print(s[i])
            if s[i]=='\n':
                s = s[:i] + f'\n{prefixes[cnt]}. ' + s[i+1:]
                cnt += 1
                
        print(s)
# Use regex to split the input string based on the pattern
# split_strings = re.findall(pattern, input_string, re.DOTALL)

# Print the split strings
# a = ""
# for string in split_strings:
#     print(string.strip())
#     break
116/115:
import re

# Input string
input_string = 'Гигиена пәніне кіріспе:\n\n1. Қазақстандағы тағам  гигиенасын  негізін қалаушы:\nШарманов Т.Ш.\nАмрин К.Р.;\nСоловьев З.П.\nЧарльз Дарвин\nНеменко Б.А.\n2. Гигиенаның түрлері:\nеңбек гигиенасы;\nэнтомология;\nпаразитология;\nжануарлардың мінез-құлық ғылымы\nэкология.\n3. Биологиялық фактор:\nмикроорганизмдер;\nылғалдылық;\nөсімдіктер, жәндіктер;\nкүн сәулесі.\n4. Гигиена бойынша зерттеу әдістері:\nсанитарлық сипаттама;\nбиохимиялық;\nультрадыбыстық;\nрадиографиялық.'

# Define the pattern for splitting the input string
pattern = r'\d+\..+?(?=\d+\.)'
prefixes = ["A", "B", "C", "D", "E", "F", "G", "H", "I", "J", "K", "L", "M", "N", "O", "P", "Q", "R", "S", "T", "U", "V", "W", "X", "Y", "Z"]
count = 1
for x in ans:
    input_string = ' '.join(x)
    split_strings = re.findall(pattern, input_string, re.DOTALL)
    if count != 3:
        count += 1 
        continue
    print(split_strings)
    for string in split_strings:
        print(string)
        s = ''.join(string)
        string = s
        cnt = 0
        print(s)
        for i in range(len(s)):
#             print(s[i])
            if s[i]=='\n':
                s = s[:i] + f'\n{prefixes[cnt]}. ' + s[i+1:]
                cnt += 1
                
        print(s)
# Use regex to split the input string based on the pattern
# split_strings = re.findall(pattern, input_string, re.DOTALL)

# Print the split strings
# a = ""
# for string in split_strings:
#     print(string.strip())
#     break
116/116:
import re

# Input string
input_string = 'Гигиена пәніне кіріспе:\n\n1. Қазақстандағы тағам  гигиенасын  негізін қалаушы:\nШарманов Т.Ш.\nАмрин К.Р.;\nСоловьев З.П.\nЧарльз Дарвин\nНеменко Б.А.\n2. Гигиенаның түрлері:\nеңбек гигиенасы;\nэнтомология;\nпаразитология;\nжануарлардың мінез-құлық ғылымы\nэкология.\n3. Биологиялық фактор:\nмикроорганизмдер;\nылғалдылық;\nөсімдіктер, жәндіктер;\nкүн сәулесі.\n4. Гигиена бойынша зерттеу әдістері:\nсанитарлық сипаттама;\nбиохимиялық;\nультрадыбыстық;\nрадиографиялық.'

# Define the pattern for splitting the input string
pattern = r'\d+\..+?(?=\d+\.)'
prefixes = ["A", "B", "C", "D", "E", "F", "G", "H", "I", "J", "K", "L", "M", "N", "O", "P", "Q", "R", "S", "T", "U", "V", "W", "X", "Y", "Z"]
count = 1
for x in ans:
    input_string = ' '.join(x)
    split_strings = re.findall(pattern, input_string, re.DOTALL)
    if count != 3:
        count += 1 
        continue
    print(split_strings)
    for string in split_strings:
        print(string)
        s = ''.join(string)
        string = s
        cnt = 0
#         print(s)
        for i in range(len(s)):
#             print(s[i])
            if s[i]=='\n':
                s = s[:i] + f'\n{prefixes[cnt]}. ' + s[i+1:]
                cnt += 1
                
        print(s)
# Use regex to split the input string based on the pattern
# split_strings = re.findall(pattern, input_string, re.DOTALL)

# Print the split strings
# a = ""
# for string in split_strings:
#     print(string.strip())
#     break
116/117:
import re

# Input string
input_string = 'Гигиена пәніне кіріспе:\n\n1. Қазақстандағы тағам  гигиенасын  негізін қалаушы:\nШарманов Т.Ш.\nАмрин К.Р.;\nСоловьев З.П.\nЧарльз Дарвин\nНеменко Б.А.\n2. Гигиенаның түрлері:\nеңбек гигиенасы;\nэнтомология;\nпаразитология;\nжануарлардың мінез-құлық ғылымы\nэкология.\n3. Биологиялық фактор:\nмикроорганизмдер;\nылғалдылық;\nөсімдіктер, жәндіктер;\nкүн сәулесі.\n4. Гигиена бойынша зерттеу әдістері:\nсанитарлық сипаттама;\nбиохимиялық;\nультрадыбыстық;\nрадиографиялық.'

# Define the pattern for splitting the input string
pattern = r'\d+\..+?(?=\d+\.)'
prefixes = ["A", "B", "C", "D", "E", "F", "G", "H", "I", "J", "K", "L", "M", "N", "O", "P", "Q", "R", "S", "T", "U", "V", "W", "X", "Y", "Z"]
count = 1
for x in ans:
    input_string = ' '.join(x)
    split_strings = re.findall(pattern, input_string, re.DOTALL)
    if count != 3:
        count += 1 
        continue
#     print(split_strings)
    split_strings = split_strings[-1]
    lines = split_strings.split('\n')
    
    # Join the lines with the symbol added before and after
    modified_string = f'\nsf'.join(lines)
    print(modified_string)
#     for string in split_strings:
#         print(string)
#         s = ''.join(string)
#         string = s
#         cnt = 0
# #         print(s)
#         for i in range(len(s)):
# #             print(s[i])
#             if s[i]=='\n':
#                 s = s[:i] + f'\n{prefixes[cnt]}. ' + s[i+1:]
#                 cnt += 1
                
#         print(s)
# Use regex to split the input string based on the pattern
# split_strings = re.findall(pattern, input_string, re.DOTALL)

# Print the split strings
# a = ""
# for string in split_strings:
#     print(string.strip())
#     break
116/118:
import re

# Input string
input_string = 'Гигиена пәніне кіріспе:\n\n1. Қазақстандағы тағам  гигиенасын  негізін қалаушы:\nШарманов Т.Ш.\nАмрин К.Р.;\nСоловьев З.П.\nЧарльз Дарвин\nНеменко Б.А.\n2. Гигиенаның түрлері:\nеңбек гигиенасы;\nэнтомология;\nпаразитология;\nжануарлардың мінез-құлық ғылымы\nэкология.\n3. Биологиялық фактор:\nмикроорганизмдер;\nылғалдылық;\nөсімдіктер, жәндіктер;\nкүн сәулесі.\n4. Гигиена бойынша зерттеу әдістері:\nсанитарлық сипаттама;\nбиохимиялық;\nультрадыбыстық;\nрадиографиялық.'

# Define the pattern for splitting the input string
pattern = r'\d+\..+?(?=\d+\.)'
prefixes = ["A", "B", "C", "D", "E", "F", "G", "H", "I", "J", "K", "L", "M", "N", "O", "P", "Q", "R", "S", "T", "U", "V", "W", "X", "Y", "Z"]
count = 1
for x in ans:
    input_string = ' '.join(x)
    split_strings = re.findall(pattern, input_string, re.DOTALL)
    if count != 3:
        count += 1 
        continue
#     print(split_strings)
    split_strings = split_strings[-1]
    lines = split_strings.split('\n')
    
    # Join the lines with the symbol added before and after
    modified_string = f'\nsf '.join(lines)
    print(modified_string)
#     for string in split_strings:
#         print(string)
#         s = ''.join(string)
#         string = s
#         cnt = 0
# #         print(s)
#         for i in range(len(s)):
# #             print(s[i])
#             if s[i]=='\n':
#                 s = s[:i] + f'\n{prefixes[cnt]}. ' + s[i+1:]
#                 cnt += 1
                
#         print(s)
# Use regex to split the input string based on the pattern
# split_strings = re.findall(pattern, input_string, re.DOTALL)

# Print the split strings
# a = ""
# for string in split_strings:
#     print(string.strip())
#     break
116/119:
import re

# Input string
input_string = 'Гигиена пәніне кіріспе:\n\n1. Қазақстандағы тағам  гигиенасын  негізін қалаушы:\nШарманов Т.Ш.\nАмрин К.Р.;\nСоловьев З.П.\nЧарльз Дарвин\nНеменко Б.А.\n2. Гигиенаның түрлері:\nеңбек гигиенасы;\nэнтомология;\nпаразитология;\nжануарлардың мінез-құлық ғылымы\nэкология.\n3. Биологиялық фактор:\nмикроорганизмдер;\nылғалдылық;\nөсімдіктер, жәндіктер;\nкүн сәулесі.\n4. Гигиена бойынша зерттеу әдістері:\nсанитарлық сипаттама;\nбиохимиялық;\nультрадыбыстық;\nрадиографиялық.'

# Define the pattern for splitting the input string
pattern = r'\d+\..+?(?=\d+\.)'
prefixes = ["A", "B", "C", "D", "E", "F", "G", "H", "I", "J", "K", "L", "M", "N", "O", "P", "Q", "R", "S", "T", "U", "V", "W", "X", "Y", "Z"]
count = 1
for x in ans:
    input_string = ' '.join(x)
    split_strings = re.findall(pattern, input_string, re.DOTALL)
    if count != 3:
        count += 1 
        continue
#     print(split_strings)
    split_strings = split_strings[-1]
    lines = split_strings.split('\n')
    print(lines)
    # Join the lines with the symbol added before and after
#     modified_string = f'\nsf '.join(lines)
#     print(modified_string)
#     for string in split_strings:
#         print(string)
#         s = ''.join(string)
#         string = s
#         cnt = 0
# #         print(s)
#         for i in range(len(s)):
# #             print(s[i])
#             if s[i]=='\n':
#                 s = s[:i] + f'\n{prefixes[cnt]}. ' + s[i+1:]
#                 cnt += 1
                
#         print(s)
# Use regex to split the input string based on the pattern
# split_strings = re.findall(pattern, input_string, re.DOTALL)

# Print the split strings
# a = ""
# for string in split_strings:
#     print(string.strip())
#     break
116/120:
import re

# Input string
input_string = 'Гигиена пәніне кіріспе:\n\n1. Қазақстандағы тағам  гигиенасын  негізін қалаушы:\nШарманов Т.Ш.\nАмрин К.Р.;\nСоловьев З.П.\nЧарльз Дарвин\nНеменко Б.А.\n2. Гигиенаның түрлері:\nеңбек гигиенасы;\nэнтомология;\nпаразитология;\nжануарлардың мінез-құлық ғылымы\nэкология.\n3. Биологиялық фактор:\nмикроорганизмдер;\nылғалдылық;\nөсімдіктер, жәндіктер;\nкүн сәулесі.\n4. Гигиена бойынша зерттеу әдістері:\nсанитарлық сипаттама;\nбиохимиялық;\nультрадыбыстық;\nрадиографиялық.'

# Define the pattern for splitting the input string
pattern = r'\d+\..+?(?=\d+\.)'
prefixes = ["A", "B", "C", "D", "E", "F", "G", "H", "I", "J", "K", "L", "M", "N", "O", "P", "Q", "R", "S", "T", "U", "V", "W", "X", "Y", "Z"]
count = 1
for x in ans:
    input_string = ' '.join(x)
    split_strings = re.findall(pattern, input_string, re.DOTALL)
    if count != 3:
        count += 1 
        continue
#     print(split_strings)
    
    lines = split_strings.split('\n')
    print(lines)
    # Join the lines with the symbol added before and after
#     modified_string = f'\nsf '.join(lines)
#     print(modified_string)
#     for string in split_strings:
#         print(string)
#         s = ''.join(string)
#         string = s
#         cnt = 0
# #         print(s)
#         for i in range(len(s)):
# #             print(s[i])
#             if s[i]=='\n':
#                 s = s[:i] + f'\n{prefixes[cnt]}. ' + s[i+1:]
#                 cnt += 1
                
#         print(s)
# Use regex to split the input string based on the pattern
# split_strings = re.findall(pattern, input_string, re.DOTALL)

# Print the split strings
# a = ""
# for string in split_strings:
#     print(string.strip())
#     break
116/121:
import re

# Input string
input_string = 'Гигиена пәніне кіріспе:\n\n1. Қазақстандағы тағам  гигиенасын  негізін қалаушы:\nШарманов Т.Ш.\nАмрин К.Р.;\nСоловьев З.П.\nЧарльз Дарвин\nНеменко Б.А.\n2. Гигиенаның түрлері:\nеңбек гигиенасы;\nэнтомология;\nпаразитология;\nжануарлардың мінез-құлық ғылымы\nэкология.\n3. Биологиялық фактор:\nмикроорганизмдер;\nылғалдылық;\nөсімдіктер, жәндіктер;\nкүн сәулесі.\n4. Гигиена бойынша зерттеу әдістері:\nсанитарлық сипаттама;\nбиохимиялық;\nультрадыбыстық;\nрадиографиялық.'

# Define the pattern for splitting the input string
pattern = r'\d+\..+?(?=\d+\.)'
prefixes = ["A", "B", "C", "D", "E", "F", "G", "H", "I", "J", "K", "L", "M", "N", "O", "P", "Q", "R", "S", "T", "U", "V", "W", "X", "Y", "Z"]
count = 1
for x in ans:
    input_string = ' '.join(x)
    split_strings = re.findall(pattern, input_string, re.DOTALL)
    if count != 3:
        count += 1 
        continue
    print(split_strings)
#     split_strings = split_strings[-1]
#     lines = split_strings.split('\n')
#     print(lines)
    # Join the lines with the symbol added before and after
#     modified_string = f'\nsf '.join(lines)
#     print(modified_string)
#     for string in split_strings:
#         print(string)
#         s = ''.join(string)
#         string = s
#         cnt = 0
# #         print(s)
#         for i in range(len(s)):
# #             print(s[i])
#             if s[i]=='\n':
#                 s = s[:i] + f'\n{prefixes[cnt]}. ' + s[i+1:]
#                 cnt += 1
                
#         print(s)
# Use regex to split the input string based on the pattern
# split_strings = re.findall(pattern, input_string, re.DOTALL)

# Print the split strings
# a = ""
# for string in split_strings:
#     print(string.strip())
#     break
116/122:
import re

# Input string
input_string = 'Гигиена пәніне кіріспе:\n\n1. Қазақстандағы тағам  гигиенасын  негізін қалаушы:\nШарманов Т.Ш.\nАмрин К.Р.;\nСоловьев З.П.\nЧарльз Дарвин\nНеменко Б.А.\n2. Гигиенаның түрлері:\nеңбек гигиенасы;\nэнтомология;\nпаразитология;\nжануарлардың мінез-құлық ғылымы\nэкология.\n3. Биологиялық фактор:\nмикроорганизмдер;\nылғалдылық;\nөсімдіктер, жәндіктер;\nкүн сәулесі.\n4. Гигиена бойынша зерттеу әдістері:\nсанитарлық сипаттама;\nбиохимиялық;\nультрадыбыстық;\nрадиографиялық.'

# Define the pattern for splitting the input string
pattern = r'\d+\..+?(?=\d+\.)'
prefixes = ["A", "B", "C", "D", "E", "F", "G", "H", "I", "J", "K", "L", "M", "N", "O", "P", "Q", "R", "S", "T", "U", "V", "W", "X", "Y", "Z"]
count = 1
for x in ans:
    input_string = ' '.join(x)
    split_strings = re.findall(pattern, input_string, re.DOTALL)
    if count != 3:
        count += 1 
        continue
    
    for string in split_strings:
        split_strings = split_strings[-1]
        lines = split_strings.split('\n')
        print(lines)
        Join the lines with the symbol added before and after
        modified_string = f'\nsf '.join(lines)
        print(modified_string)
        print(string)
        s = ''.join(string)
        string = s
        cnt = 0
        break
#         print(s)
        for i in range(len(s)):
#             print(s[i])
            if s[i]=='\n':
                s = s[:i] + f'\n{prefixes[cnt]}. ' + s[i+1:]
                cnt += 1
                
#         print(s)
# Use regex to split the input string based on the pattern
# split_strings = re.findall(pattern, input_string, re.DOTALL)

# Print the split strings
# a = ""
# for string in split_strings:
#     print(string.strip())
#     break
116/123:
import re

# Input string
input_string = 'Гигиена пәніне кіріспе:\n\n1. Қазақстандағы тағам  гигиенасын  негізін қалаушы:\nШарманов Т.Ш.\nАмрин К.Р.;\nСоловьев З.П.\nЧарльз Дарвин\nНеменко Б.А.\n2. Гигиенаның түрлері:\nеңбек гигиенасы;\nэнтомология;\nпаразитология;\nжануарлардың мінез-құлық ғылымы\nэкология.\n3. Биологиялық фактор:\nмикроорганизмдер;\nылғалдылық;\nөсімдіктер, жәндіктер;\nкүн сәулесі.\n4. Гигиена бойынша зерттеу әдістері:\nсанитарлық сипаттама;\nбиохимиялық;\nультрадыбыстық;\nрадиографиялық.'

# Define the pattern for splitting the input string
pattern = r'\d+\..+?(?=\d+\.)'
prefixes = ["A", "B", "C", "D", "E", "F", "G", "H", "I", "J", "K", "L", "M", "N", "O", "P", "Q", "R", "S", "T", "U", "V", "W", "X", "Y", "Z"]
count = 1
for x in ans:
    input_string = ' '.join(x)
    split_strings = re.findall(pattern, input_string, re.DOTALL)
    if count != 3:
        count += 1 
        continue
    
    for string in split_strings:
        split_strings = split_strings[-1]
        lines = split_strings.split('\n')
        print(lines)
        modified_string = f'\nsf '.join(lines)
        print(modified_string)
        print(string)
        s = ''.join(string)
        string = s
        cnt = 0
        break
#         print(s)
        for i in range(len(s)):
#             print(s[i])
            if s[i]=='\n':
                s = s[:i] + f'\n{prefixes[cnt]}. ' + s[i+1:]
                cnt += 1
                
#         print(s)
# Use regex to split the input string based on the pattern
# split_strings = re.findall(pattern, input_string, re.DOTALL)

# Print the split strings
# a = ""
# for string in split_strings:
#     print(string.strip())
#     break
116/124:
import re

# Input string
input_string = 'Гигиена пәніне кіріспе:\n\n1. Қазақстандағы тағам  гигиенасын  негізін қалаушы:\nШарманов Т.Ш.\nАмрин К.Р.;\nСоловьев З.П.\nЧарльз Дарвин\nНеменко Б.А.\n2. Гигиенаның түрлері:\nеңбек гигиенасы;\nэнтомология;\nпаразитология;\nжануарлардың мінез-құлық ғылымы\nэкология.\n3. Биологиялық фактор:\nмикроорганизмдер;\nылғалдылық;\nөсімдіктер, жәндіктер;\nкүн сәулесі.\n4. Гигиена бойынша зерттеу әдістері:\nсанитарлық сипаттама;\nбиохимиялық;\nультрадыбыстық;\nрадиографиялық.'

# Define the pattern for splitting the input string
pattern = r'\d+\..+?(?=\d+\.)'
prefixes = ["A", "B", "C", "D", "E", "F", "G", "H", "I", "J", "K", "L", "M", "N", "O", "P", "Q", "R", "S", "T", "U", "V", "W", "X", "Y", "Z"]
count = 1
for x in ans:
    input_string = ' '.join(x)
    split_strings = re.findall(pattern, input_string, re.DOTALL)
    if count != 3:
        count += 1 
        continue
    
    for string in split_strings:
        split_strings = split_strings[-1]
        lines = split_strings.split('\n')
        print(lines)
        modified_string = f'\nsf '.join(lines)
#         print(modified_string)
#         print(string)
        s = ''.join(string)
        string = s
        cnt = 0
        break
#         print(s)
        for i in range(len(s)):
#             print(s[i])
            if s[i]=='\n':
                s = s[:i] + f'\n{prefixes[cnt]}. ' + s[i+1:]
                cnt += 1
                
#         print(s)
# Use regex to split the input string based on the pattern
# split_strings = re.findall(pattern, input_string, re.DOTALL)

# Print the split strings
# a = ""
# for string in split_strings:
#     print(string.strip())
#     break
116/125:
import re

# Input string
input_string = 'Гигиена пәніне кіріспе:\n\n1. Қазақстандағы тағам  гигиенасын  негізін қалаушы:\nШарманов Т.Ш.\nАмрин К.Р.;\nСоловьев З.П.\nЧарльз Дарвин\nНеменко Б.А.\n2. Гигиенаның түрлері:\nеңбек гигиенасы;\nэнтомология;\nпаразитология;\nжануарлардың мінез-құлық ғылымы\nэкология.\n3. Биологиялық фактор:\nмикроорганизмдер;\nылғалдылық;\nөсімдіктер, жәндіктер;\nкүн сәулесі.\n4. Гигиена бойынша зерттеу әдістері:\nсанитарлық сипаттама;\nбиохимиялық;\nультрадыбыстық;\nрадиографиялық.'

# Define the pattern for splitting the input string
pattern = r'\d+\..+?(?=\d+\.)'
prefixes = ["A", "B", "C", "D", "E", "F", "G", "H", "I", "J", "K", "L", "M", "N", "O", "P", "Q", "R", "S", "T", "U", "V", "W", "X", "Y", "Z"]
count = 1
for x in ans:
    input_string = ' '.join(x)
    split_strings = re.findall(pattern, input_string, re.DOTALL)
    if count != 3:
        count += 1 
        continue
    
    for string in split_strings:
        split_strings = split_strings[-1]
        lines = split_strings.split('\n')
        print(lines)
        modified_string = f'\nsf '.join(lines)
#         print(modified_string)
#         print(string)
        s = ''.join(string)
        string = s
        cnt = 0
#         break
#         print(s)
#         for i in range(len(s)):
# #             print(s[i])
#             if s[i]=='\n':
#                 s = s[:i] + f'\n{prefixes[cnt]}. ' + s[i+1:]
#                 cnt += 1
                
#         print(s)
# Use regex to split the input string based on the pattern
# split_strings = re.findall(pattern, input_string, re.DOTALL)

# Print the split strings
# a = ""
# for string in split_strings:
#     print(string.strip())
#     break
116/126:
import re

# Input string
input_string = 'Гигиена пәніне кіріспе:\n\n1. Қазақстандағы тағам  гигиенасын  негізін қалаушы:\nШарманов Т.Ш.\nАмрин К.Р.;\nСоловьев З.П.\nЧарльз Дарвин\nНеменко Б.А.\n2. Гигиенаның түрлері:\nеңбек гигиенасы;\nэнтомология;\nпаразитология;\nжануарлардың мінез-құлық ғылымы\nэкология.\n3. Биологиялық фактор:\nмикроорганизмдер;\nылғалдылық;\nөсімдіктер, жәндіктер;\nкүн сәулесі.\n4. Гигиена бойынша зерттеу әдістері:\nсанитарлық сипаттама;\nбиохимиялық;\nультрадыбыстық;\nрадиографиялық.'

# Define the pattern for splitting the input string
pattern = r'\d+\..+?(?=\d+\.)'
prefixes = ["A", "B", "C", "D", "E", "F", "G", "H", "I", "J", "K", "L", "M", "N", "O", "P", "Q", "R", "S", "T", "U", "V", "W", "X", "Y", "Z"]
count = 1
for x in ans:
    input_string = ' '.join(x)
    split_strings = re.findall(pattern, input_string, re.DOTALL)
    if count != 3:
        count += 1 
        continue
    
    for string in split_strings:

        lines = split_strings.split('\n')
        print(lines)
        modified_string = f'\nsf '.join(lines)
#         print(modified_string)
#         print(string)
        s = ''.join(string)
        string = s
        cnt = 0
#         break
#         print(s)
#         for i in range(len(s)):
# #             print(s[i])
#             if s[i]=='\n':
#                 s = s[:i] + f'\n{prefixes[cnt]}. ' + s[i+1:]
#                 cnt += 1
                
#         print(s)
# Use regex to split the input string based on the pattern
# split_strings = re.findall(pattern, input_string, re.DOTALL)

# Print the split strings
# a = ""
# for string in split_strings:
#     print(string.strip())
#     break
116/127:
import re

# Input string
input_string = 'Гигиена пәніне кіріспе:\n\n1. Қазақстандағы тағам  гигиенасын  негізін қалаушы:\nШарманов Т.Ш.\nАмрин К.Р.;\nСоловьев З.П.\nЧарльз Дарвин\nНеменко Б.А.\n2. Гигиенаның түрлері:\nеңбек гигиенасы;\nэнтомология;\nпаразитология;\nжануарлардың мінез-құлық ғылымы\nэкология.\n3. Биологиялық фактор:\nмикроорганизмдер;\nылғалдылық;\nөсімдіктер, жәндіктер;\nкүн сәулесі.\n4. Гигиена бойынша зерттеу әдістері:\nсанитарлық сипаттама;\nбиохимиялық;\nультрадыбыстық;\nрадиографиялық.'

# Define the pattern for splitting the input string
pattern = r'\d+\..+?(?=\d+\.)'
prefixes = ["A", "B", "C", "D", "E", "F", "G", "H", "I", "J", "K", "L", "M", "N", "O", "P", "Q", "R", "S", "T", "U", "V", "W", "X", "Y", "Z"]
count = 1
for x in ans:
    input_string = ' '.join(x)
    split_strings = re.findall(pattern, input_string, re.DOTALL)
    if count != 3:
        count += 1 
        continue
    
    for string in split_strings:

        lines = string.split('\n')
        print(lines)
        modified_string = f'\nsf '.join(lines)
#         print(modified_string)
#         print(string)
        s = ''.join(string)
        string = s
        cnt = 0
#         break
#         print(s)
#         for i in range(len(s)):
# #             print(s[i])
#             if s[i]=='\n':
#                 s = s[:i] + f'\n{prefixes[cnt]}. ' + s[i+1:]
#                 cnt += 1
                
#         print(s)
# Use regex to split the input string based on the pattern
# split_strings = re.findall(pattern, input_string, re.DOTALL)

# Print the split strings
# a = ""
# for string in split_strings:
#     print(string.strip())
#     break
116/128:
import re

# Input string
input_string = 'Гигиена пәніне кіріспе:\n\n1. Қазақстандағы тағам  гигиенасын  негізін қалаушы:\nШарманов Т.Ш.\nАмрин К.Р.;\nСоловьев З.П.\nЧарльз Дарвин\nНеменко Б.А.\n2. Гигиенаның түрлері:\nеңбек гигиенасы;\nэнтомология;\nпаразитология;\nжануарлардың мінез-құлық ғылымы\nэкология.\n3. Биологиялық фактор:\nмикроорганизмдер;\nылғалдылық;\nөсімдіктер, жәндіктер;\nкүн сәулесі.\n4. Гигиена бойынша зерттеу әдістері:\nсанитарлық сипаттама;\nбиохимиялық;\nультрадыбыстық;\nрадиографиялық.'

# Define the pattern for splitting the input string
pattern = r'\d+\..+?(?=\d+\.)'
prefixes = ["A", "B", "C", "D", "E", "F", "G", "H", "I", "J", "K", "L", "M", "N", "O", "P", "Q", "R", "S", "T", "U", "V", "W", "X", "Y", "Z"]
count = 1
for x in ans:
    input_string = ' '.join(x)
    split_strings = re.findall(pattern, input_string, re.DOTALL)
    if count != 3:
        count += 1 
        continue
    
    for string in split_strings:

#         lines = string.split('\n')
#         for e in range()
#         print(lines)
#         modified_string = f'\nsf '.join(lines)
#         print(modified_string)
#         print(string)
        s = ''.join(string)
        print(s)
#         break
#         print(s)
#         for i in range(len(s)):
# #             print(s[i])
#             if s[i]=='\n':
#                 s = s[:i] + f'\n{prefixes[cnt]}. ' + s[i+1:]
#                 cnt += 1
                
#         print(s)
# Use regex to split the input string based on the pattern
# split_strings = re.findall(pattern, input_string, re.DOTALL)

# Print the split strings
# a = ""
# for string in split_strings:
#     print(string.strip())
#     break
116/129:
import re

# Input string
input_string = 'Гигиена пәніне кіріспе:\n\n1. Қазақстандағы тағам  гигиенасын  негізін қалаушы:\nШарманов Т.Ш.\nАмрин К.Р.;\nСоловьев З.П.\nЧарльз Дарвин\nНеменко Б.А.\n2. Гигиенаның түрлері:\nеңбек гигиенасы;\nэнтомология;\nпаразитология;\nжануарлардың мінез-құлық ғылымы\nэкология.\n3. Биологиялық фактор:\nмикроорганизмдер;\nылғалдылық;\nөсімдіктер, жәндіктер;\nкүн сәулесі.\n4. Гигиена бойынша зерттеу әдістері:\nсанитарлық сипаттама;\nбиохимиялық;\nультрадыбыстық;\nрадиографиялық.'

# Define the pattern for splitting the input string
pattern = r'\d+\..+?(?=\d+\.)'
prefixes = ["A", "B", "C", "D", "E", "F", "G", "H", "I", "J", "K", "L", "M", "N", "O", "P", "Q", "R", "S", "T", "U", "V", "W", "X", "Y", "Z"]
count = 1
for x in ans:
    input_string = ' '.join(x)
    split_strings = re.findall(pattern, input_string, re.DOTALL)
    if count != 3:
        count += 1 
        continue
    
    for string in split_strings:


#         modified_string = f'\nsf '.join(lines)
#         print(modified_string)
#         print(string)
        s = ''.join(string)
        lines = s.split('\n')
#         for e in range()
        print(lines)
        print(s)
#         break
#         print(s)
#         for i in range(len(s)):
# #             print(s[i])
#             if s[i]=='\n':
#                 s = s[:i] + f'\n{prefixes[cnt]}. ' + s[i+1:]
#                 cnt += 1
                
#         print(s)
# Use regex to split the input string based on the pattern
# split_strings = re.findall(pattern, input_string, re.DOTALL)

# Print the split strings
# a = ""
# for string in split_strings:
#     print(string.strip())
#     break
116/130:
import re

# Input string
input_string = 'Гигиена пәніне кіріспе:\n\n1. Қазақстандағы тағам  гигиенасын  негізін қалаушы:\nШарманов Т.Ш.\nАмрин К.Р.;\nСоловьев З.П.\nЧарльз Дарвин\nНеменко Б.А.\n2. Гигиенаның түрлері:\nеңбек гигиенасы;\nэнтомология;\nпаразитология;\nжануарлардың мінез-құлық ғылымы\nэкология.\n3. Биологиялық фактор:\nмикроорганизмдер;\nылғалдылық;\nөсімдіктер, жәндіктер;\nкүн сәулесі.\n4. Гигиена бойынша зерттеу әдістері:\nсанитарлық сипаттама;\nбиохимиялық;\nультрадыбыстық;\nрадиографиялық.'

# Define the pattern for splitting the input string
pattern = r'\d+\..+?(?=\d+\.)'
prefixes = ["A", "B", "C", "D", "E", "F", "G", "H", "I", "J", "K", "L", "M", "N", "O", "P", "Q", "R", "S", "T", "U", "V", "W", "X", "Y", "Z"]
count = 1
for x in ans:
    input_string = ' '.join(x)
    split_strings = re.findall(pattern, input_string, re.DOTALL)
    if count != 3:
        count += 1 
        continue
    
    for string in split_strings:


#         modified_string = f'\nsf '.join(lines)
#         print(modified_string)
#         print(string)
        s = ''.join(string)
        lines = s.split('\n')
#         for e in range()
        print(lines)
#         print(s)
#         break
#         print(s)
#         for i in range(len(s)):
# #             print(s[i])
#             if s[i]=='\n':
#                 s = s[:i] + f'\n{prefixes[cnt]}. ' + s[i+1:]
#                 cnt += 1
                
#         print(s)
# Use regex to split the input string based on the pattern
# split_strings = re.findall(pattern, input_string, re.DOTALL)

# Print the split strings
# a = ""
# for string in split_strings:
#     print(string.strip())
#     break
116/131:
import re

# Input string
input_string = 'Гигиена пәніне кіріспе:\n\n1. Қазақстандағы тағам  гигиенасын  негізін қалаушы:\nШарманов Т.Ш.\nАмрин К.Р.;\nСоловьев З.П.\nЧарльз Дарвин\nНеменко Б.А.\n2. Гигиенаның түрлері:\nеңбек гигиенасы;\nэнтомология;\nпаразитология;\nжануарлардың мінез-құлық ғылымы\nэкология.\n3. Биологиялық фактор:\nмикроорганизмдер;\nылғалдылық;\nөсімдіктер, жәндіктер;\nкүн сәулесі.\n4. Гигиена бойынша зерттеу әдістері:\nсанитарлық сипаттама;\nбиохимиялық;\nультрадыбыстық;\nрадиографиялық.'

# Define the pattern for splitting the input string
pattern = r'\d+\..+?(?=\d+\.)'
prefixes = ["A", "B", "C", "D", "E", "F", "G", "H", "I", "J", "K", "L", "M", "N", "O", "P", "Q", "R", "S", "T", "U", "V", "W", "X", "Y", "Z"]
count = 1
for x in ans:
    input_string = ' '.join(x)
    split_strings = re.findall(pattern, input_string, re.DOTALL)
    if count != 3:
        count += 1 
        continue
    
    for string in split_strings:


#         modified_string = f'\nsf '.join(lines)
#         print(modified_string)
#         print(string)
        s = ''.join(string)
        lines = s.split('\n')
#         for e in range()
        print(lines)
        print(s)
#         break
#         print(s)
#         for i in range(len(s)):
# #             print(s[i])
#             if s[i]=='\n':
#                 s = s[:i] + f'\n{prefixes[cnt]}. ' + s[i+1:]
#                 cnt += 1
                
#         print(s)
# Use regex to split the input string based on the pattern
# split_strings = re.findall(pattern, input_string, re.DOTALL)

# Print the split strings
# a = ""
# for string in split_strings:
#     print(string.strip())
#     break
116/132:
import re

# Input string
input_string = 'Гигиена пәніне кіріспе:\n\n1. Қазақстандағы тағам  гигиенасын  негізін қалаушы:\nШарманов Т.Ш.\nАмрин К.Р.;\nСоловьев З.П.\nЧарльз Дарвин\nНеменко Б.А.\n2. Гигиенаның түрлері:\nеңбек гигиенасы;\nэнтомология;\nпаразитология;\nжануарлардың мінез-құлық ғылымы\nэкология.\n3. Биологиялық фактор:\nмикроорганизмдер;\nылғалдылық;\nөсімдіктер, жәндіктер;\nкүн сәулесі.\n4. Гигиена бойынша зерттеу әдістері:\nсанитарлық сипаттама;\nбиохимиялық;\nультрадыбыстық;\nрадиографиялық.'

# Define the pattern for splitting the input string
pattern = r'\d+\..+?(?=\d+\.)'
prefixes = ["A", "B", "C", "D", "E", "F", "G", "H", "I", "J", "K", "L", "M", "N", "O", "P", "Q", "R", "S", "T", "U", "V", "W", "X", "Y", "Z"]
count = 1
for x in ans:
    input_string = ' '.join(x)
    print(input_string)
    split_strings = re.findall(pattern, input_string)
    print(split_strings)
    break
    if count != 3:
        count += 1 
        continue
    
    for string in split_strings:


#         modified_string = f'\nsf '.join(lines)
#         print(modified_string)
#         print(string)
        s = ''.join(string)
        lines = s.split('\n')
#         for e in range()
        print(lines)
        print(s)
#         break
#         print(s)
#         for i in range(len(s)):
# #             print(s[i])
#             if s[i]=='\n':
#                 s = s[:i] + f'\n{prefixes[cnt]}. ' + s[i+1:]
#                 cnt += 1
                
#         print(s)
# Use regex to split the input string based on the pattern
# split_strings = re.findall(pattern, input_string, re.DOTALL)

# Print the split strings
# a = ""
# for string in split_strings:
#     print(string.strip())
#     break
116/133:
import re

# Input string
input_string = 'Гигиена пәніне кіріспе:\n\n1. Қазақстандағы тағам  гигиенасын  негізін қалаушы:\nШарманов Т.Ш.\nАмрин К.Р.;\nСоловьев З.П.\nЧарльз Дарвин\nНеменко Б.А.\n2. Гигиенаның түрлері:\nеңбек гигиенасы;\nэнтомология;\nпаразитология;\nжануарлардың мінез-құлық ғылымы\nэкология.\n3. Биологиялық фактор:\nмикроорганизмдер;\nылғалдылық;\nөсімдіктер, жәндіктер;\nкүн сәулесі.\n4. Гигиена бойынша зерттеу әдістері:\nсанитарлық сипаттама;\nбиохимиялық;\nультрадыбыстық;\nрадиографиялық.'

# Define the pattern for splitting the input string
pattern = r'\d+\..+?(?=\d+\.)'
prefixes = ["A", "B", "C", "D", "E", "F", "G", "H", "I", "J", "K", "L", "M", "N", "O", "P", "Q", "R", "S", "T", "U", "V", "W", "X", "Y", "Z"]
count = 1
for x in ans:
    input_string = ' '.join(x)
    print(input_string)
    split_strings = re.findall(pattern, input_string)
    
    if count != 3:
        count += 1 
        continue
    print(split_strings)
    break
    for string in split_strings:


#         modified_string = f'\nsf '.join(lines)
#         print(modified_string)
#         print(string)
        s = ''.join(string)
        lines = s.split('\n')
#         for e in range()
        print(lines)
        print(s)
#         break
#         print(s)
#         for i in range(len(s)):
# #             print(s[i])
#             if s[i]=='\n':
#                 s = s[:i] + f'\n{prefixes[cnt]}. ' + s[i+1:]
#                 cnt += 1
                
#         print(s)
# Use regex to split the input string based on the pattern
# split_strings = re.findall(pattern, input_string, re.DOTALL)

# Print the split strings
# a = ""
# for string in split_strings:
#     print(string.strip())
#     break
116/134:
import re

# Input string
input_string = 'Гигиена пәніне кіріспе:\n\n1. Қазақстандағы тағам  гигиенасын  негізін қалаушы:\nШарманов Т.Ш.\nАмрин К.Р.;\nСоловьев З.П.\nЧарльз Дарвин\nНеменко Б.А.\n2. Гигиенаның түрлері:\nеңбек гигиенасы;\nэнтомология;\nпаразитология;\nжануарлардың мінез-құлық ғылымы\nэкология.\n3. Биологиялық фактор:\nмикроорганизмдер;\nылғалдылық;\nөсімдіктер, жәндіктер;\nкүн сәулесі.\n4. Гигиена бойынша зерттеу әдістері:\nсанитарлық сипаттама;\nбиохимиялық;\nультрадыбыстық;\nрадиографиялық.'

# Define the pattern for splitting the input string
pattern = r'\d+\..+?(?=\d+\.)'
prefixes = ["A", "B", "C", "D", "E", "F", "G", "H", "I", "J", "K", "L", "M", "N", "O", "P", "Q", "R", "S", "T", "U", "V", "W", "X", "Y", "Z"]
count = 1
for x in ans:
#     input_string = ' '.join(x)
#     print(input_string)
    split_strings = re.findall(pattern, input_string)
    
    if count != 3:
        count += 1 
        continue
    print(split_strings)
    break
    for string in split_strings:


#         modified_string = f'\nsf '.join(lines)
#         print(modified_string)
#         print(string)
        s = ''.join(string)
        lines = s.split('\n')
#         for e in range()
        print(lines)
        print(s)
#         break
#         print(s)
#         for i in range(len(s)):
# #             print(s[i])
#             if s[i]=='\n':
#                 s = s[:i] + f'\n{prefixes[cnt]}. ' + s[i+1:]
#                 cnt += 1
                
#         print(s)
# Use regex to split the input string based on the pattern
# split_strings = re.findall(pattern, input_string, re.DOTALL)

# Print the split strings
# a = ""
# for string in split_strings:
#     print(string.strip())
#     break
116/135:
import re

# Input string
input_string = 'Гигиена пәніне кіріспе:\n\n1. Қазақстандағы тағам  гигиенасын  негізін қалаушы:\nШарманов Т.Ш.\nАмрин К.Р.;\nСоловьев З.П.\nЧарльз Дарвин\nНеменко Б.А.\n2. Гигиенаның түрлері:\nеңбек гигиенасы;\nэнтомология;\nпаразитология;\nжануарлардың мінез-құлық ғылымы\nэкология.\n3. Биологиялық фактор:\nмикроорганизмдер;\nылғалдылық;\nөсімдіктер, жәндіктер;\nкүн сәулесі.\n4. Гигиена бойынша зерттеу әдістері:\nсанитарлық сипаттама;\nбиохимиялық;\nультрадыбыстық;\nрадиографиялық.'

# Define the pattern for splitting the input string
pattern = r'\d+\..+?(?=\d+\.)'
prefixes = ["A", "B", "C", "D", "E", "F", "G", "H", "I", "J", "K", "L", "M", "N", "O", "P", "Q", "R", "S", "T", "U", "V", "W", "X", "Y", "Z"]
count = 1
for x in ans:
#     input_string = ' '.join(x)
#     print(input_string)
    split_strings = re.findall(pattern, input_string)
    
    if count != 3:
        count += 1 
        continue
    print(split_strings)
    input_string = ' '.join(x)
    print(input_string)
    break
    for string in split_strings:


#         modified_string = f'\nsf '.join(lines)
#         print(modified_string)
#         print(string)
        s = ''.join(string)
        lines = s.split('\n')
#         for e in range()
        print(lines)
        print(s)
#         break
#         print(s)
#         for i in range(len(s)):
# #             print(s[i])
#             if s[i]=='\n':
#                 s = s[:i] + f'\n{prefixes[cnt]}. ' + s[i+1:]
#                 cnt += 1
                
#         print(s)
# Use regex to split the input string based on the pattern
# split_strings = re.findall(pattern, input_string, re.DOTALL)

# Print the split strings
# a = ""
# for string in split_strings:
#     print(string.strip())
#     break
116/136:
import re

# Input string
input_string = 'Гигиена пәніне кіріспе:\n\n1. Қазақстандағы тағам  гигиенасын  негізін қалаушы:\nШарманов Т.Ш.\nАмрин К.Р.;\nСоловьев З.П.\nЧарльз Дарвин\nНеменко Б.А.\n2. Гигиенаның түрлері:\nеңбек гигиенасы;\nэнтомология;\nпаразитология;\nжануарлардың мінез-құлық ғылымы\nэкология.\n3. Биологиялық фактор:\nмикроорганизмдер;\nылғалдылық;\nөсімдіктер, жәндіктер;\nкүн сәулесі.\n4. Гигиена бойынша зерттеу әдістері:\nсанитарлық сипаттама;\nбиохимиялық;\nультрадыбыстық;\nрадиографиялық.'

# Define the pattern for splitting the input string
pattern = r'\d+\..+?(?=\d+\.)'
prefixes = ["A", "B", "C", "D", "E", "F", "G", "H", "I", "J", "K", "L", "M", "N", "O", "P", "Q", "R", "S", "T", "U", "V", "W", "X", "Y", "Z"]
count = 1
for x in ans:
#     input_string = ' '.join(x)
#     print(input_string)
    split_strings = re.findall(pattern, input_string)
    
    if count != 3:
        count += 1 
        continue
#     print(split_strings)
    input_string = ' '.join(x)
    print(input_string)
    break
    for string in split_strings:


#         modified_string = f'\nsf '.join(lines)
#         print(modified_string)
#         print(string)
        s = ''.join(string)
        lines = s.split('\n')
#         for e in range()
        print(lines)
        print(s)
#         break
#         print(s)
#         for i in range(len(s)):
# #             print(s[i])
#             if s[i]=='\n':
#                 s = s[:i] + f'\n{prefixes[cnt]}. ' + s[i+1:]
#                 cnt += 1
                
#         print(s)
# Use regex to split the input string based on the pattern
# split_strings = re.findall(pattern, input_string, re.DOTALL)

# Print the split strings
# a = ""
# for string in split_strings:
#     print(string.strip())
#     break
116/137:
import re

# Input string
input_string = 'Гигиена пәніне кіріспе:\n\n1. Қазақстандағы тағам  гигиенасын  негізін қалаушы:\nШарманов Т.Ш.\nАмрин К.Р.;\nСоловьев З.П.\nЧарльз Дарвин\nНеменко Б.А.\n2. Гигиенаның түрлері:\nеңбек гигиенасы;\nэнтомология;\nпаразитология;\nжануарлардың мінез-құлық ғылымы\nэкология.\n3. Биологиялық фактор:\nмикроорганизмдер;\nылғалдылық;\nөсімдіктер, жәндіктер;\nкүн сәулесі.\n4. Гигиена бойынша зерттеу әдістері:\nсанитарлық сипаттама;\nбиохимиялық;\nультрадыбыстық;\nрадиографиялық.'

# Define the pattern for splitting the input string
pattern = r'\d+\..+?(?=\d+\.)'
prefixes = ["A", "B", "C", "D", "E", "F", "G", "H", "I", "J", "K", "L", "M", "N", "O", "P", "Q", "R", "S", "T", "U", "V", "W", "X", "Y", "Z"]
count = 1
for x in ans:
#     input_string = ' '.join(x)
#     print(input_string)
    split_strings = re.findall(pattern, input_string)
    
    if count != 3:
        count += 1 
        continue
#     print(split_strings)
    input_string = ' '.join(x)
    print(input_string.split('\n'))
    
    break
    for string in split_strings:


#         modified_string = f'\nsf '.join(lines)
#         print(modified_string)
#         print(string)
        s = ''.join(string)
        lines = s.split('\n')
#         for e in range()
        print(lines)
        print(s)
#         break
#         print(s)
#         for i in range(len(s)):
# #             print(s[i])
#             if s[i]=='\n':
#                 s = s[:i] + f'\n{prefixes[cnt]}. ' + s[i+1:]
#                 cnt += 1
                
#         print(s)
# Use regex to split the input string based on the pattern
# split_strings = re.findall(pattern, input_string, re.DOTALL)

# Print the split strings
# a = ""
# for string in split_strings:
#     print(string.strip())
#     break
116/138:
import re

# Input string
input_string = 'Гигиена пәніне кіріспе:\n\n1. Қазақстандағы тағам  гигиенасын  негізін қалаушы:\nШарманов Т.Ш.\nАмрин К.Р.;\nСоловьев З.П.\nЧарльз Дарвин\nНеменко Б.А.\n2. Гигиенаның түрлері:\nеңбек гигиенасы;\nэнтомология;\nпаразитология;\nжануарлардың мінез-құлық ғылымы\nэкология.\n3. Биологиялық фактор:\nмикроорганизмдер;\nылғалдылық;\nөсімдіктер, жәндіктер;\nкүн сәулесі.\n4. Гигиена бойынша зерттеу әдістері:\nсанитарлық сипаттама;\nбиохимиялық;\nультрадыбыстық;\nрадиографиялық.'

# Define the pattern for splitting the input string
pattern = r'\d+\..+?(?=\d+\.)'
prefixes = ["A", "B", "C", "D", "E", "F", "G", "H", "I", "J", "K", "L", "M", "N", "O", "P", "Q", "R", "S", "T", "U", "V", "W", "X", "Y", "Z"]
count = 1
for x in ans:
#     input_string = ' '.join(x)
#     print(input_string)
    split_strings = re.findall(pattern, input_string)
    
    if count != 3:
        count += 1 
        continue
#     print(split_strings)
    input_string = ' '.join(x)
    print(split_strings)
#     print(input_string.split('\n'))
    
    break
    for string in split_strings:


#         modified_string = f'\nsf '.join(lines)
#         print(modified_string)
#         print(string)
        s = ''.join(string)
        lines = s.split('\n')
#         for e in range()
        print(lines)
        print(s)
#         break
#         print(s)
#         for i in range(len(s)):
# #             print(s[i])
#             if s[i]=='\n':
#                 s = s[:i] + f'\n{prefixes[cnt]}. ' + s[i+1:]
#                 cnt += 1
                
#         print(s)
# Use regex to split the input string based on the pattern
# split_strings = re.findall(pattern, input_string, re.DOTALL)

# Print the split strings
# a = ""
# for string in split_strings:
#     print(string.strip())
#     break
116/139:
import re

# Input string
input_string = 'Гигиена пәніне кіріспе:\n\n1. Қазақстандағы тағам  гигиенасын  негізін қалаушы:\nШарманов Т.Ш.\nАмрин К.Р.;\nСоловьев З.П.\nЧарльз Дарвин\nНеменко Б.А.\n2. Гигиенаның түрлері:\nеңбек гигиенасы;\nэнтомология;\nпаразитология;\nжануарлардың мінез-құлық ғылымы\nэкология.\n3. Биологиялық фактор:\nмикроорганизмдер;\nылғалдылық;\nөсімдіктер, жәндіктер;\nкүн сәулесі.\n4. Гигиена бойынша зерттеу әдістері:\nсанитарлық сипаттама;\nбиохимиялық;\nультрадыбыстық;\nрадиографиялық.'

# Define the pattern for splitting the input string
pattern = r'\d+\..+?(?=\d+\.)'
prefixes = ["A", "B", "C", "D", "E", "F", "G", "H", "I", "J", "K", "L", "M", "N", "O", "P", "Q", "R", "S", "T", "U", "V", "W", "X", "Y", "Z"]
count = 1
for x in ans:
    input_string = ' '.join(x)
#     print(input_string)
    split_strings = re.findall(pattern, input_string)
    
    if count != 3:
        count += 1 
        continue
#     print(split_strings)
    input_string = ' '.join(x)
    print(split_strings)
#     print(input_string.split('\n'))
    
    break
    for string in split_strings:


#         modified_string = f'\nsf '.join(lines)
#         print(modified_string)
#         print(string)
        s = ''.join(string)
        lines = s.split('\n')
#         for e in range()
        print(lines)
        print(s)
#         break
#         print(s)
#         for i in range(len(s)):
# #             print(s[i])
#             if s[i]=='\n':
#                 s = s[:i] + f'\n{prefixes[cnt]}. ' + s[i+1:]
#                 cnt += 1
                
#         print(s)
# Use regex to split the input string based on the pattern
# split_strings = re.findall(pattern, input_string, re.DOTALL)

# Print the split strings
# a = ""
# for string in split_strings:
#     print(string.strip())
#     break
116/140:
import re

# Input string
input_string = 'Гигиена пәніне кіріспе:\n\n1. Қазақстандағы тағам  гигиенасын  негізін қалаушы:\nШарманов Т.Ш.\nАмрин К.Р.;\nСоловьев З.П.\nЧарльз Дарвин\nНеменко Б.А.\n2. Гигиенаның түрлері:\nеңбек гигиенасы;\nэнтомология;\nпаразитология;\nжануарлардың мінез-құлық ғылымы\nэкология.\n3. Биологиялық фактор:\nмикроорганизмдер;\nылғалдылық;\nөсімдіктер, жәндіктер;\nкүн сәулесі.\n4. Гигиена бойынша зерттеу әдістері:\nсанитарлық сипаттама;\nбиохимиялық;\nультрадыбыстық;\nрадиографиялық.'

# Define the pattern for splitting the input string
pattern = r'\d+\..+?(?=\d+\.)'
prefixes = ["A", "B", "C", "D", "E", "F", "G", "H", "I", "J", "K", "L", "M", "N", "O", "P", "Q", "R", "S", "T", "U", "V", "W", "X", "Y", "Z"]
count = 1
for x in ans:
    input_string = ' '.join(x)
#     print(input_string)
    split_strings = re.findall(pattern, input_string)
    
    if count != 3:
        count += 1 
        continue
#     print(split_strings)
    input_string = ' '.join(x)
#     print(split_strings)
    print(input_string.split('\n'))
    
    break
    for string in split_strings:


#         modified_string = f'\nsf '.join(lines)
#         print(modified_string)
#         print(string)
        s = ''.join(string)
        lines = s.split('\n')
#         for e in range()
        print(lines)
        print(s)
#         break
#         print(s)
#         for i in range(len(s)):
# #             print(s[i])
#             if s[i]=='\n':
#                 s = s[:i] + f'\n{prefixes[cnt]}. ' + s[i+1:]
#                 cnt += 1
                
#         print(s)
# Use regex to split the input string based on the pattern
# split_strings = re.findall(pattern, input_string, re.DOTALL)

# Print the split strings
# a = ""
# for string in split_strings:
#     print(string.strip())
#     break
116/141:
import re

# Input string
input_string = 'Гигиена пәніне кіріспе:\n\n1. Қазақстандағы тағам  гигиенасын  негізін қалаушы:\nШарманов Т.Ш.\nАмрин К.Р.;\nСоловьев З.П.\nЧарльз Дарвин\nНеменко Б.А.\n2. Гигиенаның түрлері:\nеңбек гигиенасы;\nэнтомология;\nпаразитология;\nжануарлардың мінез-құлық ғылымы\nэкология.\n3. Биологиялық фактор:\nмикроорганизмдер;\nылғалдылық;\nөсімдіктер, жәндіктер;\nкүн сәулесі.\n4. Гигиена бойынша зерттеу әдістері:\nсанитарлық сипаттама;\nбиохимиялық;\nультрадыбыстық;\nрадиографиялық.'

# Define the pattern for splitting the input string
pattern = r'\d+\..+?(?=\d+\.)'
prefixes = ["A", "B", "C", "D", "E", "F", "G", "H", "I", "J", "K", "L", "M", "N", "O", "P", "Q", "R", "S", "T", "U", "V", "W", "X", "Y", "Z"]
count = 1
for x in ans:
    input_string = ' '.join(x)
#     print(input_string)
    split_strings = re.findall(pattern, input_string)
    
#     if count != 3:
#         count += 1 
#         continue
#     print(split_strings)
    input_string = ' '.join(x)
#     print(split_strings)
    print(input_string.split('\n'))
    
#     break
#     for string in split_strings:


# #         modified_string = f'\nsf '.join(lines)
# #         print(modified_string)
# #         print(string)
#         s = ''.join(string)
#         lines = s.split('\n')
# #         for e in range()
#         print(lines)
#         print(s)
#         break
#         print(s)
#         for i in range(len(s)):
# #             print(s[i])
#             if s[i]=='\n':
#                 s = s[:i] + f'\n{prefixes[cnt]}. ' + s[i+1:]
#                 cnt += 1
                
#         print(s)
# Use regex to split the input string based on the pattern
# split_strings = re.findall(pattern, input_string, re.DOTALL)

# Print the split strings
# a = ""
# for string in split_strings:
#     print(string.strip())
#     break
116/142:
import re

# Input string
input_string = 'Гигиена пәніне кіріспе:\n\n1. Қазақстандағы тағам  гигиенасын  негізін қалаушы:\nШарманов Т.Ш.\nАмрин К.Р.;\nСоловьев З.П.\nЧарльз Дарвин\nНеменко Б.А.\n2. Гигиенаның түрлері:\nеңбек гигиенасы;\nэнтомология;\nпаразитология;\nжануарлардың мінез-құлық ғылымы\nэкология.\n3. Биологиялық фактор:\nмикроорганизмдер;\nылғалдылық;\nөсімдіктер, жәндіктер;\nкүн сәулесі.\n4. Гигиена бойынша зерттеу әдістері:\nсанитарлық сипаттама;\nбиохимиялық;\nультрадыбыстық;\nрадиографиялық.'

# Define the pattern for splitting the input string
pattern = r'\d+\..+?(?=\d+\.)'
prefixes = ["A", "B", "C", "D", "E", "F", "G", "H", "I", "J", "K", "L", "M", "N", "O", "P", "Q", "R", "S", "T", "U", "V", "W", "X", "Y", "Z"]
count = 1
for x in ans:
    input_string = ' '.join(x)
#     print(input_string)
    split_strings = re.findall(pattern, input_string)
    
    if count != 3:
        count += 1 
        continue
#     print(split_strings)
    input_string = ' '.join(x)
    print(input_string)
#     print(split_strings)
#     print(input_string.split('\n'))
    
#     break
#     for string in split_strings:


# #         modified_string = f'\nsf '.join(lines)
# #         print(modified_string)
# #         print(string)
#         s = ''.join(string)
#         lines = s.split('\n')
# #         for e in range()
#         print(lines)
#         print(s)
#         break
#         print(s)
#         for i in range(len(s)):
# #             print(s[i])
#             if s[i]=='\n':
#                 s = s[:i] + f'\n{prefixes[cnt]}. ' + s[i+1:]
#                 cnt += 1
                
#         print(s)
# Use regex to split the input string based on the pattern
# split_strings = re.findall(pattern, input_string, re.DOTALL)

# Print the split strings
# a = ""
# for string in split_strings:
#     print(string.strip())
#     break
116/143:
import re

# Input string
input_string = 'Гигиена пәніне кіріспе:\n\n1. Қазақстандағы тағам  гигиенасын  негізін қалаушы:\nШарманов Т.Ш.\nАмрин К.Р.;\nСоловьев З.П.\nЧарльз Дарвин\nНеменко Б.А.\n2. Гигиенаның түрлері:\nеңбек гигиенасы;\nэнтомология;\nпаразитология;\nжануарлардың мінез-құлық ғылымы\nэкология.\n3. Биологиялық фактор:\nмикроорганизмдер;\nылғалдылық;\nөсімдіктер, жәндіктер;\nкүн сәулесі.\n4. Гигиена бойынша зерттеу әдістері:\nсанитарлық сипаттама;\nбиохимиялық;\nультрадыбыстық;\nрадиографиялық.'

# Define the pattern for splitting the input string
pattern = r'\d+\..+?(?=\d+\.)'
prefixes = ["A", "B", "C", "D", "E", "F", "G", "H", "I", "J", "K", "L", "M", "N", "O", "P", "Q", "R", "S", "T", "U", "V", "W", "X", "Y", "Z"]
count = 1
for x in ans:
    print(x)
    input_string = ' '.join(x)
#     print(input_string)
    split_strings = re.findall(pattern, input_string)
    
    if count != 3:
        count += 1 
        continue
#     print(split_strings)
    input_string = ' '.join(x)
    print(input_string)
#     print(split_strings)
#     print(input_string.split('\n'))
    
#     break
#     for string in split_strings:


# #         modified_string = f'\nsf '.join(lines)
# #         print(modified_string)
# #         print(string)
#         s = ''.join(string)
#         lines = s.split('\n')
# #         for e in range()
#         print(lines)
#         print(s)
#         break
#         print(s)
#         for i in range(len(s)):
# #             print(s[i])
#             if s[i]=='\n':
#                 s = s[:i] + f'\n{prefixes[cnt]}. ' + s[i+1:]
#                 cnt += 1
                
#         print(s)
# Use regex to split the input string based on the pattern
# split_strings = re.findall(pattern, input_string, re.DOTALL)

# Print the split strings
# a = ""
# for string in split_strings:
#     print(string.strip())
#     break
116/144:
import re

text = "'1', 'Гигиена пәніне кіріспе:\n\n1. Қазақстандағы тағам  гигиенасын  негізін қалаушы:\nШарманов Т.Ш.\nАмрин К.Р.;\nСоловьев З.П.\nЧарльз Дарвин\nНеменко Б.А.\n2. Гигиенаның түрлері:\nеңбек гигиенасы;\nэнтомология;\nпаразитология;\nжануарлардың мінез-құлық ғылымы\nэкология."

# Define a pattern to match the numbered options
pattern = r'\d+\.\s+([\w\s\.\-]+)\s*:\s*\n((?:[A-Z]\.\s+[\w\s\.\-]+\n)+)'

# Find all matches in the text
matches = re.findall(pattern, text)

# Process the matches
for match in matches:
    question = match[0].strip()
    options = match[1].strip().split('\n')
    
    print(question)
    for option in options:
        print(option.strip())
116/145:
import re

text = "'1', 'Гигиена пәніне кіріспе:\n\n1. Қазақстандағы тағам  гигиенасын  негізін қалаушы:\nШарманов Т.Ш.\nАмрин К.Р.;\nСоловьев З.П.\nЧарльз Дарвин\nНеменко Б.А.\n2. Гигиенаның түрлері:\nеңбек гигиенасы;\nэнтомология;\nпаразитология;\nжануарлардың мінез-құлық ғылымы\nэкология."

# Define a pattern to match the numbered options
pattern = r'\d+\.\s+([\w\s\.\-]+)\s*:\s*\n((?:[A-Z]\.\s+[\w\s\.\-]+\n)+)'

# Find all matches in the text
matches = re.findall(pattern, text)

# Process the matches
for match in matches:
    question = match[0].strip()
    options = match[1].strip().split('\n')
    
    print(question)
    for option in options:
        print(option.strip())
116/146:
from docx import Document

def read_word_table(file_path):
    doc = Document('8.docx')
    tables = []
    for table in doc.tables:
        data = []
        for row_idx, row in enumerate(table.rows):
            row_data = []
            for cell_idx, cell in enumerate(row.cells):
                # Check if this is the first cell in the row and if it contains numbering
                if cell_idx == 0:
                    numbering = cell.text.strip()
                else:
                    # If it's not the first cell, prepend the numbering to the cell data
                    row_data.append(f"{numbering} {cell.text.strip()}")
            # Append the row data to the table
            data.append(row_data)
        tables.append(data)
    return tables

# Example usage:
file_path = 'path_to_your_word_file.docx'
tables = read_word_table(file_path)
for table in tables:
    print(table)
116/147:
from docx2python import docx2python
document = docx2python("C:/input/MyDoc.docx")
print(document.body)
116/148: pip install docx2python
116/149:
from docx2python import docx2python
document = docx2python("8.docx")
print(document.body)
116/150:
tables = []
for table in doc.tables:
    # Iterate through each row in the table
    for row in table.rows:
        # Create an empty list to store the data from each cell in the row
        row_data = []
        # Iterate through each cell in the row
        for cell in row.cells:
            # Append the text content of the cell to the row_data list
            row_data.append(cell.text)
            print(cell.text)
        # Print the row data
        tables.append(row_data)

ans = tables[2:-43]
116/151:
tables = []
for table in doc.tables:
    # Iterate through each row in the table
    for row in table.rows:
        # Create an empty list to store the data from each cell in the row
        row_data = []
        # Iterate through each cell in the row
        for cell in row.cells:
            # Append the text content of the cell to the row_data list
            row_data.append(cell.text)
            print(cell.text)
            break
        break
        # Print the row data
        tables.append(row_data)

ans = tables[2:-43]
116/152:
tables = []
cnt = 0
for table in doc.tables:
    # Iterate through each row in the table
    if cnt != 2:
        cnt += 1
        continue
    for row in table.rows:
        # Create an empty list to store the data from each cell in the row
        row_data = []
        # Iterate through each cell in the row
        for cell in row.cells:
            # Append the text content of the cell to the row_data list
            row_data.append(cell.text)
#             print(cell.text)
#             break
#         break
        # Print the row data
        tables.append(row_data)

ans = tables[2:-43]
116/153:
tables = []
cnt = 0
for table in doc.tables:
    # Iterate through each row in the table
    if cnt != 2:
        cnt += 1
        continue
    for row in table.rows:
        # Create an empty list to store the data from each cell in the row
        row_data = []
        # Iterate through each cell in the row
        for cell in row.cells:
            # Append the text content of the cell to the row_data list
            row_data.append(cell.text)
            print(cell.text)
#             break
#         break
        # Print the row data
        tables.append(row_data)

ans = tables[2:-43]
116/154:
tables = []
cnt = 0
for table in doc.tables:
    # Iterate through each row in the table
    if cnt != 3:
        cnt += 1
        continue
    for row in table.rows:
        # Create an empty list to store the data from each cell in the row
        row_data = []
        # Iterate through each cell in the row
        for cell in row.cells:
            # Append the text content of the cell to the row_data list
            row_data.append(cell.text)
            print(cell.text)
#             break
#         break
        # Print the row data
        tables.append(row_data)

ans = tables[2:-43]
116/155:
tables = []
cnt = 0
for table in doc.tables:
    # Iterate through each row in the table
    if cnt != 5:
        cnt += 1
        continue
    for row in table.rows:
        # Create an empty list to store the data from each cell in the row
        row_data = []
        # Iterate through each cell in the row
        for cell in row.cells:
            # Append the text content of the cell to the row_data list
            row_data.append(cell.text)
            print(cell.text)
#             break
#         break
        # Print the row data
        tables.append(row_data)

ans = tables[2:-43]
116/156:
tables = []
cnt = 0
for table in doc.tables:
    # Iterate through each row in the table
    if cnt != 10:
        cnt += 1
        continue
    for row in table.rows:
        # Create an empty list to store the data from each cell in the row
        row_data = []
        # Iterate through each cell in the row
        for cell in row.cells:
            # Append the text content of the cell to the row_data list
            row_data.append(cell.text)
            print(cell.text)
#             break
#         break
        # Print the row data
        tables.append(row_data)

ans = tables[2:-43]
116/157:
tables = []
cnt = 0
for table in doc.tables:
    # Iterate through each row in the table
    
    for row in table.rows:
        print(row.cells)
        break
        # Create an empty list to store the data from each cell in the row
        row_data = []
        # Iterate through each cell in the row
        for cell in row.cells:
            # Append the text content of the cell to the row_data list
            row_data.append(cell.text)
            print(cell.text)
#             break
#         break
        # Print the row data
        tables.append(row_data)

ans = tables[2:-43]
116/158:
tables = []
cnt = 0
for table in doc.tables:
    # Iterate through each row in the table
    
    for row in table.rows:
        # Create an empty list to store the data from each cell in the row
        row_data = []
        if cnt != 2:
            cnt += 1
            continue
        # Iterate through each cell in the row
        for cell in row.cells:
            # Append the text content of the cell to the row_data list
            row_data.append(cell.text)
            print(cell.text)
#             break
#         break
        # Print the row data
        tables.append(row_data)

ans = tables[2:-43]
116/159:
tables = []
cnt = 0
for table in doc.tables:
    # Iterate through each row in the table
    
    for row in table.rows:
        # Create an empty list to store the data from each cell in the row
        row_data = []
        if cnt != 2:
            cnt += 1
            continue
        # Iterate through each cell in the row
        for cell in row.cells:
            # Append the text content of the cell to the row_data list
            row_data.append(cell.text)
            print(cell.text)
            break
#         break
        # Print the row data
        tables.append(row_data)

ans = tables[2:-43]
116/160:
tables = []
cnt = 0
for table in doc.tables:
    # Iterate through each row in the table
    
    for row in table.rows:
        # Create an empty list to store the data from each cell in the row
        row_data = []
        if cnt != 3:
            cnt += 1
            continue
        # Iterate through each cell in the row
        for cell in row.cells:
            # Append the text content of the cell to the row_data list
            row_data.append(cell.text)
            print(cell.text)
            break
#         break
        # Print the row data
        tables.append(row_data)

ans = tables[2:-43]
116/161:
tables = []
cnt = 0
for table in doc.tables:
    # Iterate through each row in the table
    
    for row in table.rows:
        # Create an empty list to store the data from each cell in the row
        row_data = []
        if cnt != 4:
            cnt += 1
            continue
        # Iterate through each cell in the row
        for cell in row.cells:
            # Append the text content of the cell to the row_data list
            row_data.append(cell.text)
            print(cell.text)
            break
#         break
        # Print the row data
        tables.append(row_data)

ans = tables[2:-43]
116/162:
tables = []
cnt = 0
for table in doc.tables:
    # Iterate through each row in the table
    
    for row in table.rows:
        # Create an empty list to store the data from each cell in the row
        row_data = []
        if cnt != 5:
            cnt += 1
            continue
        # Iterate through each cell in the row
        for cell in row.cells:
            # Append the text content of the cell to the row_data list
            row_data.append(cell.text)
            print(cell.text)
            break
#         break
        # Print the row data
        tables.append(row_data)

ans = tables[2:-43]
116/163:
tables = []
cnt = 0
for table in doc.tables:
    # Iterate through each row in the table
    
    for row in table.rows:
        # Create an empty list to store the data from each cell in the row
        row_data = []
        
        # Iterate through each cell in the row
        for cell in row.cells:
            if cnt != 5:
                cnt += 1
                continue
            # Append the text content of the cell to the row_data list
            row_data.append(cell.text)
            print(cell.text)
            break
#         break
        # Print the row data
        tables.append(row_data)

ans = tables[2:-43]
116/164:
tables = []
cnt = 0
for table in doc.tables:
    # Iterate through each row in the table
    
    for row in table.rows:
        # Create an empty list to store the data from each cell in the row
        row_data = []
        
        # Iterate through each cell in the row
        for cell in row.cells:
            if cnt != 10:
                cnt += 1
                continue
            # Append the text content of the cell to the row_data list
            row_data.append(cell.text)
            print(cell.text)
            break
#         break
        # Print the row data
        tables.append(row_data)

ans = tables[2:-43]
116/165:
tables = []
cnt = 0
for table in doc.tables:
    # Iterate through each row in the table
    
    for row in table.rows:
        # Create an empty list to store the data from each cell in the row
        row_data = []
        
        # Iterate through each cell in the row
        for cell in row.cells:
            if cnt != 20:
                cnt += 1
                continue
            # Append the text content of the cell to the row_data list
            row_data.append(cell.text)
            print(cell.text)
            break
#         break
        # Print the row data
        tables.append(row_data)

ans = tables[2:-43]
116/166:
tables = []
cnt = 0
for table in doc.tables:
    # Iterate through each row in the table
    
    for row in table.rows:
        # Create an empty list to store the data from each cell in the row
        row_data = []
        
        # Iterate through each cell in the row
        for cell in row.cells:
            if cnt != 20:
                cnt += 1
                continue
            # Append the text content of the cell to the row_data list
            row_data.append(cell.text)
            print(cell.text)
#             break
#         break
        # Print the row data
        tables.append(row_data)

ans = tables[2:-43]
116/167:
tables = []
cnt = 0
print(len(doc.tables))
for table in doc.tables:
    # Iterate through each row in the table
    
    for row in table.rows:
        # Create an empty list to store the data from each cell in the row
        row_data = []
        
        # Iterate through each cell in the row
        for cell in row.cells:
            if cnt != 20:
                cnt += 1
                continue
            # Append the text content of the cell to the row_data list
            row_data.append(cell.text)
#             print(cell.text)
#             break
#         break
        # Print the row data
        tables.append(row_data)

ans = tables[2:-43]
116/168:
tables = []
cnt = 0
for table in doc.tables:
    # Iterate through each row in the table
    print(len(table))
    for row in table.rows:
        # Create an empty list to store the data from each cell in the row
        row_data = []
        

        # Iterate through each cell in the row
        for cell in row.cells:
            if cnt != 20:
                cnt += 1
                continue
            # Append the text content of the cell to the row_data list
            row_data.append(cell.text)
#             print(cell.text)
#             break
#         break
        # Print the row data
        tables.append(row_data)

ans = tables[2:-43]
116/169:
tables = []
cnt = 0
for table in doc.tables:
    # Iterate through each row in the table
    for row in table.rows:
        # Create an empty list to store the data from each cell in the row
        row_data = []
        

        # Iterate through each cell in the row
        for cell in row.cells:
            # Append the text content of the cell to the row_data list
            row_data.append(cell.text)
            print(cell.text)
#             break
#         break
        # Print the row data
        tables.append(row_data)

ans = tables[2:-43]
116/170:
tables = []
cnt = 0
for table in doc.tables:
    # Iterate through each row in the table
    for row in table.rows:
        # Create an empty list to store the data from each cell in the row
        row_data = []
        

        # Iterate through each cell in the row
        for cell in row.cells:
            # Append the text content of the cell to the row_data list
            row_data.append(cell.text)
            print(cell.text[0])
#             break
#         break
        # Print the row data
        tables.append(row_data)

ans = tables[2:-43]
116/171:
tables = []
cnt = 0
for table in doc.tables:
    # Iterate through each row in the table
    for row in table.rows:
        # Create an empty list to store the data from each cell in the row
        row_data = []
        

        # Iterate through each cell in the row
        for cell in row.cells:
            # Append the text content of the cell to the row_data list
            row_data.append(cell.text)
            print(cell.text)
#             break
#         break
        # Print the row data
        tables.append(row_data)

ans = tables[2:-43]
116/172:
doc = Document('8.docx')
tables = []
for table in doc.tables:
    table_data = []
    for row in table.rows:
        row_data = []
        for cell in row.cells:
            row_data.append(cell.text)
        table_data.append(row_data)
    tables.append(table_data)
print(tables)
116/173:
import re

# Input string
input_string = 'Гигиена пәніне кіріспе:\n\n1. Қазақстандағы тағам  гигиенасын  негізін қалаушы:\nШарманов Т.Ш.\nАмрин К.Р.;\nСоловьев З.П.\nЧарльз Дарвин\nНеменко Б.А.\n2. Гигиенаның түрлері:\nеңбек гигиенасы;\nэнтомология;\nпаразитология;\nжануарлардың мінез-құлық ғылымы\nэкология.\n3. Биологиялық фактор:\nмикроорганизмдер;\nылғалдылық;\nөсімдіктер, жәндіктер;\nкүн сәулесі.\n4. Гигиена бойынша зерттеу әдістері:\nсанитарлық сипаттама;\nбиохимиялық;\nультрадыбыстық;\nрадиографиялық.'

# Define the pattern for splitting the input string
pattern = r'\d+\..+?(?=\d+\.)'
prefixes = ["A", "B", "C", "D", "E", "F", "G", "H", "I", "J", "K", "L", "M", "N", "O", "P", "Q", "R", "S", "T", "U", "V", "W", "X", "Y", "Z"]
count = 1
for x in ans:
#     print(x)
    input_string = ' '.join(x)
#     print(input_string)
    split_strings = re.findall(pattern, input_string, re.DOTALL)
    
    for string in split_strings:


        for i in range(len(s)):
            if s[i]=='\n':
                s = s[:i] + f'\n{prefixes[cnt]}. ' + s[i+1:]
                cnt += 1
                
        print(s)
# Use regex to split the input string based on the pattern
# split_strings = re.findall(pattern, input_string, re.DOTALL)

# Print the split strings
# a = ""
# for string in split_strings:
#     print(string.strip())
#     break
116/174:
tables = []
cnt = 0
for table in doc.tables:
    # Iterate through each row in the table
    for row in table.rows:
        # Create an empty list to store the data from each cell in the row
        row_data = []
        

        # Iterate through each cell in the row
        for cell in row.cells:
            # Append the text content of the cell to the row_data list
            row_data.append(cell.text)
            print(cell.text)
#             break
#         break
        # Print the row data
        tables.append(row_data)

ans = tables[2:-43]
116/175:
import re

# Input string
input_string = 'Гигиена пәніне кіріспе:\n\n1. Қазақстандағы тағам  гигиенасын  негізін қалаушы:\nШарманов Т.Ш.\nАмрин К.Р.;\nСоловьев З.П.\nЧарльз Дарвин\nНеменко Б.А.\n2. Гигиенаның түрлері:\nеңбек гигиенасы;\nэнтомология;\nпаразитология;\nжануарлардың мінез-құлық ғылымы\nэкология.\n3. Биологиялық фактор:\nмикроорганизмдер;\nылғалдылық;\nөсімдіктер, жәндіктер;\nкүн сәулесі.\n4. Гигиена бойынша зерттеу әдістері:\nсанитарлық сипаттама;\nбиохимиялық;\nультрадыбыстық;\nрадиографиялық.'

# Define the pattern for splitting the input string
pattern = r'\d+\..+?(?=\d+\.)'
prefixes = ["A", "B", "C", "D", "E", "F", "G", "H", "I", "J", "K", "L", "M", "N", "O", "P", "Q", "R", "S", "T", "U", "V", "W", "X", "Y", "Z"]
count = 1
for x in ans:
#     print(x)
    input_string = ' '.join(x)
#     print(input_string)
    split_strings = re.findall(pattern, input_string, re.DOTALL)
    
    for string in split_strings:


        for i in range(len(s)):
            if s[i]=='\n':
                s = s[:i] + f'\n{prefixes[cnt]}. ' + s[i+1:]
                cnt += 1
                
        print(s)
# Use regex to split the input string based on the pattern
# split_strings = re.findall(pattern, input_string, re.DOTALL)

# Print the split strings
# a = ""
# for string in split_strings:
#     print(string.strip())
#     break
116/176:
import re

# Input string
input_string = 'Гигиена пәніне кіріспе:\n\n1. Қазақстандағы тағам  гигиенасын  негізін қалаушы:\nШарманов Т.Ш.\nАмрин К.Р.;\nСоловьев З.П.\nЧарльз Дарвин\nНеменко Б.А.\n2. Гигиенаның түрлері:\nеңбек гигиенасы;\nэнтомология;\nпаразитология;\nжануарлардың мінез-құлық ғылымы\nэкология.\n3. Биологиялық фактор:\nмикроорганизмдер;\nылғалдылық;\nөсімдіктер, жәндіктер;\nкүн сәулесі.\n4. Гигиена бойынша зерттеу әдістері:\nсанитарлық сипаттама;\nбиохимиялық;\nультрадыбыстық;\nрадиографиялық.'

# Define the pattern for splitting the input string
pattern = r'\d+\..+?(?=\d+\.)'
prefixes = ["A", "B", "C", "D", "E", "F", "G", "H", "I", "J", "K", "L", "M", "N", "O", "P", "Q", "R", "S", "T", "U", "V", "W", "X", "Y", "Z"]
count = 1
for x in ans:
#     print(x)
    input_string = ' '.join(x)
#     print(input_string)
    split_strings = re.findall(pattern, input_string, re.DOTALL)
    
    for string in split_strings:
        s = string

        for i in range(len(s)):
            if s[i]=='\n':
                s = s[:i] + f'\n{prefixes[cnt]}. ' + s[i+1:]
                cnt += 1
                
        print(s)
# Use regex to split the input string based on the pattern
# split_strings = re.findall(pattern, input_string, re.DOTALL)

# Print the split strings
# a = ""
# for string in split_strings:
#     print(string.strip())
#     break
116/177:
import re

# Input string
input_string = 'Гигиена пәніне кіріспе:\n\n1. Қазақстандағы тағам  гигиенасын  негізін қалаушы:\nШарманов Т.Ш.\nАмрин К.Р.;\nСоловьев З.П.\nЧарльз Дарвин\nНеменко Б.А.\n2. Гигиенаның түрлері:\nеңбек гигиенасы;\nэнтомология;\nпаразитология;\nжануарлардың мінез-құлық ғылымы\nэкология.\n3. Биологиялық фактор:\nмикроорганизмдер;\nылғалдылық;\nөсімдіктер, жәндіктер;\nкүн сәулесі.\n4. Гигиена бойынша зерттеу әдістері:\nсанитарлық сипаттама;\nбиохимиялық;\nультрадыбыстық;\nрадиографиялық.'

# Define the pattern for splitting the input string
pattern = r'\d+\..+?(?=\d+\.)'
prefixes = ["A", "B", "C", "D", "E", "F", "G", "H", "I", "J", "K", "L", "M", "N", "O", "P", "Q", "R", "S", "T", "U", "V", "W", "X", "Y", "Z"]
count = 1
for x in ans:
#     print(x)
    input_string = ' '.join(x)
#     print(input_string)
    split_strings = re.findall(pattern, input_string, re.DOTALL)
    cnt = 0
    for string in split_strings:
        s = string

        for i in range(len(s)):
            if s[i]=='\n':
                s = s[:i] + f'\n{prefixes[cnt]}. ' + s[i+1:]
                cnt += 1
                
        print(s)
# Use regex to split the input string based on the pattern
# split_strings = re.findall(pattern, input_string, re.DOTALL)

# Print the split strings
# a = ""
# for string in split_strings:
#     print(string.strip())
#     break
116/178:
import re

# Input string
input_string = 'Гигиена пәніне кіріспе:\n\n1. Қазақстандағы тағам  гигиенасын  негізін қалаушы:\nШарманов Т.Ш.\nАмрин К.Р.;\nСоловьев З.П.\nЧарльз Дарвин\nНеменко Б.А.\n2. Гигиенаның түрлері:\nеңбек гигиенасы;\nэнтомология;\nпаразитология;\nжануарлардың мінез-құлық ғылымы\nэкология.\n3. Биологиялық фактор:\nмикроорганизмдер;\nылғалдылық;\nөсімдіктер, жәндіктер;\nкүн сәулесі.\n4. Гигиена бойынша зерттеу әдістері:\nсанитарлық сипаттама;\nбиохимиялық;\nультрадыбыстық;\nрадиографиялық.'

# Define the pattern for splitting the input string
pattern = r'\d+\..+?(?=\d+\.)'
prefixes = ["A", "B", "C", "D", "E", "F", "G", "H", "I", "J", "K", "L", "M", "N", "O", "P", "Q", "R", "S", "T", "U", "V", "W", "X", "Y", "Z"]
count = 1
for x in ans:
#     print(x)
    input_string = ' '.join(x)
#     print(input_string)
    split_strings = re.findall(pattern, input_string, re.DOTALL)
    for string in split_strings:
        s = string
        cnt = 0
        for i in range(len(s)):
            if s[i]=='\n':
                s = s[:i] + f'\n{prefixes[cnt]}. ' + s[i+1:]
                cnt += 1
                
        print(s)
# Use regex to split the input string based on the pattern
# split_strings = re.findall(pattern, input_string, re.DOTALL)

# Print the split strings
# a = ""
# for string in split_strings:
#     print(string.strip())
#     break
116/179:
from docx import Document

def read_numbered_lists_from_tables(docx_file):
    doc = Document(docx_file)
    numbered_lists = []
    for table in doc.tables:
        for row in table.rows:
            for cell in row.cells:
                for paragraph in cell.paragraphs:
                    if paragraph.style.name.startswith('List Number'):
                        numbered_lists.append(paragraph.text)
    return numbered_lists

# Example usage:
docx_file = "example.docx"  # Change this to your Word document file path
numbered_lists = read_numbered_lists_from_tables(docx_file)
for i, item in enumerate(numbered_lists):
    print(f"{i+1}. {item}")
116/180:
from docx import Document

def read_numbered_lists_from_tables(docx_file):
    doc = Document(docx_file)
    numbered_lists = []
    for table in doc.tables:
        for row in table.rows:
            for cell in row.cells:
                for paragraph in cell.paragraphs:
                    if paragraph.style.name.startswith('List Number'):
                        numbered_lists.append(paragraph.text)
    return numbered_lists

# Example usage:
docx_file = "8.docx"  # Change this to your Word document file path
numbered_lists = read_numbered_lists_from_tables(docx_file)
for i, item in enumerate(numbered_lists):
    print(f"{i+1}. {item}")
116/181:
from docx import Document

def read_numbered_lists_from_tables(docx_file):
    doc = Document(8.docx)
    numbered_lists = []
    for table in doc.tables:
        for row in table.rows:
            for cell in row.cells:
                for paragraph in cell.paragraphs:
                    if paragraph.style.name.startswith('List Number'):
                        numbered_lists.append(paragraph.text)
    return numbered_lists

# Example usage:
docx_file = "8.docx"  # Change this to your Word document file path
numbered_lists = read_numbered_lists_from_tables(docx_file)
for i, item in enumerate(numbered_lists):
    print(f"{i+1}. {item}")
116/182:
from docx import Document

def read_numbered_lists_from_tables(docx_file):
    doc = Document('8.docx')
    numbered_lists = []
    for table in doc.tables:
        for row in table.rows:
            for cell in row.cells:
                for paragraph in cell.paragraphs:
                    if paragraph.style.name.startswith('List Number'):
                        numbered_lists.append(paragraph.text)
    return numbered_lists

# Example usage:
docx_file = "8.docx"  # Change this to your Word document file path
numbered_lists = read_numbered_lists_from_tables(docx_file)
for i, item in enumerate(numbered_lists):
    print(f"{i+1}. {item}")
116/183:
from docx import Document

def read_numbered_lists_from_tables(docx_file):
    doc = Document('8.docx')
    numbered_lists = []
    for table in doc.tables:
        for row in table.rows:
            for cell in row.cells:
                for paragraph in cell.paragraphs:
                    if paragraph.style.name.startswith('List Number'):
                        numbered_lists.append(paragraph.text)
    return numbered_lists

# Example usage:
docx_file = "88.docx"  # Change this to your Word document file path
numbered_lists = read_numbered_lists_from_tables(docx_file)
for i, item in enumerate(numbered_lists):
    print(f"{i+1}. {item}")
116/184:
import re
res = []
# Input string
input_string = 'Гигиена пәніне кіріспе:\n\n1. Қазақстандағы тағам  гигиенасын  негізін қалаушы:\nШарманов Т.Ш.\nАмрин К.Р.;\nСоловьев З.П.\nЧарльз Дарвин\nНеменко Б.А.\n2. Гигиенаның түрлері:\nеңбек гигиенасы;\nэнтомология;\nпаразитология;\nжануарлардың мінез-құлық ғылымы\nэкология.\n3. Биологиялық фактор:\nмикроорганизмдер;\nылғалдылық;\nөсімдіктер, жәндіктер;\nкүн сәулесі.\n4. Гигиена бойынша зерттеу әдістері:\nсанитарлық сипаттама;\nбиохимиялық;\nультрадыбыстық;\nрадиографиялық.'

# Define the pattern for splitting the input string
pattern = r'\d+\..+?(?=\d+\.)'
prefixes = ["A", "B", "C", "D", "E", "F", "G", "H", "I", "J", "K", "L", "M", "N", "O", "P", "Q", "R", "S", "T", "U", "V", "W", "X", "Y", "Z"]

for x in ans:
#     print(x)
    input_string = ' '.join(x)
#     print(input_string)
    split_strings = re.findall(pattern, input_string, re.DOTALL)
    for string in split_strings:
        s = string
        cnt = 0
        for i in range(len(s)):
            if s[i]=='\n':
                s = s[:i] + f'\n{prefixes[cnt]}. ' + s[i+1:]
                cnt += 1
                
#         print(s)
        res.append(s)
116/185: res
116/186:
from docx import Document

# Create a new Document
doc = Document()

# Add a table with 3 rows and 3 columns
table = doc.add_table(rows=3, cols=3)

# Iterate through each cell in the table and set some sample data
for i in range(len(res)):
    cell = table.cell(i, 0)
    cell.text = res[i]
    cell = table.cell(i, 1)
    cell.text = ""
# for i in range(3):
        



doc.save('example_table.docx')
116/187:
from docx import Document

# Create a new Document
doc = Document()

# Add a table with 3 rows and 3 columns
table = doc.add_table(rows=len(res), cols=2)

# Iterate through each cell in the table and set some sample data
for i in range(len(res)):
    cell = table.cell(i, 0)
    cell.text = res[i]
    cell = table.cell(i, 1)
    cell.text = ""
# for i in range(3):
        



doc.save('example_table.docx')
116/188:
import re
res = []
# Input string
input_string = 'Гигиена пәніне кіріспе:\n\n1. Қазақстандағы тағам  гигиенасын  негізін қалаушы:\nШарманов Т.Ш.\nАмрин К.Р.;\nСоловьев З.П.\nЧарльз Дарвин\nНеменко Б.А.\n2. Гигиенаның түрлері:\nеңбек гигиенасы;\nэнтомология;\nпаразитология;\nжануарлардың мінез-құлық ғылымы\nэкология.\n3. Биологиялық фактор:\nмикроорганизмдер;\nылғалдылық;\nөсімдіктер, жәндіктер;\nкүн сәулесі.\n4. Гигиена бойынша зерттеу әдістері:\nсанитарлық сипаттама;\nбиохимиялық;\nультрадыбыстық;\nрадиографиялық.'

# Define the pattern for splitting the input string
pattern = r'\d+\..+?(?=\d+\.)'
prefixes = ["A", "B", "C", "D", "E", "F", "G", "H", "I", "J", "K", "L", "M", "N", "O", "P", "Q", "R", "S", "T", "U", "V", "W", "X", "Y", "Z"]
count = 43
for x in ans:
#     print(x)
    input_string = ' '.join(x)
#     print(input_string)
    split_strings = re.findall(pattern, input_string, re.DOTALL)
    for string in split_strings:
        s = string
        cnt = 0
        for i in range(len(s)):
            if s[i]=='\n':
                s = s[:i] + f'\n{prefixes[cnt]}. ' + s[i+1:]
                cnt += 1
        p = r'\d+'
        match = re.search(p, s)        
         if match:
            start, end = match.span()
            modified_string = s[:start] + 'sf' + s[end:]
            print(modified_string)
        res.append(s)
116/189:
import re
res = []
# Input string
input_string = 'Гигиена пәніне кіріспе:\n\n1. Қазақстандағы тағам  гигиенасын  негізін қалаушы:\nШарманов Т.Ш.\nАмрин К.Р.;\nСоловьев З.П.\nЧарльз Дарвин\nНеменко Б.А.\n2. Гигиенаның түрлері:\nеңбек гигиенасы;\nэнтомология;\nпаразитология;\nжануарлардың мінез-құлық ғылымы\nэкология.\n3. Биологиялық фактор:\nмикроорганизмдер;\nылғалдылық;\nөсімдіктер, жәндіктер;\nкүн сәулесі.\n4. Гигиена бойынша зерттеу әдістері:\nсанитарлық сипаттама;\nбиохимиялық;\nультрадыбыстық;\nрадиографиялық.'

# Define the pattern for splitting the input string
pattern = r'\d+\..+?(?=\d+\.)'
prefixes = ["A", "B", "C", "D", "E", "F", "G", "H", "I", "J", "K", "L", "M", "N", "O", "P", "Q", "R", "S", "T", "U", "V", "W", "X", "Y", "Z"]
count = 43
for x in ans:
#     print(x)
    input_string = ' '.join(x)
#     print(input_string)
    split_strings = re.findall(pattern, input_string, re.DOTALL)
    for string in split_strings:
        s = string
        cnt = 0
        for i in range(len(s)):
            if s[i]=='\n':
                s = s[:i] + f'\n{prefixes[cnt]}. ' + s[i+1:]
                cnt += 1
        p = r'\d+'
        match = re.search(p, s)        
        if match:
            start, end = match.span()
            modified_string = s[:start] + 'sf' + s[end:]
            print(modified_string)
        res.append(s)
116/190:
import re
res = []
# Input string
input_string = 'Гигиена пәніне кіріспе:\n\n1. Қазақстандағы тағам  гигиенасын  негізін қалаушы:\nШарманов Т.Ш.\nАмрин К.Р.;\nСоловьев З.П.\nЧарльз Дарвин\nНеменко Б.А.\n2. Гигиенаның түрлері:\nеңбек гигиенасы;\nэнтомология;\nпаразитология;\nжануарлардың мінез-құлық ғылымы\nэкология.\n3. Биологиялық фактор:\nмикроорганизмдер;\nылғалдылық;\nөсімдіктер, жәндіктер;\nкүн сәулесі.\n4. Гигиена бойынша зерттеу әдістері:\nсанитарлық сипаттама;\nбиохимиялық;\nультрадыбыстық;\nрадиографиялық.'

# Define the pattern for splitting the input string
pattern = r'\d+\..+?(?=\d+\.)'
prefixes = ["A", "B", "C", "D", "E", "F", "G", "H", "I", "J", "K", "L", "M", "N", "O", "P", "Q", "R", "S", "T", "U", "V", "W", "X", "Y", "Z"]
count = 43
for x in ans:
#     print(x)
    input_string = ' '.join(x)
#     print(input_string)
    split_strings = re.findall(pattern, input_string, re.DOTALL)
    for string in split_strings:
        s = string
        cnt = 0
        for i in range(len(s)):
            if s[i]=='\n':
                s = s[:i] + f'\n{prefixes[cnt]}. ' + s[i+1:]
                cnt += 1
        p = r'\d+'
        match = re.search(p, s)        
        if match:
            start, end = match.span()
            modified_string = s[:start] + f'{count}' + s[end:]
            count += 1
#             print(modified_string)
        res.append(modified_string)
116/191: res
116/192:
from docx import Document

# Create a new Document
doc = Document()

# Add a table with 3 rows and 3 columns
table = doc.add_table(rows=len(res), cols=2)

# Iterate through each cell in the table and set some sample data
for i in range(len(res)):
    cell = table.cell(i, 0)
    cell.text = res[i]
    cell = table.cell(i, 1)
    cell.text = ""
# for i in range(3):
        



doc.save('example_table.docx')
116/193:
from docx import Document

# Create a new Document
doc = Document()

# Add a table with 3 rows and 3 columns
table = doc.add_table(rows=len(res), cols=2)

# Iterate through each cell in the table and set some sample data
for i in range(len(res)):
    cell = table.cell(i, 0)
    cell.text = res[i]
    cell = table.cell(i, 1)
    cell.text = ""
# for i in range(3):
        



doc.save('example_table.docx')
116/194:
from docx import Document

# Sample data
res = ["This is regular text.", "This is some bold text.", "This is another regular text."]

# Create a new Document
doc = Document()

# Add a table with the same number of rows as res and 2 columns
table = doc.add_table(rows=len(res), cols=2)

# Iterate through each cell in the table and set some sample data
for i in range(len(res)):
    cell = table.cell(i, 0)
    cell_paragraph = cell.add_paragraph()
    # Split the text to make part of it bold
    parts = res[i].split("bold")
    for part in parts:
        cell_paragraph.add_run(part)  # Add the part
        if "bold" in part:
            cell_paragraph.add_run("bold").bold = True  # Make only the part containing "bold" bold
    cell = table.cell(i, 1)
    cell_paragraph = cell.add_paragraph()
    # You can add any text here, as an example, I'm leaving it blank
    # If you want to make this text bold as well, just use cell_paragraph.add_run("Your text here").bold = True
    cell_paragraph.add_run("").bold = True  # Make text bold

doc.save('asf.docx')
116/195:
from docx import Document

# Create a new Document
doc = Document()

# Add a table with 3 rows and 3 columns
table = doc.add_table(rows=len(res), cols=2)

# Iterate through each cell in the table and set some sample data
for i in range(len(res)):
    cell = table.cell(i, 0)
    cell.text = res[i]
#     p = doc.add_paragraph()
    print(res[i])
#     runner = p.add_run(word)
#     runner.bold = True
    cell = table.cell(i, 1)
    cell.text = ""
# for i in range(3):
        



doc.save('example_table.docx')
116/196:
import re
res = []
# Input string
input_string = 'Гигиена пәніне кіріспе:\n\n1. Қазақстандағы тағам  гигиенасын  негізін қалаушы:\nШарманов Т.Ш.\nАмрин К.Р.;\nСоловьев З.П.\nЧарльз Дарвин\nНеменко Б.А.\n2. Гигиенаның түрлері:\nеңбек гигиенасы;\nэнтомология;\nпаразитология;\nжануарлардың мінез-құлық ғылымы\nэкология.\n3. Биологиялық фактор:\nмикроорганизмдер;\nылғалдылық;\nөсімдіктер, жәндіктер;\nкүн сәулесі.\n4. Гигиена бойынша зерттеу әдістері:\nсанитарлық сипаттама;\nбиохимиялық;\nультрадыбыстық;\nрадиографиялық.'

# Define the pattern for splitting the input string
pattern = r'\d+\..+?(?=\d+\.)'
prefixes = ["A", "B", "C", "D", "E", "F", "G", "H", "I", "J", "K", "L", "M", "N", "O", "P", "Q", "R", "S", "T", "U", "V", "W", "X", "Y", "Z"]
count = 43
for x in ans:
#     print(x)
    input_string = ' '.join(x)
#     print(input_string)
    split_strings = re.findall(pattern, input_string, re.DOTALL)
    for string in split_strings:
        s = string
        cnt = 0
        for i in range(len(s)):
            if s[i]=='\n':
                s = s[:i] + f'\n{prefixes[cnt]}. ' + s[i+1:]
                cnt += 1
        p = r'\d+'
        match = re.search(p, s)        
        if match:
            start, end = match.span()
            modified_string = s[:start] + f'{count}' + s[end:]
            count += 1
#             print(modified_string)
        res.append(modified_string)
116/197: res
116/198:
from docx import Document

# Create a new Document
doc = Document()

# Add a table with 3 rows and 3 columns
table = doc.add_table(rows=len(res), cols=2)

# Iterate through each cell in the table and set some sample data
for i in range(len(res)):
    cell = table.cell(i, 0)
    cell.text = res[i]
#     p = doc.add_paragraph()
    print(res[i])
#     runner = p.add_run(word)
#     runner.bold = True
    cell = table.cell(i, 1)
    cell.text = ""
# for i in range(3):
        



doc.save('example_table.docx')
116/199:
from docx import Document

# Create a new Document
doc = Document()

# Add a table with 3 rows and 3 columns
table = doc.add_table(rows=len(res), cols=2)

# Iterate through each cell in the table and set some sample data
for i in range(len(res)):
    cell = table.cell(i, 0)
    cell.text = res[i]
    match = re.search(r'A\. (\S+ \S+\.)', res[i])

    if match:
        option_a = match.group(1)
        print("Option A:", option_a)
#     p = doc.add_paragraph()
#     print(res[i])
#     runner = p.add_run(word)
#     runner.bold = True
    cell = table.cell(i, 1)
    cell.text = ""
# for i in range(3):
        



doc.save('example_table.docx')
116/200:
from docx import Document

# Create a new Document
doc = Document()

# Add a table with 3 rows and 3 columns
table = doc.add_table(rows=len(res), cols=2)

# Iterate through each cell in the table and set some sample data
for i in range(len(res)):
    cell = table.cell(i, 0)
    cell.text = res[i]
    match = re.search(r'A\. (\S+ \S+\.)', res[i])
    print(res[i])
    if match:
        option_a = match.group(1)
        print("Option A:", option_a)
#     p = doc.add_paragraph()
#     print(res[i])
#     runner = p.add_run(word)
#     runner.bold = True
    cell = table.cell(i, 1)
    cell.text = ""
# for i in range(3):
        



doc.save('example_table.docx')
116/201: res
116/202:
from docx import Document

# Create a new Document
doc = Document()

# Add a table with 3 rows and 3 columns
table = doc.add_table(rows=len(res), cols=2)

# Iterate through each cell in the table and set some sample data
for i in range(len(res)):
    cell = table.cell(i, 0)
    cell.text = res[i]
    pattern = r'\bA\.\s.*'

# Extract strings containing variant A
    variant_A_strings = [s for s in strings if re.search(pattern, res[i])]

    # Print the extracted strings
    for string in variant_A_strings:
        print(string)
#     p = doc.add_paragraph()
#     print(res[i])
#     runner = p.add_run(word)
#     runner.bold = True
    cell = table.cell(i, 1)
    cell.text = ""
# for i in range(3):
        



doc.save('example_table.docx')
116/203:
from docx import Document

# Create a new Document
doc = Document()

# Add a table with 3 rows and 3 columns
table = doc.add_table(rows=len(res), cols=2)

# Iterate through each cell in the table and set some sample data
for i in range(len(res)):
    cell = table.cell(i, 0)
    cell.text = res[i]
    pattern = r'\bA\.\s.*'

# Extract strings containing variant A
    variant_A_strings = [s for s in res[i] if re.search(pattern, res[i])]

    # Print the extracted strings
    for string in variant_A_strings:
        print(string)
#     p = doc.add_paragraph()
#     print(res[i])
#     runner = p.add_run(word)
#     runner.bold = True
    cell = table.cell(i, 1)
    cell.text = ""
# for i in range(3):
        



doc.save('example_table.docx')
116/204:
from docx import Document

# Create a new Document
doc = Document()

# Add a table with 3 rows and 3 columns
table = doc.add_table(rows=len(res), cols=2)

# Iterate through each cell in the table and set some sample data
for i in range(len(res)):
    cell = table.cell(i, 0)
    cell.text = res[i]
    pattern = r'\bA\.\s.*'

# Extract strings containing variant A
    variant_A_strings = [s for s in res if re.search(pattern, res[i])]

    # Print the extracted strings
    for string in variant_A_strings:
        print(string)
#     p = doc.add_paragraph()
#     print(res[i])
#     runner = p.add_run(word)
#     runner.bold = True
    cell = table.cell(i, 1)
    cell.text = ""
# for i in range(3):
        



doc.save('example_table.docx')
116/205:
from docx import Document

# Create a new Document
doc = Document()

# Add a table with 3 rows and 3 columns
table = doc.add_table(rows=len(res), cols=2)

# Iterate through each cell in the table and set some sample data
for i in range(len(res)):
    cell = table.cell(i, 0)
    cell.text = res[i]
    pattern = r'A\.\s[\p{Cyrillic}\s.]+'

# Extract strings containing variant A

    # Print the extracted strings
    variant_A_texts = [re.search(pattern, s).group(0) for s in res if re.search(pattern, s)]

# Print the extracted texts
    for text in variant_A_texts:
        print(text)
#     p = doc.add_paragraph()
#     print(res[i])
#     runner = p.add_run(word)
#     runner.bold = True
    cell = table.cell(i, 1)
    cell.text = ""
# for i in range(3):
        



doc.save('example_table.docx')
116/206:
import re

# Example string
text = "Қазақстандағы тағам гигиенасын негізін қалаушы: A. Шарманов Т.Ш. B. Амрин К.Р.; C. Соловьев З.П. D. Чарльз Дарвин E. Неменко Б.А."

# Regular expression pattern to match variant A
pattern = r'A\.\s[А-ЯҮӨҰҚІЁҒҢҺЦҚЭа-яүөұқіёғңһцқэ\s.]+'

# Find all occurrences of variant A
matches = re.findall(pattern, text)

# Output the result
if matches:
    print("Variant A found:", matches[0])
116/207:
from docx import Document

# Create a new Document
doc = Document()

# Add a table with 3 rows and 3 columns
table = doc.add_table(rows=len(res), cols=2)

# Iterate through each cell in the table and set some sample data
for i in range(len(res)):
    cell = table.cell(i, 0)
    cell.text = res[i]
    pattern = r'A\.\s[А-ЯҮӨҰҚІЁҒҢҺЦҚЭа-яүөұқіёғңһцқэ\s.]+'

# Find all occurrences of variant A
    matches = re.findall(pattern, res[i])

    # Output the result
    if matches:
        print("Variant A found:", matches[0])
#     p = doc.add_paragraph()
#     print(res[i])
#     runner = p.add_run(word)
#     runner.bold = True
    cell = table.cell(i, 1)
    cell.text = ""
# for i in range(3):
        



doc.save('example_table.docx')
116/208:
from docx import Document

# Create a new Document
doc = Document()

# Add a table with 3 rows and 3 columns
table = doc.add_table(rows=len(res), cols=2)

# Iterate through each cell in the table and set some sample data
for i in range(len(res)):
    cell = table.cell(i, 0)
    cell.text = res[i]
    pattern = r'A\.\s[А-ЯҮӨҰҚІЁҒҢҺЦҚЭа-яүөұқіёғңһцқэ\s.]+'

# Find all occurrences of variant A
    matches = re.findall(pattern, res[i])

    # Output the result
    if matches:
        variant_A = matches[0]
        p = doc.add_paragraph()
    #     print(res[i])
        runner = p.add_run(variant_A)
        runner.bold = True
    cell = table.cell(i, 1)
    cell.text = ""
# for i in range(3):
        



doc.save('example_table.docx')
116/209:
from docx import Document

# Create a new Document
doc = Document()

# Add a table with 3 rows and 3 columns
table = doc.add_table(rows=len(res), cols=2)

# Iterate through each cell in the table and set some sample data
for i in range(len(res)):
    cell = table.cell(i, 0)
    cell.text = res[i]
    pattern = r'A\.\s[А-ЯҮӨҰҚІЁҒҢҺЦҚЭа-яүөұқіёғңһцқэ\s.]+'

# Find all occurrences of variant A
    matches = re.findall(pattern, res[i])

    # Output the result
    if matches:
        variant_A = matches[0]
        p = doc.add_paragraph()
    #     print(res[i])
        runner = p.add_run(variant_A)
        runner.bold = True
    cell = table.cell(i, 1)
    cell.text = ""
# for i in range(3):
        



doc.save('example_table.docx')
116/210:
from docx import Document

# Create a new Document
doc = Document()

# Add a table with 3 rows and 3 columns
table = doc.add_table(rows=len(res), cols=2)

# Iterate through each cell in the table and set some sample data
for i in range(len(res)):
    
    pattern = r'A\.\s[А-ЯҮӨҰҚІЁҒҢҺЦҚЭа-яүөұқіёғңһцқэ\s.]+'

# Find all occurrences of variant A
    matches = re.findall(pattern, res[i])

    # Output the result
    if matches:
        variant_A = matches[0]
        p = doc.add_paragraph()
    #     print(res[i])
        runner = p.add_run(variant_A)
        runner.bold = True
    cell = table.cell(i, 0)
    cell.text = res[i]
    cell = table.cell(i, 1)
    cell.text = ""
# for i in range(3):
        



doc.save('example_table.docx')
116/211:
from docx import Document

# Create a new Document
doc = Document()

# Add a table with 3 rows and 3 columns
table = doc.add_table(rows=len(res), cols=2)

# Iterate through each cell in the table and set some sample data
for i in range(len(res)):
    
    pattern = r'A\.\s[А-ЯҮӨҰҚІЁҒҢҺЦҚЭа-яүөұқіёғңһцқэ\s.]+'

# Find all occurrences of variant A
    matches = re.findall(pattern, res[i])

    # Output the result
    if matches:
        if matches:
        variant_A = matches[0]
        cell = table.cell(i, 0)
        paragraph = cell.add_paragraph()
        paragraph.add_run(variant_A).bold = True
    cell = table.cell(i, 0)
    cell.text = res[i]
    cell = table.cell(i, 1)
    cell.text = ""
# for i in range(3):
        



doc.save('example_table.docx')
116/212:
from docx import Document

# Create a new Document
doc = Document()

# Add a table with 3 rows and 3 columns
table = doc.add_table(rows=len(res), cols=2)

# Iterate through each cell in the table and set some sample data
for i in range(len(res)):
    
    pattern = r'A\.\s[А-ЯҮӨҰҚІЁҒҢҺЦҚЭа-яүөұқіёғңһцқэ\s.]+'

# Find all occurrences of variant A
    matches = re.findall(pattern, res[i])

    # Output the result
    if matches:
        variant_A = matches[0]
        cell = table.cell(i, 0)
        paragraph = cell.add_paragraph()
        paragraph.add_run(variant_A).bold = True
    cell = table.cell(i, 0)
    cell.text = res[i]
    cell = table.cell(i, 1)
    cell.text = ""
# for i in range(3):
        



doc.save('example_table.docx')
116/213:
from docx import Document

# Create a new Document
doc = Document()

# Add a table with 3 rows and 3 columns
table = doc.add_table(rows=len(res), cols=2)

# Iterate through each cell in the table and set some sample data
for i in range(len(res)):
    
    pattern = r'A\.\s[А-ЯҮӨҰҚІЁҒҢҺЦҚЭа-яүөұқіёғңһцқэ\s.]+'

# Find all occurrences of variant A
    matches = re.findall(pattern, res[i])

    # Output the result
    if matches:
        variant_A = matches[0]
        cell = table.cell(i, 0)
        paragraph = cell.add_paragraph()
        paragraph.add_run(variant_A).bold = True
    cell = table.cell(i, 0)
    cell.text = res[i]
    cell = table.cell(i, 1)
    cell.text = ""
# for i in range(3):
        



doc.save('res.docx')
116/214:
from docx import Document

# Create a new Document
doc = Document()

# Add a table with 3 rows and 3 columns
table = doc.add_table(rows=len(res), cols=2)

# Iterate through each cell in the table and set some sample data
for i in range(len(res)):
    
    pattern = r'A\.\s[А-ЯҮӨҰҚІЁҒҢҺЦҚЭа-яүөұқіёғңһцқэ\s.]+'

# Find all occurrences of variant A
    matches = re.findall(pattern, res[i])

    # Output the result
    if matches:
        variant_A = matches[0]
        cell = table.cell(i, 0)
        paragraph = cell.add_paragraph()
        paragraph.add_run(variant_A).bold = True
#     cell = table.cell(i, 0)
    cell.text = res[i]
    cell = table.cell(i, 1)
    cell.text = ""
# for i in range(3):
        



doc.save('res.docx')
116/215:
from docx import Document

# Create a new Document
doc = Document()

# Add a table with 3 rows and 3 columns
table = doc.add_table(rows=len(res), cols=2)

# Iterate through each cell in the table and set some sample data
for i in range(len(res)):
    
    pattern = r'A\.\s[А-ЯҮӨҰҚІЁҒҢҺЦҚЭа-яүөұқіёғңһцқэ\s.]+'

# Find all occurrences of variant A
    matches = re.findall(pattern, res[i])

    # Output the result
    if matches:
        variant_A = matches[0]
        cell = table.cell(i, 0)
        paragraph = cell.add_paragraph()
        paragraph.add_run(variant_A).bold = True
#     cell = table.cell(i, 0)
    cell.text = res[i]
    cell = table.cell(i, 1)
    cell.text = ""
# for i in range(3):
        



doc.save('res.docx')
116/216:
from langchain.embeddings.openai import OpenAIEmbeddings
embedding = OpenAIEmbeddings(openai_api_key='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')
116/217:
from langchain_openai import ChatOpenAI
from langchain_openai import OpenAI
import httpx
llm = ChatOpenAI(model_name="gpt-3.5-turbo-0125",openai_api_key='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')
116/218: pip install langchain_openai
116/219:
from langchain_openai import ChatOpenAI
from langchain_openai import OpenAI
import httpx
llm = ChatOpenAI(model_name="gpt-3.5-turbo-0125",openai_api_key='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')
116/220:
from docx import Document
import re


# Create a new Document
doc = Document()

# Add a table with the same number of rows as res and 2 columns
table = doc.add_table(rows=len(res), cols=2)

# Iterate through each row of the table
for i, row_text in enumerate(res):
    # Split the row text by newline character
    row_parts = row_text.split('\n')
    
    # Iterate through each part of the row text
    for part in row_parts:
        # If the part starts with 'A.', make it bold
        if part.startswith('A.'):
            cell = table.cell(i, 0)
            paragraph = cell.add_paragraph()
            paragraph.add_run(part).bold = True
        else:
            cell = table.cell(i, 0)
            cell.text = part
    
    # Leave the second column empty
    cell = table.cell(i, 1)
    cell.text = ""

# Save the document
doc.save('ans.docx')
116/221:

doc = Document('2.docx')
full_text = []
for paragraph in doc.paragraphs:
    full_text.append(paragraph.text)

print(full_text)
116/222:

doc = Document('2.docx')
full_text = []
for paragraph in doc.paragraphs:
    print(paragraph.text)
#     full_text.append(paragraph.text)

print(full_text)
116/223:

doc = Document('2.docx')
full_text = []
for paragraph in doc.paragraphs:
    print(paragraph.text)
#     full_text.append(paragraph.text)

print(full_text)
116/224:

doc = Document('2.docx')
full_text = []
for paragraph in doc.paragraphs:
#     print(paragraph.text)
    full_text.append(paragraph.text)

print(full_text)
116/225:
def group_and_reorder(lst):
    questions = []
    variants = []

    for item in lst:
        if item.startswith('+'):
            variants.append(item[2:])  # Removing the prefix
        elif item.startswith('-'):
            variants.append(item[2:])  # Removing the prefix
        else:
            questions.append(item)

    # Concatenate variants at the beginning
    result = variants + questions

    return result

# Example usage
lst = ['5.Жалпы экология қандай қарым – қатынасты оқытады:', '- тірі ағзалар арасындағы', '- қоршаған орта және адам арасындағы', '- қоршаған орта факторлары арасындағы', '+ тірі ағзалар және олардың тіршілік ортасы арасындағы', '- адам және тірі ағазалар арасындағы', '6.Жалпы экология – оқытады:', '+ биологиялық ұйымдастыру деңгейі ағзалықтан басталатын биотикалық жүйедегі клмпоненттердің өзара қатынасын зерттейтін ғылым', '- биосфераны бұзушы механизмдер, бұл үрдісті болдырмайтын тәсілдер және табиғат пайдаланудың ұтымды қағидаларын өңдейтін механизмдер', '- адамзат қауымдыстығы мен географиялық – кеңістіктік қоршаған орта арасындағы, әлеуметтік және мәдени орта қатынасы', '- денсаулыққа антропогендік ландшафтардың экологиялық әсері және адамзат популяциясының генефонды ', '- адам эволюциясының ерекшелігі биологиялық түр ретінде, оның биосфера үрдісіндегі алатын орны, басқа тірі заттардан айырмашылығы\n\n\n']

result = group_and_reorder(lst)
print(result)
116/226:
def group_and_reorder(lst):
    questions = []
    variants = []

    for item in lst:
        if item.startswith('+'):
            variants.append(item[2:])  # Removing the prefix
        elif item.startswith('-'):
            variants.append(item[2:])  # Removing the prefix
        else:
            questions.append(item)

    # Concatenate variants at the beginning
    result = variants + questions

    return result

# Example usage
lst = ['5.Жалпы экология қандай қарым – қатынасты оқытады:', '- тірі ағзалар арасындағы', '- қоршаған орта және адам арасындағы', '- қоршаған орта факторлары арасындағы', '+ тірі ағзалар және олардың тіршілік ортасы арасындағы', '- адам және тірі ағазалар арасындағы', '6.Жалпы экология – оқытады:', '+ биологиялық ұйымдастыру деңгейі ағзалықтан басталатын биотикалық жүйедегі клмпоненттердің өзара қатынасын зерттейтін ғылым', '- биосфераны бұзушы механизмдер, бұл үрдісті болдырмайтын тәсілдер және табиғат пайдаланудың ұтымды қағидаларын өңдейтін механизмдер', '- адамзат қауымдыстығы мен географиялық – кеңістіктік қоршаған орта арасындағы, әлеуметтік және мәдени орта қатынасы', '- денсаулыққа антропогендік ландшафтардың экологиялық әсері және адамзат популяциясының генефонды ', '- адам эволюциясының ерекшелігі биологиялық түр ретінде, оның биосфера үрдісіндегі алатын орны, басқа тірі заттардан айырмашылығы\n\n\n']

result = group_and_reorder(lst)
print(result)
116/227:
def group_questions_and_variants(lst):
    grouped_data = {}
    current_question = None

    for item in lst:
        if item.startswith('+'):
            if current_question:
                grouped_data[current_question].append(item[2:])
        elif item.startswith('-'):
            if current_question:
                grouped_data[current_question].append(item[2:])
        else:
            current_question = item
            grouped_data[current_question] = []

    return grouped_data

# Example usage
lst = ['5.Жалпы экология қандай қарым – қатынасты оқытады:', '- тірі ағзалар арасындағы', '- қоршаған орта және адам арасындағы', '- қоршаған орта факторлары арасындағы', '+ тірі ағзалар және олардың тіршілік ортасы арасындағы', '- адам және тірі ағазалар арасындағы', '6.Жалпы экология – оқытады:', '+ биологиялық ұйымдастыру деңгейі ағзалықтан басталатын биотикалық жүйедегі клмпоненттердің өзара қатынасын зерттейтін ғылым', '- биосфераны бұзушы механизмдер, бұл үрдісті болдырмайтын тәсілдер және табиғат пайдаланудың ұтымды қағидаларын өңдейтін механизмдер', '- адамзат қауымдыстығы мен географиялық – кеңістіктік қоршаған ор
116/228:
def group_questions_and_variants(lst):
    grouped_data = {}
    current_question = None

    for item in lst:
        if item.startswith('+'):
            if current_question:
                grouped_data[current_question].append(item[2:])
        elif item.startswith('-'):
            if current_question:
                grouped_data[current_question].append(item[2:])
        else:
            current_question = item
            grouped_data[current_question] = []

    return grouped_data

# Example usage
lst = ['5.Жалпы экология қандай қарым – қатынасты оқытады:', '- тірі ағзалар арасындағы', '- қоршаған орта және адам арасындағы', '- қоршаған орта факторлары арасындағы', '+ тірі ағзалар және олардың тіршілік ортасы арасындағы', '- адам және тірі ағазалар арасындағы', '6.Жалпы экология – оқытады:', '+ биологиялық ұйымдастыру деңгейі ағзалықтан басталатын биотикалық жүйедегі клмпоненттердің өзара қатынасын зерттейтін ғылым', '- биосфераны бұзушы механизмдер, бұл үрдісті болдырмайтын тәсілдер және табиғат пайдаланудың ұтымды қағидаларын өңдейтін механизмдер', '- адамзат қауымдыстығы мен географиялық – кеңістіктік қоршаған орта арасындағы, әлеуметтік және мәдени орта қатынасы', '- денсаулыққа антропогендік ландшафтардың экологиялық әсері және адамзат популяциясының генефонды ', '- адам эволюциясының ерекшелігі биологиялық түр ретінде, оның биосфера үрдісіндегі алатын орны, басқа тірі заттардан айырмашылығы\n\n\n']

grouped_data = group_questions_and_variants(lst)
for question, variants in grouped_data.items():
    print(question)
    for variant in variants:
        print(variant)
    print()
116/229:
def group_questions_and_variants(lst):
    grouped_data = {}
    current_question = None

    for item in lst:
        if item.startswith('+'):
            if current_question:
                grouped_data[current_question].append(item[2:])
        elif item.startswith('-'):
            if current_question:
                grouped_data[current_question].append(item[2:])
        else:
            current_question = item
            grouped_data[current_question] = []

    return grouped_data

# Example usage
lst = ['5.Жалпы экология қандай қарым – қатынасты оқытады:', '- тірі ағзалар арасындағы', '- қоршаған орта және адам арасындағы', '- қоршаған орта факторлары арасындағы', '+ тірі ағзалар және олардың тіршілік ортасы арасындағы', '- адам және тірі ағазалар арасындағы', '6.Жалпы экология – оқытады:', '+ биологиялық ұйымдастыру деңгейі ағзалықтан басталатын биотикалық жүйедегі клмпоненттердің өзара қатынасын зерттейтін ғылым', '- биосфераны бұзушы механизмдер, бұл үрдісті болдырмайтын тәсілдер және табиғат пайдаланудың ұтымды қағидаларын өңдейтін механизмдер', '- адамзат қауымдыстығы мен географиялық – кеңістіктік қоршаған орта арасындағы, әлеуметтік және мәдени орта қатынасы', '- денсаулыққа антропогендік ландшафтардың экологиялық әсері және адамзат популяциясының генефонды ', '- адам эволюциясының ерекшелігі биологиялық түр ретінде, оның биосфера үрдісіндегі алатын орны, басқа тірі заттардан айырмашылығы\n\n\n']

grouped_data = group_questions_and_variants(res)
for question, variants in grouped_data.items():
    print(question)
    for variant in variants:
        print(variant)
    print()
116/230:
def group_questions_and_variants(lst):
    grouped_data = {}
    current_question = None

    for item in lst:
        if item.startswith('+'):
            if current_question:
                grouped_data[current_question].append(item[2:])
        elif item.startswith('-'):
            if current_question:
                grouped_data[current_question].append(item[2:])
        else:
            current_question = item
            grouped_data[current_question] = []

    return grouped_data

# Example usage
lst = ['5.Жалпы экология қандай қарым – қатынасты оқытады:', '- тірі ағзалар арасындағы', '- қоршаған орта және адам арасындағы', '- қоршаған орта факторлары арасындағы', '+ тірі ағзалар және олардың тіршілік ортасы арасындағы', '- адам және тірі ағазалар арасындағы', '6.Жалпы экология – оқытады:', '+ биологиялық ұйымдастыру деңгейі ағзалықтан басталатын биотикалық жүйедегі клмпоненттердің өзара қатынасын зерттейтін ғылым', '- биосфераны бұзушы механизмдер, бұл үрдісті болдырмайтын тәсілдер және табиғат пайдаланудың ұтымды қағидаларын өңдейтін механизмдер', '- адамзат қауымдыстығы мен географиялық – кеңістіктік қоршаған орта арасындағы, әлеуметтік және мәдени орта қатынасы', '- денсаулыққа антропогендік ландшафтардың экологиялық әсері және адамзат популяциясының генефонды ', '- адам эволюциясының ерекшелігі биологиялық түр ретінде, оның биосфера үрдісіндегі алатын орны, басқа тірі заттардан айырмашылығы\n\n\n']

grouped_data = group_questions_and_variants(full_text)
for question, variants in grouped_data.items():
    print(question)
    for variant in variants:
        print(variant)
    print()
116/231:
def group_questions_and_variants(lst):
    grouped_data = {}
    current_question = None

    for item in lst:
        if item.startswith('+'):
            if current_question:
                grouped_data[current_question].append(item[2:])
        elif item.startswith('-'):
            if current_question:
                grouped_data[current_question].append(item[2:])
        else:
            current_question = item
            grouped_data[current_question] = []

    return grouped_data

# Example usage

grouped_data = group_questions_and_variants(full_text)
print(grouped_data)
for question, variants in grouped_data.items():
    print(question)
    for variant in variants:
        print(variant)
    print()
116/232:
def group_questions_and_variants(lst):
    grouped_data = {}
    current_question = None

    for item in lst:
        print(item)
        if item.startswith('+'):
            if current_question:
                grouped_data[current_question].append(item[2:])
        elif item.startswith('-'):
            if current_question:
                grouped_data[current_question].append(item[2:])
        else:
            current_question = item
            grouped_data[current_question] = []

    return grouped_data

# Example usage

grouped_data = group_questions_and_variants(full_text)
print(grouped_data)
for question, variants in grouped_data.items():
    print(question)
    for variant in variants:
        print(variant)
    print()
116/233:
def group_questions_and_variants(lst):
    grouped_data = {}
    current_question = None

    for item in lst:
        print(item)
        break
        if item.startswith('+'):
            if current_question:
                grouped_data[current_question].append(item[2:])
        elif item.startswith('-'):
            if current_question:
                grouped_data[current_question].append(item[2:])
        else:
            current_question = item
            grouped_data[current_question] = []

    return grouped_data

# Example usage

grouped_data = group_questions_and_variants(full_text)
print(grouped_data)
for question, variants in grouped_data.items():
    print(question)
    for variant in variants:
        print(variant)
    print()
116/234:
def group_questions_and_variants(lst):
    grouped_data = {}
    current_question = None
    cnt = 0
    for item in lst:
        print(item)
        if not cnt:
            cnt += 1
            break
        if item.startswith('+'):
            if current_question:
                grouped_data[current_question].append(item[2:])
        elif item.startswith('-'):
            if current_question:
                grouped_data[current_question].append(item[2:])
        else:
            current_question = item
            grouped_data[current_question] = []

    return grouped_data

# Example usage

grouped_data = group_questions_and_variants(full_text)
print(grouped_data)
for question, variants in grouped_data.items():
    print(question)
    for variant in variants:
        print(variant)
    print()
116/235:
def group_questions_and_variants(lst):
    grouped_data = {}
    current_question = None
    cnt = 0
    for item in lst:
        print(item)
        if cnt:
            break
        else:
            cnt += 1
        if item.startswith('+'):
            if current_question:
                grouped_data[current_question].append(item[2:])
        elif item.startswith('-'):
            if current_question:
                grouped_data[current_question].append(item[2:])
        else:
            current_question = item
            grouped_data[current_question] = []

    return grouped_data

# Example usage

grouped_data = group_questions_and_variants(full_text)
print(grouped_data)
for question, variants in grouped_data.items():
    print(question)
    for variant in variants:
        print(variant)
    print()
116/236:
def group_questions_and_variants(lst):
    grouped_data = {}
    current_question = None
    cnt = 0
    correct = []
    temp = []
    for item in lst:
        
        if item.startswith('+'):
            if current_question:
                correct.append(item[2:])
        elif item.startswith('-'):
            if current_question:
                temp.append(item[2:])
#                 grouped_data[current_question].append(item[2:])
        else:
            current_question = item
            grouped_data[current_question] = []
    
    grouped_data[current_question].extend(correct)
    grouped_data[current_question].extend(temp)

    return grouped_data

# Example usage

grouped_data = group_questions_and_variants(full_text)
print(grouped_data)
for question, variants in grouped_data.items():
    print(question)
    for variant in variants:
        print(variant)
    print()
116/237:
def group_questions_and_variants(lst):
    grouped_data = {}
    current_question = None
    cnt = 0
    correct = []
    temp = []
    for item in lst:
        
        if item.startswith('+'):
            if current_question:
                correct.append(item[2:])
        elif item.startswith('-'):
            if current_question:
                temp.append(item[2:])
#                 grouped_data[current_question].append(item[2:])
        else:
            current_question = item
            grouped_data[current_question] = []
    print(temp)
    grouped_data[current_question].extend(correct)
    grouped_data[current_question].extend(temp)

    return grouped_data

# Example usage

grouped_data = group_questions_and_variants(full_text)
print(grouped_data)
for question, variants in grouped_data.items():
    print(question)
    for variant in variants:
        print(variant)
    print()
116/238:
def group_questions_and_variants(lst):
    grouped_data = {}
    current_question = None
    cnt = 0
    correct = []
    temp = []
    for item in lst:
        
        if item.startswith('+'):
            if current_question:
                correct.append(item[2:])
        elif item.startswith('-'):
            if current_question:
                temp.append(item[2:])
#                 grouped_data[current_question].append(item[2:])
        else:
            current_question = item
            grouped_data[current_question] = []
    print(correct)
    grouped_data[current_question].extend(correct)
    grouped_data[current_question].extend(temp)

    return grouped_data

# Example usage

grouped_data = group_questions_and_variants(full_text)
print(grouped_data)
for question, variants in grouped_data.items():
    print(question)
    for variant in variants:
        print(variant)
    print()
116/239:
def group_questions_and_variants(lst):
    grouped_data = {}
    current_question = None
    cnt = 0
    correct = []
    temp = []
    for item in lst:
        
        if item.startswith('+'):
            if current_question:
                correct.append(item[2:])
        elif item.startswith('-'):
            if current_question:
                temp.append(item[2:])
#                 grouped_data[current_question].append(item[2:])
        else:
            current_question = item
            grouped_data[current_question] = []
    print(correct)
    grouped_data[current_question].extend(correct)
    grouped_data[current_question].extend(temp)

    return grouped_data

# Example usage

grouped_data = group_questions_and_variants(full_text)
# print(grouped_data)
# for question, variants in grouped_data.items():
#     print(question)
#     for variant in variants:
#         print(variant)
#     print()
116/240:
def group_questions_and_variants(lst):
    grouped_data = {}
    current_question = None
    cnt = 0
    correct = []
    temp = []
    for item in lst:
        
        if item.startswith('+'):
            if current_question:
                correct.append(item[2:])
        elif item.startswith('-'):
            if current_question:
                temp.append(item[2:])
#                 grouped_data[current_question].append(item[2:])
        else:
            current_question = item
            grouped_data[current_question] = []
    print(correct)
    grouped_data[current_question].extend(correct)
    grouped_data[current_question].extend(temp)

    return grouped_data

# Example usage

grouped_data = group_questions_and_variants(full_text)
print(grouped_data)
# for question, variants in grouped_data.items():
#     print(question)
#     for variant in variants:
#         print(variant)
#     print()
116/241:
def group_questions_and_variants(lst):
    grouped_data = {}
    current_question = None
    cnt = 0
    correct = []
    temp = []
    for item in lst:
        
        if item.startswith('+'):
            if current_question:
                correct.append(item[2:])
        elif item.startswith('-'):
            if current_question:
                temp.append(item[2:])
#                 grouped_data[current_question].append(item[2:])
        else:
            current_question = item
            grouped_data[current_question] = []
#     print(correct)
    grouped_data[current_question].extend(correct)
    grouped_data[current_question].extend(temp)

    return grouped_data

# Example usage

grouped_data = group_questions_and_variants(full_text)
print(grouped_data)
# for question, variants in grouped_data.items():
#     print(question)
#     for variant in variants:
#         print(variant)
#     print()
116/242:
def group_questions_and_variants(lst):
    grouped_data = {}
    current_question = None
    cnt = 0
    correct = []
    temp = []
    for item in lst:
        
        if item.startswith('+'):
            if current_question:
                correct.append(item[2:])
        elif item.startswith('-'):
            if current_question:
                temp.append(item[2:])
#                 grouped_data[current_question].append(item[2:])
        else:
            current_question = item
            grouped_data[current_question] = []
    correct.extend(temp)
    grouped_data[current_question] = correct

    return grouped_data

# Example usage

grouped_data = group_questions_and_variants(full_text)
print(grouped_data)
# for question, variants in grouped_data.items():
#     print(question)
#     for variant in variants:
#         print(variant)
#     print()
116/243:
def group_questions_and_variants(lst):
    grouped_data = {}
    current_question = None
    cnt = 0
    correct = []
    temp = []
    for item in lst:
        
        if item.startswith('+'):
            if current_question:
                correct.append(item[2:])
        elif item.startswith('-'):
            if current_question:
                temp.append(item[2:])
#                 grouped_data[current_question].append(item[2:])
        else:
            current_question = item
            grouped_data[current_question] = []
    correct.extend(temp)
    grouped_data[current_question] = correct

    return grouped_data

# Example usage

grouped_data = group_questions_and_variants(full_text)
print(grouped_data)
for question, variants in grouped_data.items():
    print(question)
    for variant in variants:
        print(variant)
    print()
116/244:
def group_questions_and_variants(lst):
    grouped_data = {}
    current_question = None
    cnt = 0
    correct = []
    temp = []
    for item in lst:
        
        if item.startswith('+'):
            if current_question:
                correct.append(item[2:])
        elif item.startswith('-'):
            if current_question:
                temp.append(item[2:])
#                 grouped_data[current_question].append(item[2:])
        else:
            current_question = item
            grouped_data[current_question] = []
    correct.extend(temp)
    grouped_data[current_question] = correct

    return grouped_data

# Example usage

grouped_data = group_questions_and_variants(full_text)
# print(grouped_data)
for question, variants in grouped_data.items():
    print(question)
    for variant in variants:
        print(variant)
    print()
116/245:
def group_questions_and_variants(lst):
    grouped_data = {}
    current_question = None
    cnt = 0
    correct = []
    temp = []
    for item in lst:
        
        if item.startswith('+'):
            if current_question:
                correct.append(item[2:])
                print(item)
        elif item.startswith('-'):
            if current_question:
                temp.append(item[2:])
#                 grouped_data[current_question].append(item[2:])
        else:
            current_question = item
            grouped_data[current_question] = []
    correct.extend(temp)
    grouped_data[current_question] = correct

    return grouped_data

# Example usage

grouped_data = group_questions_and_variants(full_text)
# print(grouped_data)
for question, variants in grouped_data.items():
    print(question)
    for variant in variants:
        print(variant)
    print()
116/246:
def group_questions_and_variants(lst):
    grouped_data = {}
    current_question = None
    cnt = 0
    correct = []
    temp = []
    for item in lst:
        
        if item.startswith('+'):
            if current_question:
                correct.append(item[2:])
                print(item)
        elif item.startswith('-'):
            if current_question:
                temp.append(item[2:])
#                 grouped_data[current_question].append(item[2:])
        else:
            current_question = item
#             grouped_data[current_question] = []
    print(current_question)
    correct.extend(temp)
    grouped_data[current_question] = correct

    return grouped_data

# Example usage

grouped_data = group_questions_and_variants(full_text)
# print(grouped_data)
for question, variants in grouped_data.items():
    print(question)
    for variant in variants:
        print(variant)
    print()
116/247:
def group_questions_and_variants(lst):
    grouped_data = {}
    current_question = None
    cnt = 0
    correct = []
    temp = []
    for item in lst:
        
        if item.startswith('+'):
            if current_question:
                correct.append(item[2:])
        elif item.startswith('-'):
            if current_question:
                temp.append(item[2:])
#                 grouped_data[current_question].append(item[2:])
        else:
            current_question = item
#             grouped_data[current_question] = []
    print(current_question)
    correct.extend(temp)
    grouped_data[current_question] = correct

    return grouped_data

# Example usage

grouped_data = group_questions_and_variants(full_text)
# print(grouped_data)
for question, variants in grouped_data.items():
    print(question)
    for variant in variants:
        print(variant)
    print()
116/248:
def group_questions_and_variants(lst):
    grouped_data = {}
    current_question = None
    cnt = 0
    correct = []
    temp = []
    for item in lst:
        
        if item.startswith('+'):
            if current_question:
                correct.append(item[2:])
        elif item.startswith('-'):
            if current_question:
                temp.append(item[2:])
#                 grouped_data[current_question].append(item[2:])
        else:
            current_question = item
#             grouped_data[current_question] = []
    print(current_question)
    correct.extend(temp)
    grouped_data[current_question] = correct

    return grouped_data

# Example usage

grouped_data = group_questions_and_variants(full_text)
# print(grouped_data)
# for question, variants in grouped_data.items():
#     print(question)
#     for variant in variants:
#         print(variant)
#     print()
116/249:
def group_questions_and_variants(lst):
    grouped_data = {}
    current_question = None
    cnt = 0
    correct = []
    temp = []
    for item in lst:
        
        if item.startswith('+'):
            if current_question:
                correct.append(item[2:])
                grouped_data[current_question].append(item[2:])
        elif item.startswith('-'):
            if current_question:
                temp.append(item[2:])
                grouped_data[current_question].append(item[2:])
        else:
            current_question = item
#             grouped_data[current_question] = []
#     print(current_question)
    correct.extend(temp)
    grouped_data[current_question] = correct

    return grouped_data

# Example usage

grouped_data = group_questions_and_variants(full_text)
# print(grouped_data)
for question, variants in grouped_data.items():
    print(question)
    for variant in variants:
        print(variant)
    print()
116/250:
def group_questions_and_variants(lst):
    grouped_data = {}
    current_question = None
    cnt = 0
    correct = []
    temp = []
    for item in lst:
        
        if item.startswith('+'):
            if current_question:
                correct.append(item[2:])
                grouped_data[current_question].append(item[2:])
        elif item.startswith('-'):
            if current_question:
                temp.append(item[2:])
                grouped_data[current_question].append(item[2:])
        else:
            current_question = item
            grouped_data[current_question] = []
#     print(current_question)
    correct.extend(temp)
    grouped_data[current_question] = correct

    return grouped_data

# Example usage

grouped_data = group_questions_and_variants(full_text)
# print(grouped_data)
for question, variants in grouped_data.items():
    print(question)
    for variant in variants:
        print(variant)
    print()
116/251:
def group_questions_and_variants(lst):
    grouped_data = {}
    current_question = None
    cnt = 0
    correct = []
    temp = []
    lst.append('end')
    for item in lst:
        
        if item.startswith('+'):
            if current_question:
                correct.append(item[2:])
                grouped_data[current_question].append(item[2:])
        elif item.startswith('-'):
            if current_question:
                temp.append(item[2:])
                grouped_data[current_question].append(item[2:])
        else:
            correct.extend(temp)
            grouped_data[current_question] = correct
            current_question = item
            grouped_data[current_question] = correct
            correct = []
            temp = []

    return grouped_data

# Example usage

grouped_data = group_questions_and_variants(full_text)
print(grouped_data)
for question, variants in grouped_data.items():
    print(question)
    for variant in variants:
        print(variant)
    print()
116/252:
def group_questions_and_variants(lst):
    grouped_data = {}
    current_question = None
    cnt = 0
    correct = []
    temp = []
    lst.append('end')
    for item in lst:
        
        if item.startswith('+'):
            if current_question:
                correct.append(item[2:])
                grouped_data[current_question].append(item[2:])
        elif item.startswith('-'):
            if current_question:
                temp.append(item[2:])
                grouped_data[current_question].append(item[2:])
        else:
            correct.extend(temp)
            if current_question:
                grouped_data[current_question] = correct
            current_question = item
            grouped_data[current_question] = correct
            correct = []
            temp = []

    return grouped_data

# Example usage

grouped_data = group_questions_and_variants(full_text)
print(grouped_data)
for question, variants in grouped_data.items():
    print(question)
    for variant in variants:
        print(variant)
    print()
116/253:
def group_questions_and_variants(lst):
    grouped_data = {}
    current_question = None
    cnt = 0
    correct = []
    temp = []
    lst.append('end')
    for item in lst:
        if item == 'end':
            break
        if item.startswith('+'):
            if current_question:
                correct.append(item[2:])
                grouped_data[current_question].append(item[2:])
        elif item.startswith('-'):
            if current_question:
                temp.append(item[2:])
                grouped_data[current_question].append(item[2:])
        else:
            correct.extend(temp)
            if current_question:
                grouped_data[current_question] = correct
            current_question = item
            grouped_data[current_question] = correct
            correct = []
            temp = []

    return grouped_data

# Example usage

grouped_data = group_questions_and_variants(full_text)
print(grouped_data)
for question, variants in grouped_data.items():
    print(question)
    for variant in variants:
        print(variant)
    print()
116/254:
def group_questions_and_variants(lst):
    grouped_data = {}
    current_question = None
    cnt = 0
    correct = []
    temp = []
    lst.append('end')
    for item in lst:
        if item == 'end':
            grouped_data[current_question] = correct
            break
        if item.startswith('+'):
            if current_question:
                correct.append(item[2:])
                grouped_data[current_question].append(item[2:])
        elif item.startswith('-'):
            if current_question:
                temp.append(item[2:])
                grouped_data[current_question].append(item[2:])
        else:
            if current_question:
                grouped_data[current_question] = correct
            current_question = item
            correct.extend(temp)
            correct = []
            temp = []

    return grouped_data

# Example usage

grouped_data = group_questions_and_variants(full_text)
print(grouped_data)
for question, variants in grouped_data.items():
    print(question)
    for variant in variants:
        print(variant)
    print()
116/255:
def group_questions_and_variants(lst):
    grouped_data = {}
    current_question = None
    cnt = 0
    correct = []
    temp = []
    lst.append('end')
    for item in lst:
        if item == 'end':
            grouped_data[current_question] = correct
            break
        if item.startswith('+'):
            if current_question:
                correct.append(item[2:])
#                 grouped_data[current_question].append(item[2:])
        elif item.startswith('-'):
            if current_question:
                temp.append(item[2:])
#                 grouped_data[current_question].append(item[2:])
        else:
            if current_question:
                grouped_data[current_question] = correct
            current_question = item
            correct.extend(temp)
            correct = []
            temp = []

    return grouped_data

# Example usage

grouped_data = group_questions_and_variants(full_text)
print(grouped_data)
for question, variants in grouped_data.items():
    print(question)
    for variant in variants:
        print(variant)
    print()
116/256:
def group_questions_and_variants(lst):
    grouped_data = {}
    current_question = None
    cnt = 0
    correct = []
    temp = []
    lst.append('end')
    print(lst)
    for item in lst:
        if item == 'end':
            grouped_data[current_question] = correct
            break
        if item.startswith('+'):
            if current_question:
                correct.append(item[2:])
#                 grouped_data[current_question].append(item[2:])
        elif item.startswith('-'):
            if current_question:
                temp.append(item[2:])
#                 grouped_data[current_question].append(item[2:])
        else:
            if current_question:
                grouped_data[current_question] = correct
            current_question = item
            correct.extend(temp)
            correct = []
            temp = []

    return grouped_data

# Example usage

grouped_data = group_questions_and_variants(full_text)
print(grouped_data)
for question, variants in grouped_data.items():
    print(question)
    for variant in variants:
        print(variant)
    print()
116/257:

doc = Document('2.docx')
full_text = []
for paragraph in doc.paragraphs:
#     print(paragraph.text)
    full_text.append(paragraph.text)

print(full_text)
116/258:
def group_questions_and_variants(lst):
    grouped_data = {}
    current_question = None
    cnt = 0
    correct = []
    temp = []
    lst.append('end')
    print(lst)
    for item in lst:
        if item == 'end':
            grouped_data[current_question] = correct
            break
        if item.startswith('+'):
            if current_question:
                idx = 0
                for i in range(len(item)):
                    if item[i] =='+':
                        idx = i
                        break
                correct.append(item[idx+1:])
#                 grouped_data[current_question].append(item[2:])
        elif item.startswith('-'):
            if current_question:
                idx = 0
                for i in range(len(item)):
                    if item[i] =='-':
                        idx = i
                        break
                correct.append(item[idx+1:])
                temp.append(item[2:])
#                 grouped_data[current_question].append(item[2:])
        else:
            if current_question:
                grouped_data[current_question] = correct
            current_question = item
            correct.extend(temp)
            correct = []
            temp = []

    return grouped_data

# Example usage

grouped_data = group_questions_and_variants(full_text)
print(grouped_data)
for question, variants in grouped_data.items():
    print(question)
    for variant in variants:
        print(variant)
    print()
116/259:
def group_questions_and_variants(lst):
    grouped_data = {}
    current_question = None
    cnt = 0
    correct = []
    temp = []
    lst.append('end')
    print(lst)
    for item in lst:
        if item == 'end':
            grouped_data[current_question] = correct
            break
        if item.startswith('+'):
            if current_question:
                idx = 0
                for i in range(len(item)):
                    if item[i] =='+':
                        idx = i
                        break
                correct.append(item[idx+1:])
#                 grouped_data[current_question].append(item[2:])
        elif item.startswith('-'):
            if current_question:
                idx = 0
                for i in range(len(item)):
                    if item[i] =='-':
                        idx = i
                        break
                correct.append(item[idx+1:])
                temp.append(item[2:])
#                 grouped_data[current_question].append(item[2:])
        else:
            if current_question:
                grouped_data[current_question] = correct
            current_question = item
            correct.extend(temp)
            correct = []
            temp = []

    return grouped_data

# Example usage

grouped_data = group_questions_and_variants(full_text)
print(grouped_data)
for question, variants in grouped_data.items():
    print(question)
    for variant in variants:
        print(variant)
    print()
116/260:
def group_questions_and_variants(lst):
    grouped_data = {}
    current_question = None
    cnt = 0
    correct = []
    temp = []
    lst.append('end')
    print(lst)
    for item in lst:
        if item == 'end':
            grouped_data[current_question] = correct
            break
        if item.startswith('+'):
            if current_question:
                idx = 0
                for i in range(len(item)):
                    if item[i] =='+':
                        idx = i
                        break
                correct.append(item[idx+1:])
#                 grouped_data[current_question].append(item[2:])
        elif item.startswith('-'):
            if current_question:
                idx = 0
                for i in range(len(item)):
                    if item[i] =='-':
                        idx = i
                        break
                correct.append(item[idx+1:])
                temp.append(item[2:])
#                 grouped_data[current_question].append(item[2:])
        else:
            if current_question:
                grouped_data[current_question] = correct
            current_question = item
            correct.extend(temp)
            correct = []
            temp = []

    return grouped_data

# Example usage

grouped_data = group_questions_and_variants(full_text)
print(grouped_data)
# for question, variants in grouped_data.items():
#     print(question)
#     for variant in variants:
#         print(variant)
#     print()
116/261:
def group_questions_and_variants(lst):
    grouped_data = {}
    current_question = None
    cnt = 0
    correct = []
    temp = []
    lst.append('end')
    print(lst)
    for item in lst:
        if item == 'end':
            grouped_data[current_question] = correct
            break
        if item.startswith('+'):
            if current_question:
                idx = 0
                for i in range(len(item)):
                    if item[i] =='+':
                        idx = i
                        break
                correct.append(item[idx+1:])
#                 grouped_data[current_question].append(item[2:])
        elif item.startswith('-'):
            if current_question:
                idx = 0
                for i in range(len(item)):
                    if item[i] =='-':
                        idx = i
                        break
                correct.append(item[idx+1:])
                temp.append(item[2:])
#                 grouped_data[current_question].append(item[2:])
        else:
            if current_question:
                grouped_data[current_question] = correct
            current_question = item
            correct.extend(temp)
            correct = []
            temp = []

    return grouped_data

# Example usage

grouped_data = group_questions_and_variants(full_text)
# print(grouped_data)
for question, variants in grouped_data.items():
    print(question)
    for variant in variants:
        print(variant)
    print()
116/262:
def group_questions_and_variants(lst):
    grouped_data = {}
    current_question = None
    cnt = 0
    correct = []
    temp = []
    lst.append('end')
#     print(lst)
    for item in lst:
        if item == 'end':
            grouped_data[current_question] = correct
            break
        if item.startswith('+'):
            if current_question:
                idx = 0
                for i in range(len(item)):
                    if item[i] =='+':
                        idx = i
                        break
                correct.append(item[idx+1:])
#                 grouped_data[current_question].append(item[2:])
        elif item.startswith('-'):
            if current_question:
                idx = 0
                for i in range(len(item)):
                    if item[i] =='-':
                        idx = i
                        break
                correct.append(item[idx+1:])
                temp.append(item[2:])
#                 grouped_data[current_question].append(item[2:])
        else:
            if current_question:
                grouped_data[current_question] = correct
            current_question = item
            correct.extend(temp)
            correct = []
            temp = []

    return grouped_data

# Example usage

grouped_data = group_questions_and_variants(full_text)
# print(grouped_data)
for question, variants in grouped_data.items():
    print(question)
    for variant in variants:
        print(variant)
    print()
116/263:
def group_questions_and_variants(lst):
    grouped_data = {}
    current_question = None
    cnt = 0
    correct = []
    temp = []
    lst.append('end')
#     print(lst)
    for item in lst:
        if item == 'end':
            grouped_data[current_question] = correct
            break
        if item.startswith('+'):
            if current_question:
                idx = 0
                for i in range(len(item)):
                    if item[i] =='+':
                        idx = i
                        break
                correct.append(item[idx+1:])
#                 grouped_data[current_question].append(item[2:])
        elif item.startswith('-'):
            if current_question:
                idx = 0
                for i in range(len(item)):
                    if item[i] =='-':
                        idx = i
                        break
                correct.append(item[idx+1:])
                temp.append(item[2:])
#                 grouped_data[current_question].append(item[2:])
        else:
            if current_question:
                grouped_data[current_question] = correct
            current_question = item
            correct.extend(temp)
            correct = []
            temp = []

    return grouped_data

# Example usage

grouped_data = group_questions_and_variants(full_text)
print(grouped_data)
# for question, variants in grouped_data.items():
#     print(question)
#     for variant in variants:
#         print(variant)
#     print()
116/264:
def group_questions_and_variants(lst):
    grouped_data = {}
    current_question = None
    cnt = 0
    correct = []
    temp = []
    lst.append('end')
#     print(lst)
    for item in lst:
        if item == 'end':
            grouped_data[current_question] = correct
            break
        if item.startswith('+'):
            if current_question:
#                 idx = 0
#                 for i in range(len(item)):
#                     if item[i] =='+':
#                         idx = i
#                         break
                correct.append(item[2:])
#                 grouped_data[current_question].append(item[2:])
        elif item.startswith('-'):
            if current_question:
                idx = 0
                for i in range(len(item)):
                    if item[i] =='-':
                        idx = i
                        break
                correct.append(item[idx+1:])
                temp.append(item[2:])
#                 grouped_data[current_question].append(item[2:])
        else:
            if current_question:
                grouped_data[current_question] = correct
            current_question = item
            correct.extend(temp)
            correct = []
            temp = []

    return grouped_data

# Example usage

grouped_data = group_questions_and_variants(full_text)
print(grouped_data)
# for question, variants in grouped_data.items():
#     print(question)
#     for variant in variants:
#         print(variant)
#     print()
116/265:
def group_questions_and_variants(lst):
    grouped_data = {}
    current_question = None
    cnt = 0
    correct = []
    temp = []
    lst.append('end')
    print(lst)
    for item in lst:
        if item == 'end':
            grouped_data[current_question] = correct
            break
        if item.startswith('+'):
            if current_question:
#                 idx = 0
#                 for i in range(len(item)):
#                     if item[i] =='+':
#                         idx = i
#                         break
                correct.append(item[2:])
#                 grouped_data[current_question].append(item[2:])
        elif item.startswith('-'):
            if current_question:
                idx = 0
                for i in range(len(item)):
                    if item[i] =='-':
                        idx = i
                        break
                correct.append(item[idx+1:])
                temp.append(item[2:])
#                 grouped_data[current_question].append(item[2:])
        else:
            if current_question:
                grouped_data[current_question] = correct
            current_question = item
            correct.extend(temp)
            correct = []
            temp = []

    return grouped_data

# Example usage

grouped_data = group_questions_and_variants(full_text)
print(grouped_data)
# for question, variants in grouped_data.items():
#     print(question)
#     for variant in variants:
#         print(variant)
#     print()
116/266:
def group_questions_and_variants(lst):
    grouped_data = {}
    current_question = None
    cnt = 0
    correct = []
    temp = []
    lst.append('end')
#     print(lst)
    for item in lst:
        if item == 'end':
            grouped_data[current_question] = correct
            break
        if item.startswith('+'):
            if current_question:
#                 idx = 0
#                 for i in range(len(item)):
#                     if item[i] =='+':
#                         idx = i
#                         break
                correct.append(item[2:])
#                 grouped_data[current_question].append(item[2:])
        elif item.startswith('-'):
            if current_question:
                idx = 0
                for i in range(len(item)):
                    if item[i] =='-':
                        idx = i
                        break
                correct.append(item[idx+1:])
                temp.append(item[2:])
#                 grouped_data[current_question].append(item[2:])
        else:
            if current_question:
                grouped_data[current_question] = correct
            current_question = item
            correct.extend(temp)
            correct = []
            temp = []

    return grouped_data

# Example usage

grouped_data = group_questions_and_variants(full_text)
print(grouped_data)
# for question, variants in grouped_data.items():
#     print(question)
#     for variant in variants:
#         print(variant)
#     print()
116/267:
def group_questions_and_variants(lst):
    grouped_data = {}
    current_question = None
    cnt = 0
    correct = []
    temp = []
    lst.append('end')
#     print(lst)
    for item in lst:
        if item == 'end':
            grouped_data[current_question] = correct
            break
        if item.startswith('+'):
            if current_question:
#                 idx = 0
#                 for i in range(len(item)):
#                     if item[i] =='+':
#                         idx = i
#                         break
                correct.append(item[2:])
#                 grouped_data[current_question].append(item[2:])
        elif item.startswith('-'):
            if current_question:
#                 idx = 0
#                 for i in range(len(item)):
#                     if item[i] =='-':
#                         idx = i
#                         break
                correct.append(item[idx+1:])
                temp.append(item[2:])
#                 grouped_data[current_question].append(item[2:])
        else:
            if current_question:
                grouped_data[current_question] = correct
            current_question = item
            correct.extend(temp)
            correct = []
            temp = []

    return grouped_data

# Example usage

grouped_data = group_questions_and_variants(full_text)
print(grouped_data)
# for question, variants in grouped_data.items():
#     print(question)
#     for variant in variants:
#         print(variant)
#     print()
116/268:
def group_questions_and_variants(lst):
    grouped_data = {}
    current_question = None
    cnt = 0
    correct = []
    temp = []
    lst.append('end')
#     print(lst)
    for item in lst:
        if item == 'end':
            grouped_data[current_question] = correct
            break
        if item.startswith('+'):
            if current_question:
#                 idx = 0
#                 for i in range(len(item)):
#                     if item[i] =='+':
#                         idx = i
#                         break
                correct.append(item[2:])
#                 grouped_data[current_question].append(item[2:])
        elif item.startswith('-'):
            if current_question:
#                 idx = 0
#                 for i in range(len(item)):
#                     if item[i] =='-':
#                         idx = i
#                         break
#                 correct.append(item[idx+1:])
                temp.append(item[2:])
#                 grouped_data[current_question].append(item[2:])
        else:
            if current_question:
                grouped_data[current_question] = correct
            current_question = item
            correct.extend(temp)
            correct = []
            temp = []

    return grouped_data

# Example usage

grouped_data = group_questions_and_variants(full_text)
print(grouped_data)
# for question, variants in grouped_data.items():
#     print(question)
#     for variant in variants:
#         print(variant)
#     print()
116/269:
def group_questions_and_variants(lst):
    grouped_data = {}
    current_question = None
    cnt = 0
    correct = []
    temp = []
    lst.append('end')
#     print(lst)
    for item in lst:
        
        if item.startswith('+'):
            if current_question:
#                 idx = 0
#                 for i in range(len(item)):
#                     if item[i] =='+':
#                         idx = i
#                         break
                correct.append(item[2:])
#                 grouped_data[current_question].append(item[2:])
        elif item.startswith('-'):
            if current_question:
#                 idx = 0
#                 for i in range(len(item)):
#                     if item[i] =='-':
#                         idx = i
#                         break
#                 correct.append(item[idx+1:])
                temp.append(item[2:])
#                 grouped_data[current_question].append(item[2:])
        else:
            if current_question:
                grouped_data[current_question] = correct
            current_question = item
            correct.extend(temp)
            correct = []
            temp = []
            if item == 'end':
                grouped_data[current_question] = correct
                break

    return grouped_data

# Example usage

grouped_data = group_questions_and_variants(full_text)
print(grouped_data)
# for question, variants in grouped_data.items():
#     print(question)
#     for variant in variants:
#         print(variant)
#     print()
116/270:
def group_questions_and_variants(lst):
    grouped_data = {}
    current_question = None
    cnt = 0
    correct = []
    temp = []
    lst.append('end')
#     print(lst)
    for item in lst:
        
        if item.startswith('+'):
            if current_question:
#                 idx = 0
#                 for i in range(len(item)):
#                     if item[i] =='+':
#                         idx = i
#                         break
                correct.append(item[2:])
#                 grouped_data[current_question].append(item[2:])
        elif item.startswith('-'):
            if current_question:
#                 idx = 0
#                 for i in range(len(item)):
#                     if item[i] =='-':
#                         idx = i
#                         break
#                 correct.append(item[idx+1:])
                temp.append(item[2:])
#                 grouped_data[current_question].append(item[2:])
        else:
            if item == 'end':
                    grouped_data[current_question] = correct
                    break
            if current_question:
                grouped_data[current_question] = correct
            current_question = item
            correct.extend(temp)
            correct = []
            temp = []
            

    return grouped_data

# Example usage

grouped_data = group_questions_and_variants(full_text)
print(grouped_data)
# for question, variants in grouped_data.items():
#     print(question)
#     for variant in variants:
#         print(variant)
#     print()
116/271:
def group_questions_and_variants(lst):
    grouped_data = {}
    current_question = None
    cnt = 0
    correct = []
    temp = []
    lst.append('end')
#     print(lst)
    for item in lst:
        
        if item.startswith('+'):
            if current_question:
#                 idx = 0
#                 for i in range(len(item)):
#                     if item[i] =='+':
#                         idx = i
#                         break
                correct.append(item[2:])
#                 grouped_data[current_question].append(item[2:])
        elif item.startswith('-'):
            if current_question:
#                 idx = 0
#                 for i in range(len(item)):
#                     if item[i] =='-':
#                         idx = i
#                         break
#                 correct.append(item[idx+1:])
                temp.append(item[2:])
#                 grouped_data[current_question].append(item[2:])
        else:
            
            if current_question:
                grouped_data[current_question] = correct
            current_question = item
            correct.extend(temp)
            if item == 'end':
                    grouped_data[current_question] = correct
                    break
            correct = []
            temp = []
            

    return grouped_data

# Example usage

grouped_data = group_questions_and_variants(full_text)
print(grouped_data)
# for question, variants in grouped_data.items():
#     print(question)
#     for variant in variants:
#         print(variant)
#     print()
116/272:
def group_questions_and_variants(lst):
    grouped_data = {}
    current_question = None
    
    cnt = 0
    correct = []
    temp = []
    lst.append('end')
#     print(lst)
    count = 260
    for item in lst:
        if current_question:
            p = r'\d+'
            s = current_question
            match = re.search(p, s)        
            if match:
                start, end = match.span()
                modified_string = s[:start] + f'{count}' + s[end:]
                count += 1
            current_question = s
        if item.startswith('+'):
            if current_question:
#                 idx = 0
#                 for i in range(len(item)):
#                     if item[i] =='+':
#                         idx = i
#                         break
                correct.append(item[2:])
#                 grouped_data[current_question].append(item[2:])
        elif item.startswith('-'):
            if current_question:
#                 idx = 0
#                 for i in range(len(item)):
#                     if item[i] =='-':
#                         idx = i
#                         break
#                 correct.append(item[idx+1:])
                temp.append(item[2:])
#                 grouped_data[current_question].append(item[2:])
        else:
            
            if current_question:
                grouped_data[current_question] = correct
            current_question = item
            correct.extend(temp)
            if item == 'end':
                    grouped_data[current_question] = correct
                    break
            correct = []
            temp = []
            

    return grouped_data

# Example usage

grouped_data = group_questions_and_variants(full_text)
print(grouped_data)
# for question, variants in grouped_data.items():
#     print(question)
#     for variant in variants:
#         print(variant)
#     print()
116/273:
def group_questions_and_variants(lst):
    grouped_data = {}
    current_question = None
    
    cnt = 0
    correct = []
    temp = []
    lst.append('end')
#     print(lst)
    count = 260
    for item in lst:
        
        if item.startswith('+'):
            if current_question:
#                 idx = 0
#                 for i in range(len(item)):
#                     if item[i] =='+':
#                         idx = i
#                         break
                correct.append(item[2:])
#                 grouped_data[current_question].append(item[2:])
        elif item.startswith('-'):
            if current_question:
#                 idx = 0
#                 for i in range(len(item)):
#                     if item[i] =='-':
#                         idx = i
#                         break
#                 correct.append(item[idx+1:])
                temp.append(item[2:])
#                 grouped_data[current_question].append(item[2:])
        else:
            if current_question:
                p = r'\d+'
                s = current_question
                match = re.search(p, s)        
                if match:
                    start, end = match.span()
                    modified_string = s[:start] + f'{count}' + s[end:]
                    count += 1
                current_question = s
            if current_question:
                grouped_data[current_question] = correct
            current_question = item
            correct.extend(temp)
            if item == 'end':
                    grouped_data[current_question] = correct
                    break
            correct = []
            temp = []
            

    return grouped_data

# Example usage

grouped_data = group_questions_and_variants(full_text)
print(grouped_data)
# for question, variants in grouped_data.items():
#     print(question)
#     for variant in variants:
#         print(variant)
#     print()
116/274:
def group_questions_and_variants(lst):
    grouped_data = {}
    current_question = None
    
    cnt = 0
    correct = []
    temp = []
    lst.append('end')
#     print(lst)
    count = 260
    for item in lst:
        
        if item.startswith('+'):
            if current_question:
#                 idx = 0
#                 for i in range(len(item)):
#                     if item[i] =='+':
#                         idx = i
#                         break
                correct.append(item[2:])
#                 grouped_data[current_question].append(item[2:])
        elif item.startswith('-'):
            if current_question:
#                 idx = 0
#                 for i in range(len(item)):
#                     if item[i] =='-':
#                         idx = i
#                         break
#                 correct.append(item[idx+1:])
                temp.append(item[2:])
#                 grouped_data[current_question].append(item[2:])
        else:
            if current_question:
                p = r'\d+'
                s = current_question
                match = re.search(p, s)        
                if match:
                    start, end = match.span()
                    modified_string = s[:start] + f'{count}' + s[end:]
                    count += 1
                current_question = s
                print(current_question)
            if current_question:
                grouped_data[current_question] = correct
            current_question = item
            correct.extend(temp)
            if item == 'end':
                    grouped_data[current_question] = correct
                    break
            correct = []
            temp = []
            

    return grouped_data

# Example usage

grouped_data = group_questions_and_variants(full_text)
print(grouped_data)
# for question, variants in grouped_data.items():
#     print(question)
#     for variant in variants:
#         print(variant)
#     print()
116/275:
def group_questions_and_variants(lst):
    grouped_data = {}
    current_question = None
    
    cnt = 0
    correct = []
    temp = []
    lst.append('end')
#     print(lst)
    count = 260
    for item in lst:
        
        if item.startswith('+'):
            if current_question:
#                 idx = 0
#                 for i in range(len(item)):
#                     if item[i] =='+':
#                         idx = i
#                         break
                correct.append(item[2:])
#                 grouped_data[current_question].append(item[2:])
        elif item.startswith('-'):
            if current_question:
#                 idx = 0
#                 for i in range(len(item)):
#                     if item[i] =='-':
#                         idx = i
#                         break
#                 correct.append(item[idx+1:])
                temp.append(item[2:])
#                 grouped_data[current_question].append(item[2:])
        else:
            if current_question:
                p = r'\d+'
                s = current_question
                match = re.search(p, s)        
                if match:
                    start, end = match.span()
                    modified_string = s[:start] + f'{count}' + s[end:]
                    count += 1
                    print('sf')
                current_question = s
                print(current_question)
            if current_question:
                grouped_data[current_question] = correct
            current_question = item
            correct.extend(temp)
            if item == 'end':
                    grouped_data[current_question] = correct
                    break
            correct = []
            temp = []
            

    return grouped_data

# Example usage

grouped_data = group_questions_and_variants(full_text)
print(grouped_data)
# for question, variants in grouped_data.items():
#     print(question)
#     for variant in variants:
#         print(variant)
#     print()
116/276:
def group_questions_and_variants(lst):
    grouped_data = {}
    current_question = None
    
    cnt = 0
    correct = []
    temp = []
    lst.append('end')
#     print(lst)
    count = 260
    for item in lst:
        
        if item.startswith('+'):
            if current_question:
#                 idx = 0
#                 for i in range(len(item)):
#                     if item[i] =='+':
#                         idx = i
#                         break
                correct.append(item[2:])
#                 grouped_data[current_question].append(item[2:])
        elif item.startswith('-'):
            if current_question:
#                 idx = 0
#                 for i in range(len(item)):
#                     if item[i] =='-':
#                         idx = i
#                         break
#                 correct.append(item[idx+1:])
                temp.append(item[2:])
#                 grouped_data[current_question].append(item[2:])
        else:
            if current_question:
                p = r'\d+'
                s = current_question
                match = re.search(p, s)        
                if match:
                    start, end = match.span()
                    modified_string = s[:start] + f'{count}' + s[end:]
                    count += 1
                current_question = modified_string
                print(current_question)
            if current_question:
                grouped_data[current_question] = correct
            current_question = item
            correct.extend(temp)
            if item == 'end':
                    grouped_data[current_question] = correct
                    break
            correct = []
            temp = []
            

    return grouped_data

# Example usage

grouped_data = group_questions_and_variants(full_text)
print(grouped_data)
# for question, variants in grouped_data.items():
#     print(question)
#     for variant in variants:
#         print(variant)
#     print()
116/277:
def group_questions_and_variants(lst):
    grouped_data = {}
    current_question = None
    
    cnt = 0
    correct = []
    temp = []
    lst.append('end')
#     print(lst)
    count = 260
    for item in lst:
        
        if item.startswith('+'):
            if current_question:
#                 idx = 0
#                 for i in range(len(item)):
#                     if item[i] =='+':
#                         idx = i
#                         break
                correct.append(item[2:])
#                 grouped_data[current_question].append(item[2:])
        elif item.startswith('-'):
            if current_question:
#                 idx = 0
#                 for i in range(len(item)):
#                     if item[i] =='-':
#                         idx = i
#                         break
#                 correct.append(item[idx+1:])
                temp.append(item[2:])
#                 grouped_data[current_question].append(item[2:])
        else:
            if current_question:
                p = r'\d+'
                s = current_question
                match = re.search(p, s)        
                if match:
                    start, end = match.span()
                    modified_string = s[:start] + f'{count}' + s[end:]
                    count += 1
                current_question = modified_string
#                 print(current_question)
            if current_question:
                grouped_data[current_question] = correct
            current_question = item
            correct.extend(temp)
            if item == 'end':
                    grouped_data[current_question] = correct
                    break
            correct = []
            temp = []
            

    return grouped_data

# Example usage

grouped_data = group_questions_and_variants(full_text)
print(grouped_data)
# for question, variants in grouped_data.items():
#     print(question)
#     for variant in variants:
#         print(variant)
#     print()
116/278:
def group_questions_and_variants(lst):
    grouped_data = {}
    current_question = None
    
    cnt = 0
    correct = []
    temp = []
    lst.append('end')
#     print(lst)
    count = 260
    for item in lst:
        
        if item.startswith('+'):
            if current_question:
#                 idx = 0
#                 for i in range(len(item)):
#                     if item[i] =='+':
#                         idx = i
#                         break
                correct.append(item[2:])
#                 grouped_data[current_question].append(item[2:])
        elif item.startswith('-'):
            if current_question:
#                 idx = 0
#                 for i in range(len(item)):
#                     if item[i] =='-':
#                         idx = i
#                         break
#                 correct.append(item[idx+1:])
                temp.append(item[2:])
#                 grouped_data[current_question].append(item[2:])
        else:
            if current_question:
                p = r'\d+'
                s = current_question
                match = re.search(p, s)        
                if match:
                    start, end = match.span()
                    modified_string = s[:start] + f'{count}' + s[end:]
                    count += 1
                current_question = modified_string
#                 print(current_question)
            if item == 'end':
                grouped_data[current_question] = correct
                break
            if current_question:
                grouped_data[current_question] = correct
            current_question = item
            correct.extend(temp)
            
            correct = []
            temp = []
            

    return grouped_data

# Example usage

grouped_data = group_questions_and_variants(full_text)
print(grouped_data)
# for question, variants in grouped_data.items():
#     print(question)
#     for variant in variants:
#         print(variant)
#     print()
116/279:
def group_questions_and_variants(lst):
    grouped_data = {}
    current_question = None
    
    cnt = 0
    correct = []
    temp = []
    lst.append('end')
#     print(lst)
    count = 260
    for item in lst:
        
        if item.startswith('+'):
            if current_question:
#                 idx = 0
#                 for i in range(len(item)):
#                     if item[i] =='+':
#                         idx = i
#                         break
                correct.append(item[2:])
#                 grouped_data[current_question].append(item[2:])
        elif item.startswith('-'):
            if current_question:
#                 idx = 0
#                 for i in range(len(item)):
#                     if item[i] =='-':
#                         idx = i
#                         break
#                 correct.append(item[idx+1:])
                temp.append(item[2:])
#                 grouped_data[current_question].append(item[2:])
        else:
            if current_question:
                p = r'\d+'
                s = current_question
                match = re.search(p, s)        
                if match:
                    start, end = match.span()
                    modified_string = s[:start] + f'{count}' + s[end:]
                    count += 1
                current_question = modified_string
#                 print(current_question)
            if current_question == 'end':
                break
            if current_question:
                grouped_data[current_question] = correct
            current_question = item
            correct.extend(temp)
            
            correct = []
            temp = []
            

    return grouped_data

# Example usage

grouped_data = group_questions_and_variants(full_text)
print(grouped_data)
# for question, variants in grouped_data.items():
#     print(question)
#     for variant in variants:
#         print(variant)
#     print()
116/280:
def group_questions_and_variants(lst):
    grouped_data = {}
    current_question = None
    
    cnt = 0
    correct = []
    temp = []
    lst.append('end')
#     print(lst)
    count = 260
    for item in lst:
        
        if item.startswith('+'):
            if current_question:
                correct.append(item[2:])
        elif item.startswith('-'):
            if current_question:
                temp.append(item[2:])
        else:
            if current_question:
                p = r'\d+'
                s = current_question
                match = re.search(p, s)        
                if match:
                    start, end = match.span()
                    modified_string = s[:start] + f'{count}' + s[end:]
                    count += 1
                current_question = modified_string
            if current_question == 'end':
                break
            if current_question:
                grouped_data[current_question] = correct
            current_question = item
            correct.extend(temp)
            
            correct = []
            temp = []
            
    grouped_data.pop()
    return grouped_data

# Example usage

grouped_data = group_questions_and_variants(full_text)
print(grouped_data)
# for question, variants in grouped_data.items():
#     print(question)
#     for variant in variants:
#         print(variant)
#     print()
116/281:
def group_questions_and_variants(lst):
    grouped_data = {}
    current_question = None
    
    cnt = 0
    correct = []
    temp = []
    lst.append('end')
#     print(lst)
    count = 260
    for item in lst:
        
        if item.startswith('+'):
            if current_question:
                correct.append(item[2:])
        elif item.startswith('-'):
            if current_question:
                temp.append(item[2:])
        else:
            if current_question:
                p = r'\d+'
                s = current_question
                match = re.search(p, s)        
                if match:
                    start, end = match.span()
                    modified_string = s[:start] + f'{count}' + s[end:]
                    count += 1
                current_question = modified_string
            if current_question == 'end':
                break
            if current_question:
                grouped_data[current_question] = correct
            current_question = item
            correct.extend(temp)
            
            correct = []
            temp = []
            
#     grouped_data.pop()
    return grouped_data

# Example usage

grouped_data = group_questions_and_variants(full_text)
print(grouped_data)
# for question, variants in grouped_data.items():
#     print(question)
#     for variant in variants:
#         print(variant)
#     print()
116/282:
def group_questions_and_variants(lst):
    grouped_data = {}
    current_question = None
    
    cnt = 0
    correct = []
    temp = []
    lst.append('end')
#     print(lst)
    count = 260
    for item in lst:
        
        if item.startswith('+'):
            if current_question:
                correct.append(item[2:])
        elif item.startswith('-'):
            if current_question:
                temp.append(item[2:])
        else:
            if current_question:
                p = r'\d+'
                s = current_question
                match = re.search(p, s)        
                if match:
                    start, end = match.span()
                    modified_string = s[:start] + f'{count}' + s[end:]
                    count += 1
                current_question = modified_string
            
            if current_question:
                grouped_data[current_question] = correct
            current_question = item
            correct.extend(temp)
            if current_question == 'end':
                break
            correct = []
            temp = []
            
#     grouped_data.pop()
    return grouped_data

# Example usage

grouped_data = group_questions_and_variants(full_text)
print(grouped_data)
# for question, variants in grouped_data.items():
#     print(question)
#     for variant in variants:
#         print(variant)
#     print()
116/283:
def group_questions_and_variants(lst):
    grouped_data = {}
    current_question = None
    
    cnt = 0
    correct = []
    temp = []
    lst.append('end')
#     print(lst)
    count = 260
    for item in lst:
        
        if item.startswith('+'):
            if current_question:
                correct.append(item[2:])
        elif item.startswith('-'):
            if current_question:
                temp.append(item[2:])
        else:
            if current_question:
                p = r'\d+'
                s = current_question
                match = re.search(p, s)        
                if match:
                    start, end = match.span()
                    modified_string = s[:start] + f'{count}' + s[end:]
                    count += 1
                current_question = modified_string
            
            if current_question:
                grouped_data[current_question] = correct
            current_question = item
            correct.extend(temp)
            if current_question == 'end':
                break
            correct = []
            temp = []
            
#     grouped_data.pop()
    return grouped_data

# Example usage

grouped_data = group_questions_and_variants(full_text)
print(grouped_data)
# for question, variants in grouped_data.items():
#     print(question)
#     for variant in variants:
#         print(variant)
#     print()
116/284:
from docx import Document
import re

# Create a new Document
doc = Document()

# Add a table with the same number of rows as res and 2 columns
table = doc.add_table(rows=len(res), cols=2)

# Iterate through each row of the table
for i, row_text in enumerate(res):
    cell = table.cell(i, 0)
    cell.text = row_text

    # Leave the second column empty
    cell = table.cell(i, 1)
    cell.text = ""

# Save the document
doc.save('sfa.docx')
116/285: grouped_data
116/286:
from docx import Document
import re

# Create a new Document
doc = Document()

# Add a table with the same number of rows as res and 2 columns
table = doc.add_table(rows=len(res), cols=2)

# Iterate through each row of the table
for i, row_text in enumerate(res):
    cell = table.cell(i, 0)
    cell.text = row_text

    # Leave the second column empty
    cell = table.cell(i, 1)
    cell.text = ""
print(i)
# Save the document
doc.save('sfa.docx')
116/287: len(grouped_data)
116/288:
for x in grouped_data:
    print(x)
116/289:
for x in grouped_data:
    print(x, d[x])
116/290:
for x in grouped_data:
    print(x, grouped_data[x])
116/291:
for x in grouped_data:
#     print(x, grouped_data[x])
    print(x.join(grouped_data[x]))
116/292:
for x in grouped_data:
#     print(x, grouped_data[x])
    print(x.join(grouped_data[x]))
116/293:
for x in grouped_data:
    print(x, grouped_data[x])
#     print(x.join(grouped_data[x]))
116/294:
for x in grouped_data:
#     print(x, grouped_data[x])
    print(grouped_data[x].join(x)
116/295:
for x in grouped_data:
#     print(x, grouped_data[x])
    print(grouped_data[x].join(x))
116/296:
for x in grouped_data:
#     print(x, grouped_data[x])
    print(x.join(grouped_data[x]))
116/297: res
116/298:
temp = []
for x in grouped_data:
    temp.append(x + '\n' + grouped_data[x])
print(temp)
116/299:
temp = []
for x in grouped_data:
    temp.append(x + '\n' + ''.join(grouped_data[x]))
print(temp)
116/300: len(temp)
116/301:
from docx import Document
import re

# Create a new Document
doc = Document()

# Add a table with the same number of rows as res and 2 columns
table = doc.add_table(rows=len(res) + 19, cols=2)

# Iterate through each row of the table
for i, row_text in enumerate(res):
    cell = table.cell(i, 0)
    cell.text = row_text

    # Leave the second column empty
    cell = table.cell(i, 1)
    cell.text = ""
for i in range(len(res), len(res) + len(grouped_data)):
    cell = table.cell(i, 0)
    cell.text = temp[len(res) - i]

    # Leave the second column empty
    cell = table.cell(i, 1)
    cell.text = ""
# Save the document
doc.save('sfa.docx')
116/302:
def group_questions_and_variants(lst):
    grouped_data = {}
    current_question = None
    
    cnt = 0
    correct = []
    temp = []
    lst.append('end')
#     print(lst)
    count = 260
    for item in lst:
        
        if item.startswith('+'):
            if current_question:
                correct.append(item[2:]+'\n')
        elif item.startswith('-'):
            if current_question:
                temp.append(item[2:]+'\n')
        else:
            if current_question:
                p = r'\d+'
                s = current_question
                match = re.search(p, s)        
                if match:
                    start, end = match.span()
                    modified_string = s[:start] + f'{count}' + s[end:]
                    count += 1
                current_question = modified_string
            
            if current_question:
                grouped_data[current_question] = correct
            current_question = item
            correct.extend(temp)
            if current_question == 'end':
                break
            correct = []
            temp = []
            
#     grouped_data.pop()
    return grouped_data

# Example usage

grouped_data = group_questions_and_variants(full_text)
print(grouped_data)
# for question, variants in grouped_data.items():
#     print(question)
#     for variant in variants:
#         print(variant)
#     print()
116/303:
temp = []
for x in grouped_data:
    a = ""
    s = grouped_data[x]
    cnt = 0
    for i in range(len(s)):
        if s[i]=='\n':
            s = s[:i] + f'\n{prefixes[cnt]}. ' + s[i+1:]
            cnt += 1
       
    temp.append(x + '\n' + ''.join(s))
print(temp)
116/304:
temp = []
for x in grouped_data:
    a = ""
    s = grouped_data[x]
    cnt = 0
    print(s)
    for i in range(len(s)):
        if s[i]=='\n':
            s = s[:i] + f'\n{prefixes[cnt]}. ' + s[i+1:]
            cnt += 1
       
    temp.append(x + '\n' + ''.join(s))
# print(temp)
116/305:
temp = []
for x in grouped_data:
    s = grouped_data[x]
    cnt = 0
#     print(s)
    for i in range(len(s)):
        if s[i]=='\n':
            s = s[:i] + f'\n{prefixes[cnt]}. ' + s[i+1:]
            cnt += 1
       
    temp.append(x + '\n' + s)
print(temp)
116/306:
temp = []
for x in grouped_data:
    s = grouped_data[x]
    cnt = 0
#     print(s)
    for i in range(len(s)):
        if s[i]=='\n':
            s = s[:i] + f'\n{prefixes[cnt]}. ' + s[i+1:]
            cnt += 1
        print(s)
    temp.append(x + '\n' + s)
print(temp)
116/307:
temp = []
for x in grouped_data:
    s = ''.join(grouped_data[x])
    cnt = 0
#     print(s)
    for i in range(len(s)):
        if s[i]=='\n':
            s = s[:i] + f'\n{prefixes[cnt]}. ' + s[i+1:]
            cnt += 1
        print(s)
    temp.append(x + '\n' + s)
print(temp)
116/308:
temp = []
for x in grouped_data:
    s = ''.join(grouped_data[x])
    cnt = 0
#     print(s)
    for i in range(len(s)):
        if s[i]=='\n':
            s = s[:i] + f'\n{prefixes[cnt]}. ' + s[i+1:]
            cnt += 1
#         print(s)
    temp.append(x + '\n' + s)
print(temp)
116/309:
from docx import Document
import re

# Create a new Document
doc = Document()

# Add a table with the same number of rows as res and 2 columns
table = doc.add_table(rows=len(res) + 19, cols=2)

# Iterate through each row of the table
for i, row_text in enumerate(res):
    cell = table.cell(i, 0)
    cell.text = row_text

    # Leave the second column empty
    cell = table.cell(i, 1)
    cell.text = ""
for i in range(len(res), len(res) + len(grouped_data)):
    cell = table.cell(i, 0)
    cell.text = temp[len(res) - i]

    # Leave the second column empty
    cell = table.cell(i, 1)
    cell.text = ""
# Save the document
doc.save('sfa.docx')
116/310:
from docx import Document
import re

# Create a new Document
doc = Document()

# Add a table with the same number of rows as res and 2 columns
table = doc.add_table(rows=len(res) + 19, cols=2)

# Iterate through each row of the table
for i, row_text in enumerate(res):
    cell = table.cell(i, 0)
    cell.text = row_text

    # Leave the second column empty
    cell = table.cell(i, 1)
    cell.text = ""
for i in range(len(res), len(res) + len(grouped_data)):
    cell = table.cell(i, 0)
    cell.text = temp[len(res) - i]

    # Leave the second column empty
    cell = table.cell(i, 1)
    cell.text = ""
# Save the document
doc.save('sfa.docx')
116/311:
from docx import Document
import re

# Create a new Document
doc = Document()

# Add a table with the same number of rows as res and 2 columns
table = doc.add_table(rows=len(res) + 19, cols=2)

# Iterate through each row of the table
for i, row_text in enumerate(res):
    cell = table.cell(i, 0)
    cell.text = row_text

    # Leave the second column empty
    cell = table.cell(i, 1)
    cell.text = ""
for i in range(len(res), len(res) + len(grouped_data)):
    cell = table.cell(i, 0)
    cell.text = temp[len(res) - i]

    # Leave the second column empty
    cell = table.cell(i, 1)
    cell.text = ""
# Save the document
doc.save('sfa.docx')
116/312:
temp = []
for x in grouped_data:
    s = ' '.join(grouped_data[x])
    cnt = 0
    print(s)
    for i in range(len(s)):
        if s[i]=='\n':
            s = s[:i] + f'\n{prefixes[cnt]}. ' + s[i+1:]
            cnt += 1
#         print(s)
    temp.append(x + '\n' + s)
print(temp)
116/313:
temp = []
for x in grouped_data:
    s = ' '.join(grouped_data[x])
    cnt = 0
#     print(s)
    for i in range(len(s)):
        if s[i]=='\n':
            s = s[:i] + f'\n{prefixes[cnt]}. ' + s[i+1:]
            cnt += 1
#         print(s)
    temp.append(x + '\n' + s)
print(temp)
116/314:
temp = []
for x in grouped_data:
    s = '\n'.join(grouped_data[x])
    cnt = 0
    print(s)
    for i in range(len(s)):
        if s[i]=='\n':
            s = s[:i] + f'\n{prefixes[cnt]}. ' + s[i+1:]
            cnt += 1
#         print(s)
    temp.append(x + '\n' + s)
print(temp)
116/315:
temp = []
for x in grouped_data:
    s += x
    s = '\n'.join(grouped_data[x])
    cnt = 0
    print(s)
    for i in range(len(s)):
        if s[i]=='\n':
            s = s[:i] + f'\n{prefixes[cnt]}. ' + s[i+1:]
            cnt += 1
#         print(s)
    temp.append(x + '\n' + s)
print(temp)
116/316:
temp = []
for x in grouped_data:
    s = x
    s += '\n'.join(grouped_data[x])
    cnt = 0
    print(s)
    for i in range(len(s)):
        if s[i]=='\n':
            s = s[:i] + f'\n{prefixes[cnt]}. ' + s[i+1:]
            cnt += 1
#         print(s)
    temp.append(x + '\n' + s)
print(temp)
116/317:
temp = []
for x in grouped_data:
    s = x
    s += '\n'.join(grouped_data[x])
    cnt = 0
#     print(s)
    for i in range(len(s)):
        if s[i]=='\n':
            s = s[:i] + f'\n{prefixes[cnt]}. ' + s[i+1:]
            cnt += 1
#         print(s)
    temp.append(x + '\n' + s)
print(temp)
116/318:
temp = []
for x in grouped_data:
    s = x
    s += '\n'.join(grouped_data[x])
    cnt = 0
#     print(s)
    for i in range(len(s)):
        if s[i]=='\n':
            s = s[:i] + f'\n{prefixes[cnt]}. ' + s[i+1:]
            cnt += 1
#         print(s)
    temp.append( s)
print(temp)
116/319:
temp = []
for x in grouped_data:
    s = x + '\n'
    s += ' '.join(grouped_data[x])
    cnt = 0
#     print(s)
    for i in range(len(s)):
        if s[i]=='\n':
            s = s[:i] + f'\n{prefixes[cnt]}. ' + s[i+1:]
            cnt += 1
#         print(s)
    temp.append( s)
print(temp)
116/320:
from docx import Document
import re

# Create a new Document
doc = Document()

# Add a table with the same number of rows as res and 2 columns
table = doc.add_table(rows=len(res) + 19, cols=2)

# Iterate through each row of the table
for i, row_text in enumerate(res):
    cell = table.cell(i, 0)
    cell.text = row_text

    # Leave the second column empty
    cell = table.cell(i, 1)
    cell.text = ""
for i in range(len(res), len(res) + len(grouped_data)):
    cell = table.cell(i, 0)
    cell.text = temp[len(res) - i]

    # Leave the second column empty
    cell = table.cell(i, 1)
    cell.text = ""
# Save the document
doc.save('sfa.docx')
116/321:
from docx import Document
import re

# Create a new Document
doc = Document()

# Add a table with the same number of rows as res and 2 columns
table = doc.add_table(rows=len(res) + 19, cols=2)

# Iterate through each row of the table
for i, row_text in enumerate(res):
    cell = table.cell(i, 0)
    cell.text = row_text

    # Leave the second column empty
    cell = table.cell(i, 1)
    cell.text = ""
for i in range(len(res), len(res) + len(grouped_data)):
    cell = table.cell(i, 0)
    cell.text = temp[len(res) - i]

    # Leave the second column empty
    cell = table.cell(i, 1)
    cell.text = ""
# Save the document
doc.save('sfa.docx')
116/322:
from docx import Document
import re

# Create a new Document
doc = Document()

# Add a table with the same number of rows as res and 2 columns
table = doc.add_table(rows=len(res) + 19, cols=2)

# Iterate through each row of the table
for i, row_text in enumerate(res):
    cell = table.cell(i, 0)
    cell.text = row_text

    # Leave the second column empty
    cell = table.cell(i, 1)
    cell.text = ""
for i in range(len(res), len(res) + len(grouped_data)):
    cell = table.cell(i, 0)
    cell.text = temp[len(res) - i]

    # Leave the second column empty
    cell = table.cell(i, 1)
    cell.text = ""
# Save the document
doc.save('sfa.docx')
116/323:
from docx import Document
import re

# Create a new Document
doc = Document()

# Add a table with the same number of rows as res and 2 columns
table = doc.add_table(rows=len(res) + 19, cols=2)

# Iterate through each row of the table
for i, row_text in enumerate(res):
    cell = table.cell(i, 0)
    cell.text = row_text

    # Leave the second column empty
    cell = table.cell(i, 1)
    cell.text = ""
for i in range(len(res), len(res) + len(grouped_data)):
    cell = table.cell(i, 0)
    cell.text = temp[len(res) - i]
    print(cell.text)
    # Leave the second column empty
    cell = table.cell(i, 1)
    cell.text = ""
# Save the document
doc.save('sfa.docx')
116/324:
from docx import Document
import re

# Create a new Document
doc = Document()

# Add a table with the same number of rows as res and 2 columns
table = doc.add_table(rows=len(res) + 19, cols=2)

# Iterate through each row of the table
for i, row_text in enumerate(res):
    cell = table.cell(i, 0)
    cell.text = row_text

    # Leave the second column empty
    cell = table.cell(i, 1)
    cell.text = ""
for i in range(len(res), len(res) + len(grouped_data)):
    cell = table.cell(i, 0)
    cell.text = temp[len(res) - i]
    print(cell.text, len(res) - i)
    # Leave the second column empty
    cell = table.cell(i, 1)
    cell.text = ""
# Save the document
doc.save('sfa.docx')
116/325:
from docx import Document
import re

# Create a new Document
doc = Document()

# Add a table with the same number of rows as res and 2 columns
table = doc.add_table(rows=len(res) + 19, cols=2)

# Iterate through each row of the table
for i, row_text in enumerate(res):
    cell = table.cell(i, 0)
    cell.text = row_text

    # Leave the second column empty
    cell = table.cell(i, 1)
    cell.text = ""
for i in range(len(res), len(res) + len(grouped_data)):
    cell = table.cell(i, 0)
    cell.text = temp[i - len(res)]
    # Leave the second column empty
    cell = table.cell(i, 1)
    cell.text = ""
# Save the document
doc.save('sfa.docx')
116/326:
from docx import Document
import re

# Create a new Document
doc = Document()

# Add a table with the same number of rows as res and 2 columns
table = doc.add_table(rows=len(res) + 19, cols=2)

# Iterate through each row of the table
for i, row_text in enumerate(res):
    cell = table.cell(i, 0)
    cell.text = row_text

    # Leave the second column empty
    cell = table.cell(i, 1)
    cell.text = ""
for i in range(len(res), len(res) + len(grouped_data)):
    cell = table.cell(i, 0)
    cell.text = temp[i - len(res)]
    # Leave the second column empty
    cell = table.cell(i, 1)
    cell.text = ""
# Save the document
doc.save('sfa.docx')
116/327:
from docx import Document
import re

# Create a new Document
doc = Document()

# Add a table with the same number of rows as res and 2 columns
table = doc.add_table(rows=len(res) + 19, cols=2)

# Iterate through each row of the table
for i, row_text in enumerate(res):
    cell = table.cell(i, 0)
    cell.text = row_text

    # Leave the second column empty
    cell = table.cell(i, 1)
    cell.text = ""
for i in range(len(res), len(res) + len(grouped_data)):
    cell = table.cell(i, 0)
    cell.text = temp[i - len(res)]
    # Leave the second column empty
    cell = table.cell(i, 1)
    cell.text = ""
# Save the document
doc.save('sfa.docx')
116/328:

doc = Document('3.docx')
full_text = []
for paragraph in doc.paragraphs:
#     print(paragraph.text)
    full_text.append(paragraph.text)

print(full_text)
116/329:

doc = Document('3.docx')
full_text = []
for paragraph in doc.paragraphs:
#     print(paragraph.text)
    full_text.append(paragraph.text)

print(full_text)
116/330:
# Given list
data = [
    '',
    '1. Қазақстандағы тағам  гигиенасын  негізін қалаушы:',
    'A.\tШарманов Т.Ш.',
    'B.\tАмрин К.Р.;',
    'C.\tСоловьев З.П.',
    'D.\tЧарльз Дарвин',
    'E.\tНеменко Б.А.',
    '2. Гигиенаның түрлері:',
    'A.\tеңбек гигиенасы;',
    'B.\tэнтомология;',
    'C.\tпаразитология;',
    'D.\tжануарлардың мінез-құлық ғылымы',
    'E.\tэкология.',
    '3. Биологиялық фактор:',
    'A.\tмикроорганизмдер;',
    'B.\tылғалдылық;',
    'C.\tөсімдіктер, жәндіктер;',
    'D.\tкүн сәулесі.'
]

# Filter out empty strings and numbers
formatted_data = [item for item in data if item and not item[0].isdigit()]

# Join the items with a newline
formatted_text = '\n'.join(formatted_data)

print(formatted_text)
116/331:
# Given list
data = ['', '1. Қазақстандағы тағам  гигиенасын  негізін қалаушы:', 'A.\tШарманов Т.Ш.', 'B.\tАмрин К.Р.;', 'C.\tСоловьев З.П.', 'D.\tЧарльз Дарвин', 'E.\tНеменко Б.А.', '2. Гигиенаның түрлері:', 'A.\tеңбек гигиенасы;', 'B.\tэнтомология;', 'C.\tпаразитология;', 'D.\tжануарлардың мінез-құлық ғылымы', 'E.\tэкология.', '3. Биологиялық фактор:', 'A.\tмикроорганизмдер;', 'B.\tылғалдылық;', 'C.\tөсімдіктер, жәндіктер;', 'D.\tкүн сәулесі.']

# Filter out empty strings and split lines
formatted_data = [line.strip() for line in data if line.strip()]

# Filter out lines that do not start with a number
formatted_data = [line for line in formatted_data if line[0].isdigit()]

# Print the formatted list
for item in formatted_data:
    print(item)
116/332:
# Given list
data = ['', '1. Қазақстандағы тағам  гигиенасын  негізін қалаушы:', 'A.\tШарманов Т.Ш.', 'B.\tАмрин К.Р.;', 'C.\tСоловьев З.П.', 'D.\tЧарльз Дарвин', 'E.\tНеменко Б.А.', '2. Гигиенаның түрлері:', 'A.\tеңбек гигиенасы;', 'B.\tэнтомология;', 'C.\tпаразитология;', 'D.\tжануарлардың мінез-құлық ғылымы', 'E.\tэкология.', '3. Биологиялық фактор:', 'A.\tмикроорганизмдер;', 'B.\tылғалдылық;', 'C.\tөсімдіктер, жәндіктер;', 'D.\tкүн сәулесі.']

questions = []
options = {}

current_question = None

for item in data:
    if item.strip().isdigit():
        current_question = item.strip()
        questions.append(current_question)
        options[current_question] = []
    elif current_question:
        options[current_question].append(item.strip())

# Print the formatted questions and options
for question in questions:
    print(question)
    for option in options[question]:
        print(option)
116/333:
# Given list
data = ['', '1. Қазақстандағы тағам  гигиенасын  негізін қалаушы:', 'A.\tШарманов Т.Ш.', 'B.\tАмрин К.Р.;', 'C.\tСоловьев З.П.', 'D.\tЧарльз Дарвин', 'E.\tНеменко Б.А.', '2. Гигиенаның түрлері:', 'A.\tеңбек гигиенасы;', 'B.\tэнтомология;', 'C.\tпаразитология;', 'D.\tжануарлардың мінез-құлық ғылымы', 'E.\tэкология.', '3. Биологиялық фактор:', 'A.\tмикроорганизмдер;', 'B.\tылғалдылық;', 'C.\tөсімдіктер, жәндіктер;', 'D.\tкүн сәулесі.']

questions = []
options = {}

current_question = None

for item in data:
    if item.strip().isdigit():
        current_question = item.strip()
        questions.append(current_question)
        options[current_question] = []
    elif current_question:
        options[current_question].append(item.strip())

# Print the formatted questions and options
for question in questions:
    print(question)
    for option in options[question]:
        print(option)
116/334:
# Given list
data = ['', '1. Қазақстандағы тағам  гигиенасын  негізін қалаушы:', 'A.\tШарманов Т.Ш.', 'B.\tАмрин К.Р.;', 'C.\tСоловьев З.П.', 'D.\tЧарльз Дарвин', 'E.\tНеменко Б.А.', '2. Гигиенаның түрлері:', 'A.\tеңбек гигиенасы;', 'B.\tэнтомология;', 'C.\tпаразитология;', 'D.\tжануарлардың мінез-құлық ғылымы', 'E.\tэкология.', '3. Биологиялық фактор:', 'A.\tмикроорганизмдер;', 'B.\tылғалдылық;', 'C.\tөсімдіктер, жәндіктер;', 'D.\tкүн сәулесі.']

questions = {}
current_question = None

for item in data:
    if item.strip().isdigit():
        current_question = item.strip()
        questions[current_question] = []
    elif current_question and item.strip().startswith(('A.', 'B.', 'C.', 'D.', 'E.')):
        questions[current_question].append(item.strip())

# Print the formatted questions and options
for question, options in questions.items():
    print(question)
    for option in options:
        print(option)
116/335: formatted_data = [line for line in full_text if line[0].isdigit()]
116/336:
d = {}
for s in full_text
116/337:
d = {}
for s in full_text:
    print(s)
116/338:
d = {}
curr = None
full_text.append('end')
for s in full_text:
    if len(s):
        if s[0].isdigit():
            print(s)
116/339:
d = {}
curr = None
full_text.append('end')
for s in full_text:
    if len(s):
        s = s.strip()
        if s[0].isdigit():
            cur = s
116/340:
d = {}
curr = None
full_text.append('end')
for s in full_text:
    if len(s):
        s = s.strip()
        if s == 'end':
            break
        if s[0].isdigit():
            cur = s
            d[cur] = []
        else:
            d[cur].append(s)
print(d)
116/341:
ans = []
for x in d:
    s = x + '\n'
    s += ' '.join(grouped_data[x])
    cnt = 0
#     print(s)
    for i in range(len(s)):
        if s[i]=='\n':
            s = s[:i] + f'\n{prefixes[cnt]}. ' + s[i+1:]
            cnt += 1
#         print(s)
    ans.append( s)
print(ans)
116/342:
ans = []
for x in d:
    print(x)
#     s = x + '\n'
#     s += ' '.join(grouped_data[x])
#     cnt = 0
# #     print(s)
#     for i in range(len(s)):
#         if s[i]=='\n':
#             s = s[:i] + f'\n{prefixes[cnt]}. ' + s[i+1:]
#             cnt += 1
# #         print(s)
#     ans.append( s)
print(ans)
116/343:
ans = []
for x in d:
    print(x)
    s = x + '\n'
    s += ' '.join(d[x])
    cnt = 0
#     print(s)
    for i in range(len(s)):
        if s[i]=='\n':
            s = s[:i] + f'\n{prefixes[cnt]}. ' + s[i+1:]
            cnt += 1
#         print(s)
    ans.append( s)
print(ans)
116/344: d
116/345:
ans = []
for x in d:
#     print(x)
    s = x + '\n'
    s += ' '.join(d[x])
    print(s)
    cnt = 0
#     print(s)
    for i in range(len(s)):
        if s[i]=='\n':
            s = s[:i] + f'\n{prefixes[cnt]}. ' + s[i+1:]
            cnt += 1
#         print(s)
    ans.append(s)
print(ans)
116/346:
ans = []
for x in d:
#     print(x)
    s = x + '\n'
    s += ' '.join(d[x])
    pri/nt(s)
    cnt = 0
#     print(s)
#     for i in range(len(s)):
#         if s[i]=='\n':
#             s = s[:i] + f'\n{prefixes[cnt]}. ' + s[i+1:]
#             cnt += 1
# #         print(s)
    ans.append(s)
print(ans)
116/347:
ans = []
for x in d:
#     print(x)
    s = x + '\n'
    s += ' '.join(d[x])
#     pri/nt(s)
    cnt = 0
#     print(s)
#     for i in range(len(s)):
#         if s[i]=='\n':
#             s = s[:i] + f'\n{prefixes[cnt]}. ' + s[i+1:]
#             cnt += 1
# #         print(s)
    ans.append(s)
print(ans)
116/348:
ans = []
for x in d:
#     print(x)
    s = x + '\n'
    s += ' '.join(d[x])
#     pri/nt(s)
    cnt = 0
    s = s.replace('\t', '')
#     print(s)
#     for i in range(len(s)):
#         if s[i]=='\n':
#             s = s[:i] + f'\n{prefixes[cnt]}. ' + s[i+1:]
#             cnt += 1
# #         print(s)
    ans.append(s)
print(ans)
116/349:
ans = []
for x in d:
#     print(x)
    s = x + '\n'
    s += ' '.join(d[x])
#     pri/nt(s)
    cnt = 0
    s = s.replace('\t', ' ')
#     print(s)
#     for i in range(len(s)):
#         if s[i]=='\n':
#             s = s[:i] + f'\n{prefixes[cnt]}. ' + s[i+1:]
#             cnt += 1
# #         print(s)
    ans.append(s)
print(ans)
116/350:
d = {}
curr = None
count = 1
full_text.append('end')
for s in full_text:
    if len(s):
        s = s.strip()
        if s == 'end':
            break
        if s[0].isdigit():
            cur = s
            p = r'\d+'
            match = re.search(p, cur)        
            if match:
                start, end = match.span()
                modified_string = cur[:start] + f'{count}' + cur[end:]
                count += 1
            cur = modified_string
            d[cur] = []
        else:
            d[cur].append(s)
print(d)
116/351:
d = {}
curr = None
count = 43
full_text.append('end')
for s in full_text:
    if len(s):
        s = s.strip()
        if s == 'end':
            break
        if s[0].isdigit():
            cur = s
            p = r'\d+'
            match = re.search(p, cur)        
            if match:
                start, end = match.span()
                modified_string = cur[:start] + f'{count}' + cur[end:]
                count += 1
            cur = modified_string
            d[cur] = []
        else:
            d[cur].append(s)
print(d)
116/352:
from docx import Document
import re

# Create a new Document
doc = Document()

# Add a table with the same number of rows as res and 2 columns
table = doc.add_table(rows=len(ans) + 19, cols=2)

# Iterate through each row of the table
for i, row_text in enumerate(ans):
    cell = table.cell(i, 0)
    cell.text = row_text

    # Leave the second column empty
    cell = table.cell(i, 1)
    cell.text = ""
for i in range(len(res), len(res) + len(grouped_data)):
    cell = table.cell(i, 0)
    cell.text = temp[i - len(res)]
    # Leave the second column empty
    cell = table.cell(i, 1)
    cell.text = ""
# Save the document
doc.save('new.docx')
116/353:
count 
ans = []
for x in d:
#     print(x)
    s = x + '\n'
    s += ' '.join(d[x])
#     pri/nt(s)
    cnt = 0
    s = s.replace('\t', ' ')
#     print(s)
#     for i in range(len(s)):
#         if s[i]=='\n':
#             s = s[:i] + f'\n{prefixes[cnt]}. ' + s[i+1:]
#             cnt += 1
# #         print(s)
    ans.append(s)
# print(ans)
116/354:
count 
ans = []
for x in d:
#     print(x)
    s = x + '\n'
    s += ' '.join(d[x])
#     pri/nt(s)
    cnt = 0
    s = s.replace('\t', ' ')
#     print(s)
#     for i in range(len(s)):
#         if s[i]=='\n':
#             s = s[:i] + f'\n{prefixes[cnt]}. ' + s[i+1:]
#             cnt += 1
# #         print(s)
    ans.append(s)
print(ans)
116/355:
from docx import Document
import re

# Create a new Document
doc = Document()

# Add a table with the same number of rows as res and 2 columns
table = doc.add_table(rows=len(ans) + 19, cols=2)

# Iterate through each row of the table
for i, row_text in enumerate(ans):
    cell = table.cell(i, 0)
    cell.text = row_text

    # Leave the second column empty
    cell = table.cell(i, 1)
    cell.text = ""
for i in range(len(ans), len(res) + len(grouped_data)):
    cell = table.cell(i, 0)
    cell.text = temp[i - len(res)]
    # Leave the second column empty
    cell = table.cell(i, 1)
    cell.text = ""
# Save the document
doc.save('new.docx')
116/356:
from docx import Document
import re

# Create a new Document
doc = Document()

# Add a table with the same number of rows as res and 2 columns
table = doc.add_table(rows=len(ans) + 19, cols=2)

# Iterate through each row of the table
for i, row_text in enumerate(ans):
    cell = table.cell(i, 0)
    cell.text = row_text

    # Leave the second column empty
    cell = table.cell(i, 1)
    cell.text = ""
for i in range(len(ans), len(ans) + len(grouped_data)):
    cell = table.cell(i, 0)
    cell.text = temp[i - len(res)]
    # Leave the second column empty
    cell = table.cell(i, 1)
    cell.text = ""
# Save the document
doc.save('new.docx')
116/357:
count 
ans = []
for x in d:
#     print(x)
    s = x + '\n'
    s += '\n'.join(d[x])
#     pri/nt(s)
    cnt = 0
    s = s.replace('\t', ' ')
#     print(s)
#     for i in range(len(s)):
#         if s[i]=='\n':
#             s = s[:i] + f'\n{prefixes[cnt]}. ' + s[i+1:]
#             cnt += 1
# #         print(s)
    ans.append(s)
print(ans)
116/358:
from docx import Document
import re

# Create a new Document
doc = Document()

# Add a table with the same number of rows as res and 2 columns
table = doc.add_table(rows=len(ans) + 19, cols=2)

# Iterate through each row of the table
for i, row_text in enumerate(ans):
    cell = table.cell(i, 0)
    cell.text = row_text

    # Leave the second column empty
    cell = table.cell(i, 1)
    cell.text = ""
for i in range(len(ans), len(ans) + len(grouped_data)):
    cell = table.cell(i, 0)
    cell.text = temp[i - len(res)]
    # Leave the second column empty
    cell = table.cell(i, 1)
    cell.text = ""
# Save the document
doc.save('new.docx')
116/359:
from docx import Document
import re

# Create a new Document
doc = Document()

# Add a table with the same number of rows as res and 2 columns
table = doc.add_table(rows=len(ans) + 19, cols=2)

# Iterate through each row of the table
for i, row_text in enumerate(ans):
    cell = table.cell(i, 0)
    cell.text = row_text

    # Leave the second column empty
    cell = table.cell(i, 1)
    cell.text = ""
for i in range(len(ans), len(ans) + len(grouped_data)):
    cell = table.cell(i, 0)
    cell.text = temp[i - len(res)]
    # Leave the second column empty
    cell = table.cell(i, 1)
    cell.text = ""
# Save the document
doc.save('new.docx')
116/360:
def group_questions_and_variants(lst):
    grouped_data = {}
    current_question = None
    
    cnt = 0
    correct = []
    temp = []
    lst.append('end')
#     print(lst)
    count = 260
    for item in lst:
        
        if item.startswith('+'):
            if current_question:
                correct.append(item[2:]+'\n')
        elif item.startswith('-'):
            if current_question:
                temp.append(item[2:]+'\n')
        else:
            if current_question:
                p = r'\d+'
                s = current_question
                match = re.search(p, s)        
                if match:
                    start, end = match.span()
                    modified_string = s[:start] + f'{count}' + s[end:]
                    count += 1
                current_question = modified_string
            
            if current_question:
                grouped_data[current_question] = correct
            current_question = item
            correct.extend(temp)
            if current_question == 'end':
                break
            correct = []
            temp = []
            
#     grouped_data.pop()
    return grouped_data

# Example usage

grouped_data = group_questions_and_variants(full_text)
print(grouped_data)
# for question, variants in grouped_data.items():
#     print(question)
#     for variant in variants:
#         print(variant)
#     print()
116/361:

doc = Document('2.docx')
full_text = []
for paragraph in doc.paragraphs:
#     print(paragraph.text)
    full_text.append(paragraph.text)

print(full_text)
116/362:
def group_questions_and_variants(lst):
    grouped_data = {}
    current_question = None
    
    cnt = 0
    correct = []
    temp = []
    lst.append('end')
#     print(lst)
    count = 260
    for item in lst:
        
        if item.startswith('+'):
            if current_question:
                correct.append(item[2:]+'\n')
        elif item.startswith('-'):
            if current_question:
                temp.append(item[2:]+'\n')
        else:
            if current_question:
                p = r'\d+'
                s = current_question
                match = re.search(p, s)        
                if match:
                    start, end = match.span()
                    modified_string = s[:start] + f'{count}' + s[end:]
                    count += 1
                current_question = modified_string
            
            if current_question:
                grouped_data[current_question] = correct
            current_question = item
            correct.extend(temp)
            if current_question == 'end':
                break
            correct = []
            temp = []
            
#     grouped_data.pop()
    return grouped_data

# Example usage

grouped_data = group_questions_and_variants(full_text)
print(grouped_data)
# for question, variants in grouped_data.items():
#     print(question)
#     for variant in variants:
#         print(variant)
#     print()
116/363:
def group_questions_and_variants(lst):
    grouped_data = {}
    current_question = None
    
    cnt = 0
    correct = []
    temp = []
    lst.append('end')
#     print(lst)
    count = 260
    for item in lst:
        
        if item.startswith('+'):
            if current_question:
                correct.append(item[2:]+'\n')
        elif item.startswith('-'):
            if current_question:
                temp.append(item[2:]+'\n')
        else:
            if current_question:
                p = r'\d+'
                s = current_question
                match = re.search(p, s)        
                if match:
                    start, end = match.span()
                    modified_string = s[:start] + f'{count}' + s[end:]
                    count += 1
                current_question = modified_string
            
            if current_question:
                grouped_data[current_question] = correct
            current_question = item
            correct.extend(temp)
            if current_question == 'end':
                break
            correct = []
            temp = []
            
#     grouped_data.pop()
    return grouped_data

# Example usage

grouped_data = group_questions_and_variants(full_text)
print(grouped_data)
# for question, variants in grouped_data.items():
#     print(question)
#     for variant in variants:
#         print(variant)
#     print()
116/364:
def group_questions_and_variants(lst):
    grouped_data = {}
    current_question = None
    
    cnt = 0
    correct = []
    temp = []
    lst.append('end')
#     print(lst)
    count = len(ans) + 1
    for item in lst:
        
        if item.startswith('+'):
            if current_question:
                correct.append(item[2:]+'\n')
        elif item.startswith('-'):
            if current_question:
                temp.append(item[2:]+'\n')
        else:
            if current_question:
                p = r'\d+'
                s = current_question
                match = re.search(p, s)        
                if match:
                    start, end = match.span()
                    modified_string = s[:start] + f'{count}' + s[end:]
                    count += 1
                current_question = modified_string
            
            if current_question:
                grouped_data[current_question] = correct
            current_question = item
            correct.extend(temp)
            if current_question == 'end':
                break
            correct = []
            temp = []
            
#     grouped_data.pop()
    return grouped_data

# Example usage

grouped_data = group_questions_and_variants(full_text)
print(grouped_data)
# for question, variants in grouped_data.items():
#     print(question)
#     for variant in variants:
#         print(variant)
#     print()
116/365:
def group_questions_and_variants(lst):
    grouped_data = {}
    current_question = None
    
    cnt = 0
    correct = []
    temp = []
    lst.append('end')
#     print(lst)
    count = 244
    for item in lst:
        
        if item.startswith('+'):
            if current_question:
                correct.append(item[2:]+'\n')
        elif item.startswith('-'):
            if current_question:
                temp.append(item[2:]+'\n')
        else:
            if current_question:
                p = r'\d+'
                s = current_question
                match = re.search(p, s)        
                if match:
                    start, end = match.span()
                    modified_string = s[:start] + f'{count}' + s[end:]
                    count += 1
                current_question = modified_string
            
            if current_question:
                grouped_data[current_question] = correct
            current_question = item
            correct.extend(temp)
            if current_question == 'end':
                break
            correct = []
            temp = []
            
#     grouped_data.pop()
    return grouped_data

# Example usage

grouped_data = group_questions_and_variants(full_text)
print(grouped_data)
# for question, variants in grouped_data.items():
#     print(question)
#     for variant in variants:
#         print(variant)
#     print()
116/366:
def group_questions_and_variants(lst):
    grouped_data = {}
    current_question = None
    
    cnt = 0
    correct = []
    temp = []
    lst.append('end')
#     print(lst)
    count = 245
    for item in lst:
        
        if item.startswith('+'):
            if current_question:
                correct.append(item[2:]+'\n')
        elif item.startswith('-'):
            if current_question:
                temp.append(item[2:]+'\n')
        else:
            if current_question:
                p = r'\d+'
                s = current_question
                match = re.search(p, s)        
                if match:
                    start, end = match.span()
                    modified_string = s[:start] + f'{count}' + s[end:]
                    count += 1
                current_question = modified_string
            
            if current_question:
                grouped_data[current_question] = correct
            current_question = item
            correct.extend(temp)
            if current_question == 'end':
                break
            correct = []
            temp = []
            
#     grouped_data.pop()
    return grouped_data

# Example usage

grouped_data = group_questions_and_variants(full_text)
print(grouped_data)
# for question, variants in grouped_data.items():
#     print(question)
#     for variant in variants:
#         print(variant)
#     print()
116/367:
def group_questions_and_variants(lst):
    grouped_data = {}
    current_question = None
    
    cnt = 0
    correct = []
    temp = []
    lst.append('end')
#     print(lst)
    count = 245
    for item in lst:
        
        if item.startswith('+'):
            if current_question:
                correct.append(item[2:]+'\n')
        elif item.startswith('-'):
            if current_question:
                temp.append(item[2:]+'\n')
        else:
            if current_question:
                p = r'\d+'
                s = current_question
                match = re.search(p, s)        
                if match:
                    start, end = match.span()
                    modified_string = s[:start] + f'{count}' + s[end:]
                    count += 1
                current_question = modified_string
            
            if current_question:
                grouped_data[current_question] = correct
            current_question = item
            correct.extend(temp)
            if current_question == 'end':
                break
            correct = []
            temp = []
            
#     grouped_data.pop()
    return grouped_data

# Example usage

grouped_data = group_questions_and_variants(full_text)
print(grouped_data)
# for question, variants in grouped_data.items():
#     print(question)
#     for variant in variants:
#         print(variant)
#     print()
116/368:
def group_questions_and_variants(lst):
    grouped_data = {}
    current_question = None
    
    cnt = 0
    correct = []
    temp = []
    lst.append('end')
#     print(lst)
    count = 245
    for item in lst:
        print(current_question)
        if item.startswith('+'):
            if current_question:
                correct.append(item[2:]+'\n')
        elif item.startswith('-'):
            if current_question:
                temp.append(item[2:]+'\n')
        else:
            if current_question:
                p = r'\d+'
                s = current_question
                match = re.search(p, s)        
                if match:
                    start, end = match.span()
                    modified_string = s[:start] + f'{count}' + s[end:]
                    count += 1
                current_question = modified_string
            
            if current_question:
                grouped_data[current_question] = correct
            current_question = item
            correct.extend(temp)
            if current_question == 'end':
                break
            correct = []
            temp = []
            
#     grouped_data.pop()
    return grouped_data

# Example usage

grouped_data = group_questions_and_variants(full_text)
print(grouped_data)
# for question, variants in grouped_data.items():
#     print(question)
#     for variant in variants:
#         print(variant)
#     print()
116/369:

doc = Document('2.docx')
full_text = []
for paragraph in doc.paragraphs:
#     print(paragraph.text)
    full_text.append(paragraph.text)

print(full_text)
116/370:
second = {}
cur = None
count = 245
c = []
w = []
full_text.append('end')
for s in full_text:
    if len(s):
        s = s.strip()
        if s == 'end':
            break
        if s[0] == '+':
            c.append(s)
        elif s[0] == '-':
            w.append(x)
        else:
            if cur:
                c.extend(w)
                second[cur].append(c)
            cur = s
            p = r'\d+'
            match = re.search(p, cur)        
            if match:
                start, end = match.span()
                modified_string = cur[:start] + f'{count}' + cur[end:]
                count += 1
            cur = modified_string
            d[cur] = []
            c = []
            w = []
        else:
            d[cur].append(s)
print(second)
116/371:
second = {}
cur = None
count = 245
c = []
w = []
full_text.append('end')
for s in full_text:
    if len(s):
        s = s.strip()
        if s == 'end':
            break
        if s[0] == '+':
            c.append(s)
        elif s[0] == '-':
            w.append(x)
        else:
            if cur:
                c.extend(w)
                second[cur].append(c)
            cur = s
            p = r'\d+'
            match = re.search(p, cur)        
            if match:
                start, end = match.span()
                modified_string = cur[:start] + f'{count}' + cur[end:]
                count += 1
            cur = modified_string
            d[cur] = []
            c = []
            w = []
print(second)
116/372:
second = {}
cur = None
count = 245
c = []
w = []
full_text.append('end')
for s in full_text:
    if len(s):
        print(s)
        s = s.strip()
        if s == 'end':
            break
        if s[0] == '+':
            c.append(s)
        elif s[0] == '-':
            w.append(x)
        else:
            if cur:
                c.extend(w)
                second[cur].append(c)
            cur = s
            p = r'\d+'
            match = re.search(p, cur)        
            if match:
                start, end = match.span()
                modified_string = cur[:start] + f'{count}' + cur[end:]
                count += 1
            cur = modified_string
            d[cur] = []
            c = []
            w = []
print(second)
116/373:
second = {}
cur = None
count = 245
c = []
w = []
full_text.append('end')
for s in full_text:
    if len(s):
        print(s)
        s = s.strip()
        if s == 'end':
            break
        if s[0] == '+':
            c.append(s)
        elif s[0] == '-':
            w.append(x)
        else:
            if cur:
                c.extend(w)
                second[cur].append(c)
            cur = s
            p = r'\d+'
            match = re.search(p, cur)        
            if match:
                start, end = match.span()
                modified_string = cur[:start] + f'{count}' + cur[end:]
                count += 1
            cur = modified_string
            second[cur] = []
            c = []
            w = []
print(second)
116/374:
second = {}
cur = None
count = 245
c = []
w = []
full_text.append('end')
for s in full_text:
    if len(s):
#         print(s)
        s = s.strip()
        if s == 'end':
            break
        if s[0] == '+':
            c.append(s)
        elif s[0] == '-':
            w.append(x)
        else:
            if cur:
                c.extend(w)
                second[cur].append(c)
            cur = s
            p = r'\d+'
            match = re.search(p, cur)        
            if match:
                start, end = match.span()
                modified_string = cur[:start] + f'{count}' + cur[end:]
                count += 1
            cur = modified_string
            second[cur] = []
            c = []
            w = []
print(second)
116/375:
second = {}
cur = None
count = 245
c = []
w = []
full_text.append('end')
for s in full_text:
    if len(s):
#         print(s)
        s = s.strip()
        if s == 'end':
            break
        if s[0] == '+':
            c.append(s[2:])
        elif s[0] == '-':
            w.append(s[2:])
        else:
            if cur:
                c.extend(w)
                second[cur].append(c)
            cur = s
            p = r'\d+'
            match = re.search(p, cur)        
            if match:
                start, end = match.span()
                modified_string = cur[:start] + f'{count}' + cur[end:]
                count += 1
            cur = modified_string
            second[cur] = []
            c = []
            w = []
print(second)
116/376:
second = {}
cur = None
count = 245
c = []
w = []
full_text.append('end')
for s in full_text:
    if len(s):
#         print(s)
        s = s.strip()
        if s == 'end':
            break
        if s[0] == '+':
            c.append(s[2:])
        elif s[0] == '-':
            w.append(s[2:])
        else:
            if cur:
                c.extend(w)
                second[cur] = c
            cur = s
            p = r'\d+'
            match = re.search(p, cur)        
            if match:
                start, end = match.span()
                modified_string = cur[:start] + f'{count}' + cur[end:]
                count += 1
            cur = modified_string
            second[cur] = []
            c = []
            w = []
print(second)
116/377:
second = {}
cur = None
count = 245
c = []
w = []
full_text.append('end')
for s in full_text:
    if len(s):
#         print(s)
        s = s.strip()
        
        if s[0] == '+':
            c.append(s[2:])
        elif s[0] == '-':
            w.append(s[2:])
        else:
            if cur:
                c.extend(w)
                second[cur] = c
            if s == 'end':
                break
            cur = s
            p = r'\d+'
            match = re.search(p, cur)        
            if match:
                start, end = match.span()
                modified_string = cur[:start] + f'{count}' + cur[end:]
                count += 1
            cur = modified_string
            second[cur] = []
            c = []
            w = []
print(second)
116/378:
www = []
for x in second:
    s = x + '\n'
    s += ' '.join(second[x])
    cnt = 0
#     print(s)
    for i in range(len(s)):
        if s[i]=='\n':
            s = s[:i] + f'\n{prefixes[cnt]}. ' + s[i+1:]
            cnt += 1
#         print(s)
    www.append( s)
print(www)
116/379:
www = []
for x in second:
    s = x + '\n'
    s += '\n'.join(second[x])
    cnt = 0
#     print(s)
    for i in range(len(s)):
        if s[i]=='\n':
            s = s[:i] + f'\n{prefixes[cnt]}. ' + s[i+1:]
            cnt += 1
#         print(s)
    www.append( s)
print(www)
116/380:
from docx import Document
import re

# Create a new Document
doc = Document()

# Add a table with the same number of rows as res and 2 columns
table = doc.add_table(rows=len(ans) + 19, cols=2)

# Iterate through each row of the table
for i, row_text in enumerate(ans):
    cell = table.cell(i, 0)
    cell.text = row_text

    # Leave the second column empty
    cell = table.cell(i, 1)
    cell.text = ""
for i in range(len(ans), len(ans) + len(second)):
    cell = table.cell(i, 0)
    cell.text = temp[i - len(res)]
    # Leave the second column empty
    cell = table.cell(i, 1)
    cell.text = ""
# Save the document
doc.save('new.docx')
116/381:
from docx import Document
import re

# Create a new Document
doc = Document()

# Add a table with the same number of rows as res and 2 columns
table = doc.add_table(rows=len(ans) + 19, cols=2)

# Iterate through each row of the table
for i, row_text in enumerate(ans):
    cell = table.cell(i, 0)
    cell.text = row_text

    # Leave the second column empty
    cell = table.cell(i, 1)
    cell.text = ""
for i in range(len(ans), len(ans) + len(second)):
    cell = table.cell(i, 0)
    cell.text = www[i - len(res)]
    # Leave the second column empty
    cell = table.cell(i, 1)
    cell.text = ""
# Save the document
doc.save('new.docx')
116/382:
from docx import Document
import re

# Create a new Document
doc = Document()

# Add a table with the same number of rows as res and 2 columns
table = doc.add_table(rows=len(ans) + 19, cols=2)

# Iterate through each row of the table
for i, row_text in enumerate(ans):
    cell = table.cell(i, 0)
    cell.text = row_text

    # Leave the second column empty
    cell = table.cell(i, 1)
    cell.text = ""
for i in range(len(ans), len(ans) + len(second)):
    cell = table.cell(i, 0)
    cell.text = www[i - len(res)]
    # Leave the second column empty
    cell = table.cell(i, 1)
    cell.text = ""
# Save the document
doc.save('new.docx')
116/383:
from docx import Document
import re

# Create a new Document
doc = Document()

# Add a table with the same number of rows as res and 2 columns
table = doc.add_table(rows=len(ans) + 19, cols=2)

# Iterate through each row of the table
for i, row_text in enumerate(ans):
    cell = table.cell(i, 0)
    cell.text = row_text

    # Leave the second column empty
    cell = table.cell(i, 1)
    cell.text = ""
for i in range(len(ans), len(ans) + len(second)):
    cell = table.cell(i, 0)
    cell.text = www[i - len(ans)]
    # Leave the second column empty
    cell = table.cell(i, 1)
    cell.text = ""
# Save the document
doc.save('new.docx')
   1:
from docx import Document

doc  = Document('8.docx')
   2:
from docx import Document

# Open the .docx file
doc = Document('1.docx')

# Iterate through each paragraph in the document
for paragraph in doc.paragraphs:
    print(paragraph.text)
   3:
from docx import Document

doc  = Document('8.docx')
   4: from docx import Document
   5:

doc = Document('3.docx')
full_text = []
for paragraph in doc.paragraphs:
#     print(paragraph.text)
    full_text.append(paragraph.text)

print(full_text)
   6:
count 
ans = []
for x in d:
#     print(x)
    s = x + '\n'
    s += '\n'.join(d[x])
#     pri/nt(s)
    cnt = 0
    s = s.replace('\t', ' ')
#     print(s)
#     for i in range(len(s)):
#         if s[i]=='\n':
#             s = s[:i] + f'\n{prefixes[cnt]}. ' + s[i+1:]
#             cnt += 1
# #         print(s)
    ans.append(s)
print(ans)
   7:
# count 
ans = []
for x in d:
#     print(x)
    s = x + '\n'
    s += '\n'.join(d[x])
#     pri/nt(s)
    cnt = 0
    s = s.replace('\t', ' ')
#     print(s)
#     for i in range(len(s)):
#         if s[i]=='\n':
#             s = s[:i] + f'\n{prefixes[cnt]}. ' + s[i+1:]
#             cnt += 1
# #         print(s)
    ans.append(s)
print(ans)
   8:
d = {}
curr = None
count = 43
full_text.append('end')
for s in full_text:
    if len(s):
        s = s.strip()
        if s == 'end':
            break
        if s[0].isdigit():
            cur = s
            p = r'\d+'
            match = re.search(p, cur)        
            if match:
                start, end = match.span()
                modified_string = cur[:start] + f'{count}' + cur[end:]
                count += 1
            cur = modified_string
            d[cur] = []
        else:
            d[cur].append(s)
print(d)
   9:
import res
d = {}
curr = None
count = 43
full_text.append('end')
for s in full_text:
    if len(s):
        s = s.strip()
        if s == 'end':
            break
        if s[0].isdigit():
            cur = s
            p = r'\d+'
            match = re.search(p, cur)        
            if match:
                start, end = match.span()
                modified_string = cur[:start] + f'{count}' + cur[end:]
                count += 1
            cur = modified_string
            d[cur] = []
        else:
            d[cur].append(s)
print(d)
  10:
import re
d = {}
curr = None
count = 43
full_text.append('end')
for s in full_text:
    if len(s):
        s = s.strip()
        if s == 'end':
            break
        if s[0].isdigit():
            cur = s
            p = r'\d+'
            match = re.search(p, cur)        
            if match:
                start, end = match.span()
                modified_string = cur[:start] + f'{count}' + cur[end:]
                count += 1
            cur = modified_string
            d[cur] = []
        else:
            d[cur].append(s)
print(d)
  11:
# count 
ans = []
for x in d:
#     print(x)
    s = x + '\n'
    s += '\n'.join(d[x])
#     pri/nt(s)
    cnt = 0
    s = s.replace('\t', ' ')
#     print(s)
#     for i in range(len(s)):
#         if s[i]=='\n':
#             s = s[:i] + f'\n{prefixes[cnt]}. ' + s[i+1:]
#             cnt += 1
# #         print(s)
    ans.append(s)
print(ans)
  12:

doc = Document('2.docx')
full_text = []
for paragraph in doc.paragraphs:
#     print(paragraph.text)
    full_text.append(paragraph.text)

print(full_text)
  13:
second = {}
cur = None
count = 245
c = []
w = []
full_text.append('end')
for s in full_text:
    if len(s):
#         print(s)
        s = s.strip()
        
        if s[0] == '+':
            c.append(s[2:])
        elif s[0] == '-':
            w.append(s[2:])
        else:
            if cur:
                c.extend(w)
                second[cur] = c
            if s == 'end':
                break
            cur = s
            p = r'\d+'
            match = re.search(p, cur)        
            if match:
                start, end = match.span()
                modified_string = cur[:start] + f'{count}' + cur[end:]
                count += 1
            cur = modified_string
            second[cur] = []
            c = []
            w = []
print(second)
  14:
second = {}
cur = None
count = 245
c = []
w = []
full_text.append('end')
for s in full_text:
    if len(s):
        s = s.strip()
        
        if s[0] == '+':
            c.append(s[2:])
        elif s[0] == '-':
            w.append(s[2:])
        else:
            if cur:
                c.extend(w)
                second[cur] = c
            if s == 'end':
                break
            cur = s
            p = r'\d+'
            match = re.search(p, cur)        
            if match:
                start, end = match.span()
                modified_string = cur[:start] + f'{count}' + cur[end:]
                count += 1
            cur = modified_string
            second[cur] = []
            c = []
            w = []
print(second)
  15:
from docx import Document
import re

# Create a new Document
doc = Document()

# Add a table with the same number of rows as res and 2 columns
table = doc.add_table(rows=len(ans) + 19, cols=2)

# Iterate through each row of the table
for i, row_text in enumerate(ans):
    cell = table.cell(i, 0)
    cell.text = row_text

    # Leave the second column empty
    cell = table.cell(i, 1)
    cell.text = ""
for i in range(len(ans), len(ans) + len(second)):
    cell = table.cell(i, 0)
    cell.text = www[i - len(ans)]
    cell = table.cell(i, 1)
    cell.text = ""
# Save the document
doc.save('new.docx')
  16: %history
  17:
second = {}
cur = None
count = 245
c = []
w = []
full_text.append('end')
for s in full_text:
    if len(s):
        s = s.strip()
        
        if s[0] == '+':
            c.append(s[2:])
        elif s[0] == '-':
            w.append(s[2:])
        else:
            if cur:
                c.extend(w)
                second[cur] = c
            if s == 'end':
                break
            cur = s
            p = r'\d+'
            match = re.search(p, cur)        
            if match:
                start, end = match.span()
                modified_string = cur[:start] + f'{count}' + cur[end:]
                count += 1
            cur = modified_string
            second[cur] = []
            c = []
            w = []
print(second)
  18: %history -g -f filename
